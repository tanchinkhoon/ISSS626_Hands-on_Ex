[
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html",
    "href": "Hands-on_Ex07/hand-on_ex07.html",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "Geographically Weighted Regression (GWR) allows relationships between predictors (independent variables) and an outcome (dependent variable) to vary by location. In hedonic pricing, we model the resale prices of condominium units using structural factors (e.g., floor area, age) and locational accessibility (e.g., proximity to transport, parks, schools).\n\n\n\n\nGeospatial: MP14_SUBZONE_WEB_PL (subzone polygons; SVY21 projection).\nAspatial: Condo_resale_2015.csv with columns such as SELLING_PRICE, AREA_SQM, AGE, and multiple proximity variables (in kilometers) to amenities (MRT, parks, schools, etc.).\n\n\n\n\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\n\n# Install and load all required packages in one call --------------------------------\n# pacman::p_load() will install any missing packages and then load them into memory\npacman::p_load(olsrr, corrplot, ggpubr,\n               sf, sfdep, GWmodel, tmap,\n               tidyverse, gtsummary,\n               performance, RColorBrewer, see)\n\nThe R packages needed for this exercise are as follows:\n\nsf: spatial vector data handling; projections.\nsfdep: spatial weights and Moran’s I with tidy‑sf interface.\nGWmodel: GWR bandwidth search and model fitting.\ntmap: static/interactive maps.\ntidyverse: wrangling (dplyr, readr, ggplot2).\nolsrr, performance: OLS diagnostics, VIF, assumption checks.\ncorrplot: correlation matrix visual.\ngtsummary: publication‑quality regression tables.\nRColorBrewer: color palettes for maps.\n\n\n\n\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.\n\n\n\n\n\n\n# Read the URA Master Plan 2014 subzone shapefile -------------------------------\n# dsn: directory; layer: shapefile base name without extension\nmpsz = st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial\",\n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n# Transform the coordinate reference system to SVY21 / EPSG:3414 ----------------\n# This ensures all distance‑based operations use meters (required by GWR bandwidth)\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\n\n# Verify the target CRS ---------------------------------------------------------\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, inspect layer extent (bounding box)\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\nThe print above reports the extent of mpsz_svy21 layer by its lower and upper limits.\n\n\n\n\n\n\n\n# Read the 2015 condo resale dataset as a tibble --------------------------------\ncondo_resale = read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Peek at structure: variable names, types, first few rows ----------------------\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE)\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE)\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nThe print above reveals that the values of LONGITITUDE and LATITUDE fields are in decimal degree. Most probably wgs84 geographic coordinate system is used.\n\n# Quick descriptive statistics for all columns ----------------------------------\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\n\n# Convert LONGITUDE/LATITUDE (WGS84) to POINT geometry and reproject to SVY21 ---\n# 1) st_as_sf(): declare coordinates (lon, lat) with crs=4326 (WGS84 degrees)\n# 2) st_transform(): project to EPSG:3414 so distances are in meters\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\n\n\n\n# Confirm the first few records including geometry --------------------------------\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame.\n\n\n\n\n\n\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\n# Plot raw SELLING_PRICE distribution -----------------\n# aes(x=SELLING_PRICE) maps the price variable to the x‑axis for a histogram\nggplot(data = condo_resale.sf,\n       aes(x = `SELLING_PRICE`)) +\n  geom_histogram(bins = 20,           # 20 equal‑width bins\n                 color = \"black\",     # black outline for readability\n                 fill = \"light blue\") # soft fill color for clarity\n\n\n\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n# Create a log‑price to reduce skewness -----------------------------------------\n# mutate() adds a new variable LOG_SELLING_PRICE = log(SELLING_PRICE)\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\n# Plot the log‑transformed price distribution -----------------------------------\nggplot(data = condo_resale.sf,\n       aes(x = `LOG_SELLING_PRICE`)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\")\n\n\n\n\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n\nn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n# Build individual histograms for key predictors ---------------------------------\nAREA_SQM &lt;- ggplot(data = condo_resale.sf, aes(x = `AREA_SQM`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nAGE &lt;- ggplot(data = condo_resale.sf, aes(x = `AGE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CBD &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CBD`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CHILDCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_MRT &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_MRT`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PARK &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PARK`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\n# Arrange the 12 histograms into a 3x4 panel ------------------------------------\n# ggarrange() helps create small‑multiples (trellis) display\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE,\n          PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA,\n          PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n# Switch tmap to interactive (leaflet) mode -------------------------------------\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n# Draw subzones base layer + condo points colored by SELLING_PRICE --------------\ntm_shape(mpsz_svy21) +\n  tm_polygons() +\n tm_shape(condo_resale.sf) +  \n  tm_dots(fill = \"SELLING_PRICE\",      # color points by price\n          fill_alpha = 0.6,            # semi‑transparent fills\n          size = 0.5,                  # dot size\n          fill.scale = tm_scale_intervals(\n            style = \"quantile\")) +     # quantile breaks (robust to skew)\n  tm_view(set_zoom_limits = c(11,14))  # limit zoom range for usability\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n\n\nset_zoom_limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\n# Switch back to static plotting mode before continuing -------------------------\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\n\nIn this section, we will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n\n\n# Fit a simple linear regression with floor area as the only predictor ------------\ncondo.slr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM,\n                data = condo_resale.sf)\n\n\n# Print model summary: coefficients, R², p‑values, residual spread ----------------\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\\text{Selling_Price} = -258121.1 + 14719\\cdot\\text{Area_SQM}\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n# Visualize scatter with best‑fit line from lm() ---------------------------------\n# geom_smooth(method = lm) overlays the OLS regression line with CI ribbon\nggplot(data = condo_resale.sf,  \n       aes(x = `AREA_SQM`, y = `SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n\n\n\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n# Visualize pairwise correlations among predictors (cols 5:23 from the CSV) ------\ncorrplot(cor(condo_resale[, 5:23]),\n         diag = FALSE,\n         order = \"AOE\",      # Angular Order of Eigenvectors (stable ordering)\n         tl.pos = \"td\",      # text labels on top diagonal\n         tl.cex = 0.5,        # smaller text\n         method = \"number\",  # print numeric correlations\n         type = \"upper\")     # upper triangle only\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n# Build the full hedonic model (drop LEASEHOLD_99YR to avoid high correlation) ---\ncondo.mlr &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET   +\n    PROX_KINDERGARTEN   + PROX_MRT  + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH +\n    PROX_SHOPPING_MALL  + PROX_SUPERMARKET + \n    PROX_BUS_STOP   + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, \n  data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk above consists of two parts:\n- lm() of Base R is used to calibrate a multiple linear regression model. The model output is stored in an lm object called condo.mlr.\n- summary() is used to print the model output.\n\n\n\n\n\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n# Remove variables with weak significance to improve parsimony -------------------\ncondo.mlr1 &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE + \n    PROX_CBD + PROX_CHILDCARE + PROX_MRT +\n    PROX_ELDERLYCARE    + PROX_URA_GROWTH_AREA +\n    PROX_PARK   + PROX_PRIMARY_SCH + \n    PROX_SHOPPING_MALL  + PROX_BUS_STOP + \n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf)\n\n\n# Verify all retained predictors are significant at 5% (or better) ---------------\nsummary(condo.mlr1)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_MRT + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_PARK + \n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\nAREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\nAGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\nPROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\nPROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \nPROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\nPROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\nPROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\nPROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \nPROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\nPROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\nNO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \nFAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \nFREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 756000 on 1421 degrees of freedom\nMultiple R-squared:  0.6507,    Adjusted R-squared:  0.6472 \nF-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16\n\n\nThe output above reveals that all explanatory variables are statistically significant at 95% confident level.\n\n\n\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n# Create a clean table of coefficients, CIs, and p‑values\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n# Append model‑level statistics as a footnote (AIC, R², sigma, etc.) -------------\ntbl_regression(condo.mlr1,\nintercept = TRUE) %&gt;%\nadd_glance_source_note(\nlabel = list(sigma ~ \"σ\"), # Greek sigma symbol\ninclude = c(r.squared, adj.r.squared,\nAIC, statistic,\np.value, sigma))\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\nR² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n\n\n\n\n\n\n\n\nFor more customization options, refer to Tutorial: tbl_regression.\n\n\n\nRegression diagnostics are a set of procedures used to check if a regression model’s assumptions are met and how well the model fits the data. These diagnostics involve checking for issues like non-linear relationships, non-normal errors, non-constant variance, and influential observations to ensure the model’s conclusions are valid and reliable. Common methods include graphical analysis, like residual plots and QQ-plots, and quantitative tests\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression diagnostics. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\n\nresidual diagnostics\n\nmeasures of influence\n\nheteroskedasticity tests\n\ncollinearity diagnostics\n\nmodel fit assessment\n\nvariable contribution assessment\n\nvariable selection procedures\n\n\n\nMulticollinearity occurs when independent variables are not truly independent, meaning a change in one is associated with a change in another. This makes it hard for the model to isolate each variable’s influence on the outcome.\nPerforming a multicollinearity test is crucial in multiple linear regression because it ensures the reliability and interpretability of the model’s results. High multicollinearity, where independent variables are highly correlated, inflates the variance of the estimated coefficients, making them unstable, unreliable, and difficult to interpret. This instability can lead to misleading statistical conclusions, such as a variable appearing statistically insignificant when it is not.\nIn the code chunk below, the check_collinearity() of performance package is used to test if there are sign of multicollinearity.\n\n# Check Variance Inflation Factors (VIF) to confirm low multicollinearity --------\nmlr.vif &lt;- check_collinearity(condo.mlr1) # compute VIFs\nmlr.vif # print the table\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                 Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n             AREA_SQM 1.15 [1.09, 1.23]     1.07      0.87     [0.81, 0.92]\n                  AGE 1.41 [1.33, 1.52]     1.19      0.71     [0.66, 0.75]\n             PROX_CBD 1.57 [1.47, 1.69]     1.25      0.64     [0.59, 0.68]\n       PROX_CHILDCARE 3.26 [3.00, 3.56]     1.81      0.31     [0.28, 0.33]\n             PROX_MRT 1.91 [1.78, 2.07]     1.38      0.52     [0.48, 0.56]\n     PROX_ELDERLYCARE 1.52 [1.42, 1.63]     1.23      0.66     [0.61, 0.70]\n PROX_URA_GROWTH_AREA 1.33 [1.26, 1.43]     1.15      0.75     [0.70, 0.80]\n            PROX_PARK 1.21 [1.15, 1.29]     1.10      0.83     [0.77, 0.87]\n     PROX_PRIMARY_SCH 2.21 [2.05, 2.40]     1.49      0.45     [0.42, 0.49]\n   PROX_SHOPPING_MALL 1.48 [1.39, 1.60]     1.22      0.67     [0.63, 0.72]\n        PROX_BUS_STOP 2.85 [2.62, 3.10]     1.69      0.35     [0.32, 0.38]\n          NO_Of_UNITS 1.45 [1.36, 1.56]     1.20      0.69     [0.64, 0.73]\n      FAMILY_FRIENDLY 1.38 [1.30, 1.48]     1.17      0.72     [0.67, 0.77]\n             FREEHOLD 1.44 [1.36, 1.55]     1.20      0.69     [0.65, 0.74]\n\nplot(mlr.vif) # quick visual of VIF levels\n\n\n\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n#check_model(condo.mlr1, check = \"linearity\")\n\n# Visual check that residuals vs fitted show no strong non‑linearity -------------\nggplot(data = data.frame(Fitted = fitted(condo.mlr1), Residuals = resid(condo.mlr1)),\naes(x = Fitted, y = Residuals)) +\ngeom_point(color = 'blue', alpha = 0.6) +\ngeom_smooth(method = 'loess', se = TRUE, color = 'green', fill = 'grey70') +\ngeom_hline(yintercept = 0, color = 'black', linetype = 'dashed') +\nlabs(title = 'Linearity', subtitle = 'Reference line should be flat and horizontal',\nx = 'Fitted values', y = 'Residuals') +\ntheme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n\nThe normality assumption test for multiple linear regression checks if the model’s residuals (the differences between observed and predicted values) are normally distributed. This is crucial for accurate hypothesis testing and confidence intervals. To test this, you can use visual methods like histograms and Q-Q plots of the residuals, or conduct statistical tests like Shapiro-Wilk test and Kolmogorov-Smirnov test.\nIn the code chunk below, check_normality() of performance package is used to perform normality assumption test on condo.mlr1 model.\n\n# Formal test (often significant with large n); complement with Q‑Q plot ---------\ncheck_normality(condo.mlr1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nThe print above reveals that the p-value of the normality assumption test is less than alpha value of 0.05. Hence we reject the normality assumption at 95% confident level.\n\n\n\n\n\n\nNote\n\n\n\n\ncheck_normality() calls stats::shapiro.test and checks the standardized residuals (or studentized residuals for mixed models) for normal distribution.\n\nNote that this formal test almost always yields significant results for the distribution of residuals and visual inspection (e.g. Q-Q plots) are preferable.\n\n\n\nInstead of showing the test statistic, plot() of see package can be used to plot a the output of check_normality() for visual inspection as shown below.\n\n# Q‑Q plot of standardized residuals from the check_normality() output -----------\nplot(check_normality(condo.mlr1), type = \"qq\")\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\nQ-Q plot above below shows that majority of the data points are felt along the zero line.\nAnother way to check for normality assumption visual is by using check_model() of performance package as shown in the code chunk below.\n\n# Alternative normality panel via performance::check_model -----------------------\ncheck_model(condo.mlr1, check = \"normality\")\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\n\n\n\nThe hedonic model we try to build are using geographically referenced data. Hence, it is crucial to check for spatial autocorrelation because its presence can produce unreliable and misleading results. Traditional regression models, such as ordinary least squares (OLS), assume that observations are independent of one another. However, spatial data often violates this assumption.\nSpatial autocorrelation is the correlation of a variable with itself across different spatial locations. Positive spatial autocorrelation means nearby features tend to be more similar, while negative autocorrelation means they tend to be more dissimilar. This phenomenon is based on the first law of geography: “Everything is related to everything else, but nearby things are more related than distant things”.\nIgnoring spatial autocorrelation in a regression model can lead to serious statistical issues:\n\nBiased and inefficient coefficient estimates: If autocorrelation is present, standard errors of the model coefficients can be wrong, leading to unreliable hypothesis tests (p-values). The model might appear more significant than it is.\nMisleading significance tests: Standard regression models cannot distinguish between true explanatory power and the influence of spatial patterns, resulting in inaccurate p-values.\nModel misspecification: Significant spatial autocorrelation in the regression residuals often signals that important explanatory variables are missing from the model. The spatial patterning of the residuals (over- and under-predictions) can provide clues about what these missing variables might be.\nInflated Type I error rates: Researchers might incorrectly reject a true null.\n\nTo test for spatial autocorrelation, We can run a Moran’s I test on the model’s residuals. Significant spatial autocorrelation in the residuals means the model is not capturing the full spatial story.\nIn order to perform spatial autocorrelation test, we need to export the residual of the hedonic pricing model and save it as a data frame first.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\n# Extract residuals into the sf layer so we can map and test them -------------\ncondo_resale.sf &lt;- cbind(condo_resale.sf,\ncondo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`) # rename residual column\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21) +\ntm_polygons(fill_alpha = 0.4) + # semi‑transparent base\ntm_shape(condo_resale.sf) +\ntm_dots(\nfill = \"MLR_RES\", # color by residual value\nsize = 0.7, # point size\ncol = \"black\", # thin border\nfill.scale = tm_scale( # custom diverging palette\nn = 10,\nvalues = rev(brewer.pal(11, \"RdBu\")), # red‑blue diverging\nstyle = \"quantile\",\nmidpoint = NA),\nfill.legend = tm_legend(title = \"Residuals\")\n) +\ntm_title(\"LM Residuals (Quantile Classification)\") +\ntm_layout(legend.outside = TRUE) +\ntm_view(set_zoom_limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, Global Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using st_dist_band() function of sfdep.\n\n# Build distance‑band neighbors and row‑standardized weights ------------------\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(\n    nb = st_dist_band(st_geometry(geometry), upper = 1500), # neighbors &lt;= 1.5 km\n    wt = st_weights(nb, style = \"W\"), # row‑standardized W\n    .before = 1)\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `nb = st_dist_band(st_geometry(geometry), upper = 1500)`.\nCaused by warning in `spdep::dnearneigh()`:\n! neighbour object has 10 sub-graphs\n\n\nNext, global_moran_perm() of sfdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\n# Permutation Moran’s I test on residuals -------------------------------------\nset.seed(1234) # for reproducibility of the permutation p‑value\nglobal_moran_perm(\n  condo_resale.sf$MLR_RES,\n  nb = condo_resale.sf$nb,\n  wt = condo_resale.sf$wt,\n  alternative = \"two.sided\",\n  nsim = 499)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.14389, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.14389 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n\n\n\n\n\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n\n\n\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument **adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\n# Search for the optimal fixed bandwidth (in meters) using CV --------------------\nbw.fixed &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\", # cross‑validation criterion\nkernel = \"gaussian\", # Gaussian kernel\nadaptive = FALSE, # fixed (distance) bandwidth\nlonglat = FALSE) # coordinates are projected (meters)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\n\n\n\n\n\n\nNoteDo you know why it is in meter?\n\n\n\nThe reason why the recommended bandwidth (971.3405) is expressed in meters because of the coordinate reference system (CRS) were projected in SVY21 (EPSG:3414), all distances and spatial computations (like buffer, bandwidth, kernel distance, etc.) are measured in meters.\n\n\n\n\n\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n# Calibrate the fixed‑bandwidth GWR model ---------------------------------------\ngwr.fixed &lt;- gwr.basic(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\nbw = bw.fixed,\nkernel = 'gaussian',\nlonglat = FALSE)\n\n\n# Inspect the fixed GWR diagnostics (AICc, R², parameter summaries) --------------\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-16 22:22:46.24389 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2025-10-16 22:22:47.140025 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n# Search for the optimal adaptive bandwidth (K neighbors) using CV ----------------\nbw.adaptive &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS +\nFAMILY_FRIENDLY + FREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\",\nkernel = \"gaussian\",\nadaptive = TRUE, # K‑NN style bandwidth\nlonglat = FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n# Calibrate the adaptive‑bandwidth GWR model ------------------------------------\ngwr.adaptive &lt;- gwr.basic(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE +\n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf,\n  bw = bw.adaptive,\n  kernel = 'gaussian',\n  adaptive = TRUE, # activate adaptive bandwidth in the fit\n  longlat = FALSE)\n\nThe code below can be used to display the model output.\n\n# Inspect the adaptive GWR diagnostics (AICc, R²) --------------------------------\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-16 22:22:54.080365 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2025-10-16 22:22:55.086396 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\n\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n# Convert the GWR output SDF (Spatial*DataFrame) to sf for mapping ---------------\ncondo_resale.sf.adaptive &lt;-\nst_as_sf(gwr.adaptive$SDF) %&gt;%\nst_transform(crs = 3414) # ensure consistent projection for mapping\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\n# Inspect the fields (coefficients, SE, t‑values, Local_R2, fitted yhat, etc.) ---\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\n# Quick summary of fitted values (yhat) ------------------------------------------\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\n# Local R²: where the local model fits well or poorly --------------------------------\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(mpsz_svy21) +\ntm_polygons(fill_alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"Local_R2\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\n# Coefficient uncertainty vs strength for AREA_SQM: SE vs t‑value side‑by‑side ----\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21) +\ntm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"AREA_SQM_SE\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21) +\ntm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"AREA_SQM_TV\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\n# Arrange the two interactive maps in a synchronized layout ----------------------\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV,\nasp = 1, ncol = 2,\nsync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\n\nGollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) “GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models”. Journal of Statistical Software, 63(17):1-50, http://www.jstatsoft.org/v63/i17/\nLu B, Harris P, Charlton M, Brunsdon C (2014) “The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models”. Geo-spatial Information Science 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453"
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#overview",
    "href": "Hands-on_Ex07/hand-on_ex07.html#overview",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "Geographically Weighted Regression (GWR) allows relationships between predictors (independent variables) and an outcome (dependent variable) to vary by location. In hedonic pricing, we model the resale prices of condominium units using structural factors (e.g., floor area, age) and locational accessibility (e.g., proximity to transport, parks, schools)."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#the-data",
    "href": "Hands-on_Ex07/hand-on_ex07.html#the-data",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "Geospatial: MP14_SUBZONE_WEB_PL (subzone polygons; SVY21 projection).\nAspatial: Condo_resale_2015.csv with columns such as SELLING_PRICE, AREA_SQM, AGE, and multiple proximity variables (in kilometers) to amenities (MRT, parks, schools, etc.)."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#getting-started",
    "href": "Hands-on_Ex07/hand-on_ex07.html#getting-started",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "Before we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\n\n# Install and load all required packages in one call --------------------------------\n# pacman::p_load() will install any missing packages and then load them into memory\npacman::p_load(olsrr, corrplot, ggpubr,\n               sf, sfdep, GWmodel, tmap,\n               tidyverse, gtsummary,\n               performance, RColorBrewer, see)\n\nThe R packages needed for this exercise are as follows:\n\nsf: spatial vector data handling; projections.\nsfdep: spatial weights and Moran’s I with tidy‑sf interface.\nGWmodel: GWR bandwidth search and model fitting.\ntmap: static/interactive maps.\ntidyverse: wrangling (dplyr, readr, ggplot2).\nolsrr, performance: OLS diagnostics, VIF, assumption checks.\ncorrplot: correlation matrix visual.\ngtsummary: publication‑quality regression tables.\nRColorBrewer: color palettes for maps."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#a-short-note-about-gwmodel",
    "href": "Hands-on_Ex07/hand-on_ex07.html#a-short-note-about-gwmodel",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "GWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex07/hand-on_ex07.html#geospatial-data-wrangling",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "# Read the URA Master Plan 2014 subzone shapefile -------------------------------\n# dsn: directory; layer: shapefile base name without extension\nmpsz = st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial\",\n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n# Transform the coordinate reference system to SVY21 / EPSG:3414 ----------------\n# This ensures all distance‑based operations use meters (required by GWR bandwidth)\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\n\n# Verify the target CRS ---------------------------------------------------------\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, inspect layer extent (bounding box)\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\nThe print above reports the extent of mpsz_svy21 layer by its lower and upper limits."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#aspatial-data-wrangling",
    "href": "Hands-on_Ex07/hand-on_ex07.html#aspatial-data-wrangling",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "# Read the 2015 condo resale dataset as a tibble --------------------------------\ncondo_resale = read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Peek at structure: variable names, types, first few rows ----------------------\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE)\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE)\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nThe print above reveals that the values of LONGITITUDE and LATITUDE fields are in decimal degree. Most probably wgs84 geographic coordinate system is used.\n\n# Quick descriptive statistics for all columns ----------------------------------\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\n\n# Convert LONGITUDE/LATITUDE (WGS84) to POINT geometry and reproject to SVY21 ---\n# 1) st_as_sf(): declare coordinates (lon, lat) with crs=4326 (WGS84 degrees)\n# 2) st_transform(): project to EPSG:3414 so distances are in meters\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\n\n\n\n# Confirm the first few records including geometry --------------------------------\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex07/hand-on_ex07.html#exploratory-data-analysis-eda",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "We can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\n# Plot raw SELLING_PRICE distribution -----------------\n# aes(x=SELLING_PRICE) maps the price variable to the x‑axis for a histogram\nggplot(data = condo_resale.sf,\n       aes(x = `SELLING_PRICE`)) +\n  geom_histogram(bins = 20,           # 20 equal‑width bins\n                 color = \"black\",     # black outline for readability\n                 fill = \"light blue\") # soft fill color for clarity\n\n\n\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n# Create a log‑price to reduce skewness -----------------------------------------\n# mutate() adds a new variable LOG_SELLING_PRICE = log(SELLING_PRICE)\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\n# Plot the log‑transformed price distribution -----------------------------------\nggplot(data = condo_resale.sf,\n       aes(x = `LOG_SELLING_PRICE`)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\")\n\n\n\n\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n\nn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n# Build individual histograms for key predictors ---------------------------------\nAREA_SQM &lt;- ggplot(data = condo_resale.sf, aes(x = `AREA_SQM`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nAGE &lt;- ggplot(data = condo_resale.sf, aes(x = `AGE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CBD &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CBD`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CHILDCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_MRT &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_MRT`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PARK &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PARK`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\n# Arrange the 12 histograms into a 3x4 panel ------------------------------------\n# ggarrange() helps create small‑multiples (trellis) display\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE,\n          PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA,\n          PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n# Switch tmap to interactive (leaflet) mode -------------------------------------\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n# Draw subzones base layer + condo points colored by SELLING_PRICE --------------\ntm_shape(mpsz_svy21) +\n  tm_polygons() +\n tm_shape(condo_resale.sf) +  \n  tm_dots(fill = \"SELLING_PRICE\",      # color points by price\n          fill_alpha = 0.6,            # semi‑transparent fills\n          size = 0.5,                  # dot size\n          fill.scale = tm_scale_intervals(\n            style = \"quantile\")) +     # quantile breaks (robust to skew)\n  tm_view(set_zoom_limits = c(11,14))  # limit zoom range for usability\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n\n\nset_zoom_limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\n# Switch back to static plotting mode before continuing -------------------------\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\"."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#hedonic-pricing-modelling-in-r-ols",
    "href": "Hands-on_Ex07/hand-on_ex07.html#hedonic-pricing-modelling-in-r-ols",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "In this section, we will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n\n\n# Fit a simple linear regression with floor area as the only predictor ------------\ncondo.slr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM,\n                data = condo_resale.sf)\n\n\n# Print model summary: coefficients, R², p‑values, residual spread ----------------\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\\text{Selling_Price} = -258121.1 + 14719\\cdot\\text{Area_SQM}\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n# Visualize scatter with best‑fit line from lm() ---------------------------------\n# geom_smooth(method = lm) overlays the OLS regression line with CI ribbon\nggplot(data = condo_resale.sf,  \n       aes(x = `AREA_SQM`, y = `SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n\n\n\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n# Visualize pairwise correlations among predictors (cols 5:23 from the CSV) ------\ncorrplot(cor(condo_resale[, 5:23]),\n         diag = FALSE,\n         order = \"AOE\",      # Angular Order of Eigenvectors (stable ordering)\n         tl.pos = \"td\",      # text labels on top diagonal\n         tl.cex = 0.5,        # smaller text\n         method = \"number\",  # print numeric correlations\n         type = \"upper\")     # upper triangle only\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n# Build the full hedonic model (drop LEASEHOLD_99YR to avoid high correlation) ---\ncondo.mlr &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET   +\n    PROX_KINDERGARTEN   + PROX_MRT  + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH +\n    PROX_SHOPPING_MALL  + PROX_SUPERMARKET + \n    PROX_BUS_STOP   + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, \n  data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk above consists of two parts:\n- lm() of Base R is used to calibrate a multiple linear regression model. The model output is stored in an lm object called condo.mlr.\n- summary() is used to print the model output.\n\n\n\n\n\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n# Remove variables with weak significance to improve parsimony -------------------\ncondo.mlr1 &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE + \n    PROX_CBD + PROX_CHILDCARE + PROX_MRT +\n    PROX_ELDERLYCARE    + PROX_URA_GROWTH_AREA +\n    PROX_PARK   + PROX_PRIMARY_SCH + \n    PROX_SHOPPING_MALL  + PROX_BUS_STOP + \n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf)\n\n\n# Verify all retained predictors are significant at 5% (or better) ---------------\nsummary(condo.mlr1)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_MRT + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_PARK + \n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\nAREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\nAGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\nPROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\nPROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \nPROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\nPROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\nPROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\nPROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \nPROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\nPROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\nNO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \nFAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \nFREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 756000 on 1421 degrees of freedom\nMultiple R-squared:  0.6507,    Adjusted R-squared:  0.6472 \nF-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16\n\n\nThe output above reveals that all explanatory variables are statistically significant at 95% confident level.\n\n\n\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n# Create a clean table of coefficients, CIs, and p‑values\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n# Append model‑level statistics as a footnote (AIC, R², sigma, etc.) -------------\ntbl_regression(condo.mlr1,\nintercept = TRUE) %&gt;%\nadd_glance_source_note(\nlabel = list(sigma ~ \"σ\"), # Greek sigma symbol\ninclude = c(r.squared, adj.r.squared,\nAIC, statistic,\np.value, sigma))\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\nR² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n\n\n\n\n\n\n\n\nFor more customization options, refer to Tutorial: tbl_regression.\n\n\n\nRegression diagnostics are a set of procedures used to check if a regression model’s assumptions are met and how well the model fits the data. These diagnostics involve checking for issues like non-linear relationships, non-normal errors, non-constant variance, and influential observations to ensure the model’s conclusions are valid and reliable. Common methods include graphical analysis, like residual plots and QQ-plots, and quantitative tests\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression diagnostics. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\n\nresidual diagnostics\n\nmeasures of influence\n\nheteroskedasticity tests\n\ncollinearity diagnostics\n\nmodel fit assessment\n\nvariable contribution assessment\n\nvariable selection procedures\n\n\n\nMulticollinearity occurs when independent variables are not truly independent, meaning a change in one is associated with a change in another. This makes it hard for the model to isolate each variable’s influence on the outcome.\nPerforming a multicollinearity test is crucial in multiple linear regression because it ensures the reliability and interpretability of the model’s results. High multicollinearity, where independent variables are highly correlated, inflates the variance of the estimated coefficients, making them unstable, unreliable, and difficult to interpret. This instability can lead to misleading statistical conclusions, such as a variable appearing statistically insignificant when it is not.\nIn the code chunk below, the check_collinearity() of performance package is used to test if there are sign of multicollinearity.\n\n# Check Variance Inflation Factors (VIF) to confirm low multicollinearity --------\nmlr.vif &lt;- check_collinearity(condo.mlr1) # compute VIFs\nmlr.vif # print the table\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                 Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n             AREA_SQM 1.15 [1.09, 1.23]     1.07      0.87     [0.81, 0.92]\n                  AGE 1.41 [1.33, 1.52]     1.19      0.71     [0.66, 0.75]\n             PROX_CBD 1.57 [1.47, 1.69]     1.25      0.64     [0.59, 0.68]\n       PROX_CHILDCARE 3.26 [3.00, 3.56]     1.81      0.31     [0.28, 0.33]\n             PROX_MRT 1.91 [1.78, 2.07]     1.38      0.52     [0.48, 0.56]\n     PROX_ELDERLYCARE 1.52 [1.42, 1.63]     1.23      0.66     [0.61, 0.70]\n PROX_URA_GROWTH_AREA 1.33 [1.26, 1.43]     1.15      0.75     [0.70, 0.80]\n            PROX_PARK 1.21 [1.15, 1.29]     1.10      0.83     [0.77, 0.87]\n     PROX_PRIMARY_SCH 2.21 [2.05, 2.40]     1.49      0.45     [0.42, 0.49]\n   PROX_SHOPPING_MALL 1.48 [1.39, 1.60]     1.22      0.67     [0.63, 0.72]\n        PROX_BUS_STOP 2.85 [2.62, 3.10]     1.69      0.35     [0.32, 0.38]\n          NO_Of_UNITS 1.45 [1.36, 1.56]     1.20      0.69     [0.64, 0.73]\n      FAMILY_FRIENDLY 1.38 [1.30, 1.48]     1.17      0.72     [0.67, 0.77]\n             FREEHOLD 1.44 [1.36, 1.55]     1.20      0.69     [0.65, 0.74]\n\nplot(mlr.vif) # quick visual of VIF levels\n\n\n\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n#check_model(condo.mlr1, check = \"linearity\")\n\n# Visual check that residuals vs fitted show no strong non‑linearity -------------\nggplot(data = data.frame(Fitted = fitted(condo.mlr1), Residuals = resid(condo.mlr1)),\naes(x = Fitted, y = Residuals)) +\ngeom_point(color = 'blue', alpha = 0.6) +\ngeom_smooth(method = 'loess', se = TRUE, color = 'green', fill = 'grey70') +\ngeom_hline(yintercept = 0, color = 'black', linetype = 'dashed') +\nlabs(title = 'Linearity', subtitle = 'Reference line should be flat and horizontal',\nx = 'Fitted values', y = 'Residuals') +\ntheme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n\nThe normality assumption test for multiple linear regression checks if the model’s residuals (the differences between observed and predicted values) are normally distributed. This is crucial for accurate hypothesis testing and confidence intervals. To test this, you can use visual methods like histograms and Q-Q plots of the residuals, or conduct statistical tests like Shapiro-Wilk test and Kolmogorov-Smirnov test.\nIn the code chunk below, check_normality() of performance package is used to perform normality assumption test on condo.mlr1 model.\n\n# Formal test (often significant with large n); complement with Q‑Q plot ---------\ncheck_normality(condo.mlr1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nThe print above reveals that the p-value of the normality assumption test is less than alpha value of 0.05. Hence we reject the normality assumption at 95% confident level.\n\n\n\n\n\n\nNote\n\n\n\n\ncheck_normality() calls stats::shapiro.test and checks the standardized residuals (or studentized residuals for mixed models) for normal distribution.\n\nNote that this formal test almost always yields significant results for the distribution of residuals and visual inspection (e.g. Q-Q plots) are preferable.\n\n\n\nInstead of showing the test statistic, plot() of see package can be used to plot a the output of check_normality() for visual inspection as shown below.\n\n# Q‑Q plot of standardized residuals from the check_normality() output -----------\nplot(check_normality(condo.mlr1), type = \"qq\")\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\nQ-Q plot above below shows that majority of the data points are felt along the zero line.\nAnother way to check for normality assumption visual is by using check_model() of performance package as shown in the code chunk below.\n\n# Alternative normality panel via performance::check_model -----------------------\ncheck_model(condo.mlr1, check = \"normality\")\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\n\n\n\nThe hedonic model we try to build are using geographically referenced data. Hence, it is crucial to check for spatial autocorrelation because its presence can produce unreliable and misleading results. Traditional regression models, such as ordinary least squares (OLS), assume that observations are independent of one another. However, spatial data often violates this assumption.\nSpatial autocorrelation is the correlation of a variable with itself across different spatial locations. Positive spatial autocorrelation means nearby features tend to be more similar, while negative autocorrelation means they tend to be more dissimilar. This phenomenon is based on the first law of geography: “Everything is related to everything else, but nearby things are more related than distant things”.\nIgnoring spatial autocorrelation in a regression model can lead to serious statistical issues:\n\nBiased and inefficient coefficient estimates: If autocorrelation is present, standard errors of the model coefficients can be wrong, leading to unreliable hypothesis tests (p-values). The model might appear more significant than it is.\nMisleading significance tests: Standard regression models cannot distinguish between true explanatory power and the influence of spatial patterns, resulting in inaccurate p-values.\nModel misspecification: Significant spatial autocorrelation in the regression residuals often signals that important explanatory variables are missing from the model. The spatial patterning of the residuals (over- and under-predictions) can provide clues about what these missing variables might be.\nInflated Type I error rates: Researchers might incorrectly reject a true null.\n\nTo test for spatial autocorrelation, We can run a Moran’s I test on the model’s residuals. Significant spatial autocorrelation in the residuals means the model is not capturing the full spatial story.\nIn order to perform spatial autocorrelation test, we need to export the residual of the hedonic pricing model and save it as a data frame first.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\n# Extract residuals into the sf layer so we can map and test them -------------\ncondo_resale.sf &lt;- cbind(condo_resale.sf,\ncondo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`) # rename residual column\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21) +\ntm_polygons(fill_alpha = 0.4) + # semi‑transparent base\ntm_shape(condo_resale.sf) +\ntm_dots(\nfill = \"MLR_RES\", # color by residual value\nsize = 0.7, # point size\ncol = \"black\", # thin border\nfill.scale = tm_scale( # custom diverging palette\nn = 10,\nvalues = rev(brewer.pal(11, \"RdBu\")), # red‑blue diverging\nstyle = \"quantile\",\nmidpoint = NA),\nfill.legend = tm_legend(title = \"Residuals\")\n) +\ntm_title(\"LM Residuals (Quantile Classification)\") +\ntm_layout(legend.outside = TRUE) +\ntm_view(set_zoom_limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, Global Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using st_dist_band() function of sfdep.\n\n# Build distance‑band neighbors and row‑standardized weights ------------------\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(\n    nb = st_dist_band(st_geometry(geometry), upper = 1500), # neighbors &lt;= 1.5 km\n    wt = st_weights(nb, style = \"W\"), # row‑standardized W\n    .before = 1)\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `nb = st_dist_band(st_geometry(geometry), upper = 1500)`.\nCaused by warning in `spdep::dnearneigh()`:\n! neighbour object has 10 sub-graphs\n\n\nNext, global_moran_perm() of sfdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\n# Permutation Moran’s I test on residuals -------------------------------------\nset.seed(1234) # for reproducibility of the permutation p‑value\nglobal_moran_perm(\n  condo_resale.sf$MLR_RES,\n  nb = condo_resale.sf$nb,\n  wt = condo_resale.sf$wt,\n  alternative = \"two.sided\",\n  nsim = 499)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.14389, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.14389 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex07/hand-on_ex07.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "In this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n\n\n\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument **adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\n# Search for the optimal fixed bandwidth (in meters) using CV --------------------\nbw.fixed &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\", # cross‑validation criterion\nkernel = \"gaussian\", # Gaussian kernel\nadaptive = FALSE, # fixed (distance) bandwidth\nlonglat = FALSE) # coordinates are projected (meters)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\n\n\n\n\n\n\nNoteDo you know why it is in meter?\n\n\n\nThe reason why the recommended bandwidth (971.3405) is expressed in meters because of the coordinate reference system (CRS) were projected in SVY21 (EPSG:3414), all distances and spatial computations (like buffer, bandwidth, kernel distance, etc.) are measured in meters.\n\n\n\n\n\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n# Calibrate the fixed‑bandwidth GWR model ---------------------------------------\ngwr.fixed &lt;- gwr.basic(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\nbw = bw.fixed,\nkernel = 'gaussian',\nlonglat = FALSE)\n\n\n# Inspect the fixed GWR diagnostics (AICc, R², parameter summaries) --------------\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-16 22:22:46.24389 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2025-10-16 22:22:47.140025 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n# Search for the optimal adaptive bandwidth (K neighbors) using CV ----------------\nbw.adaptive &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS +\nFAMILY_FRIENDLY + FREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\",\nkernel = \"gaussian\",\nadaptive = TRUE, # K‑NN style bandwidth\nlonglat = FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n# Calibrate the adaptive‑bandwidth GWR model ------------------------------------\ngwr.adaptive &lt;- gwr.basic(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE +\n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf,\n  bw = bw.adaptive,\n  kernel = 'gaussian',\n  adaptive = TRUE, # activate adaptive bandwidth in the fit\n  longlat = FALSE)\n\nThe code below can be used to display the model output.\n\n# Inspect the adaptive GWR diagnostics (AICc, R²) --------------------------------\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-16 22:22:54.080365 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2025-10-16 22:22:55.086396 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\n\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n# Convert the GWR output SDF (Spatial*DataFrame) to sf for mapping ---------------\ncondo_resale.sf.adaptive &lt;-\nst_as_sf(gwr.adaptive$SDF) %&gt;%\nst_transform(crs = 3414) # ensure consistent projection for mapping\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\n# Inspect the fields (coefficients, SE, t‑values, Local_R2, fitted yhat, etc.) ---\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\n# Quick summary of fitted values (yhat) ------------------------------------------\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\n# Local R²: where the local model fits well or poorly --------------------------------\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(mpsz_svy21) +\ntm_polygons(fill_alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"Local_R2\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\n# Coefficient uncertainty vs strength for AREA_SQM: SE vs t‑value side‑by‑side ----\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21) +\ntm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"AREA_SQM_SE\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21) +\ntm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = \"AREA_SQM_TV\",\nborder.col = \"gray60\",\nborder.lwd = 1) +\ntm_view(set.zoom.limits = c(11,14))\n\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits\n\n# Arrange the two interactive maps in a synchronized layout ----------------------\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV,\nasp = 1, ncol = 2,\nsync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\"."
  },
  {
    "objectID": "Hands-on_Ex07/hand-on_ex07.html#reference",
    "href": "Hands-on_Ex07/hand-on_ex07.html#reference",
    "title": "Hands-on Ex07",
    "section": "",
    "text": "Gollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) “GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models”. Journal of Statistical Software, 63(17):1-50, http://www.jstatsoft.org/v63/i17/\nLu B, Harris P, Charlton M, Brunsdon C (2014) “The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models”. Geo-spatial Information Science 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453"
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html",
    "href": "Hands-on_Ex06/hand-on_ex06.html",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "In this hands‑on exercise, we will learn how to delineate homogeneous regions by using geographically referenced multivariate data. Two major analyses are covered:\n\nhierarchical cluster analysis; and\n\nspatially constrained cluster analysis.\n\n\n\nBy the end of this exercise, we will be able to:\n\nconvert GIS polygon data into R’s simple feature data.frame using sf;\nconvert a simple feature data.frame into SpatialPolygonsDataFrame using sf → sp conversion for SKATER;\nperform cluster analysis using hclust() (Base R) and hclustgeo() (ClustGeo);\nperform spatially constrained clustering using skater() (spdep) and hclustgeo() with spatial dissimilarities; and\nvisualise outputs using ggplot2 and tmap.\n\n\n\n\n\n\n\nSegment Shan State, Myanmar into homogeneous regions at the township level using multiple ICT indicators: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.\n\n\n\n\nTwo datasets are used:\n\nMyanmar Township Boundary Data (mynamar_township_boundaries) — ESRI Shapefile; polygons at township level.\nShan-ICT.csv — extract of The 2014 Myanmar Population and Housing Census Myanmar at township level.\n\n\n\n\n# Install and load all packages used in one call -----------------\npacman::p_load(spdep, tmap, sf, ClustGeo, ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)  # loads if installed; installs if missing\n\n\n\n\n\n\n\nNote\n\n\n\nWith tidyverse, we get readr, ggplot2, dplyr, purrr, etc.\n\n\n\n\n\n\n\n\n\n# Read township boundaries shapefile (sf) ---------------------------------------\nshan_sf &lt;- st_read(dsn = \"data/geospatial\",      # folder containing the shapefile\n                   layer = \"myanmar_township_boundaries\") %&gt;%     # shapefile layer name (without .shp)\n  dplyr::filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%  # keep only Shan State parts\n  dplyr::select(c(2:7))                          # keep fields 2..7\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n# Inspect the resulting simple feature data.frame --------------------------------\nshan_sf                                          # prints sf summary (rows, cols, bbox)\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\nglimpse(shan_sf)  \n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\n\n# Read the ICT attributes table (CSV) ------------------------------------------\nict &lt;- readr::read_csv(\"data/aspatial/Shan-ICT.csv\")  # loads as a tibble data.frame\n\nRows: 55 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Review the summary statistics of raw counts ----------------------------------\nsummary(ict)         \n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\nWe convert raw household counts to per‑thousand‑household penetration rates to remove bias due to township size.\n\n# Derive penetration rates and tidy names --------------------------------------\nict_derived &lt;- ict %&gt;%\n  dplyr::mutate(RADIO_PR    = `Radio`/`Total households` * 1000) %&gt;%            # Radio per 1000 households\n  dplyr::mutate(TV_PR       = `Television`/`Total households` * 1000) %&gt;%       # TV per 1000 households\n  dplyr::mutate(LLPHONE_PR  = `Land line phone`/`Total households` * 1000) %&gt;%  # Landline per 1000 households\n  dplyr::mutate(MPHONE_PR   = `Mobile phone`/`Total households` * 1000) %&gt;%     # Mobile per 1000 households\n  dplyr::mutate(COMPUTER_PR = `Computer`/`Total households` * 1000) %&gt;%         # Computer per 1000 households\n  dplyr::mutate(INTERNET_PR = `Internet at home`/`Total households` * 1000) %&gt;% # Internet per 1000 households\n  dplyr::rename(`DT_PCODE`     = `District Pcode`,                              # harmonise id fields to tidy names\n                `DT`           = `District Name`,\n                `TS_PCODE`     = `Township Pcode`,\n                `TS`           = `Township Name`,\n                `TT_HOUSEHOLDS`= `Total households`,\n                `RADIO`        = `Radio`,\n                `TV`           = `Television`,\n                `LLPHONE`      = `Land line phone`,\n                `MPHONE`       = `Mobile phone`,\n                `COMPUTER`     = `Computer`,\n                `INTERNET`     = `Internet at home`)\n\n\n# Review penetration rates after derivation ------------------------------------\nsummary(ict_derived)     # confirms new *_PR fields       \n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.\n\n\n\n\n\n\n\n\nThe code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\n# Histogram and boxplot for RADIO (raw counts) ---------------------------------\nggplot(data = ict_derived, aes(x = `RADIO`)) +          # choose RADIO (raw) for distribution\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")  # 20 bins; black border; light blue fill\n\n\n\n\n\n\n\nggplot(data = ict_derived, aes(x = `RADIO`)) +          # same variable for outlier check\n  geom_boxplot(color = \"black\", fill = \"light blue\")    # boxplot style\n\n\n\n\n\n\n\n# Histogram and boxplot for RADIO_PR (per-thousand) ----------------------------\nggplot(data = ict_derived, aes(x = `RADIO_PR`)) +       # scaled rate variable\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")  # distribution after scaling\n\n\n\n\n\n\n\nggplot(data = ict_derived, aes(x = `RADIO_PR`)) +       # outlier check on rate\n  geom_boxplot(color = \"black\", fill = \"light blue\")    # single extreme points easily seen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat can you observed from the distributions reveal in the histogram and boxplot?\n\n\n\nFor RADIO (original values)\nHistogram:\n\nThe distribution is highly right-skewed (positively skewed)\n\nMost values are concentrated at the lower end (close to 0–5000)\n\nThere are a few very large values (e.g., ~30,000), suggesting the presence of extreme outliers\n\nBoxplot:\n\nConfirms the positive skewness seen in the histogram\n\nA long whisker extends to the right, and several outliers (dots) appear at high values (&gt;10,000, &gt;30,000).\nThe median is much closer to the lower quartile, reinforcing that most data is clustered at the low end.\n\n\nInterpretation: The RADIO variable has extreme variation with a few very large outliers dominating the scale. Data transformation (e.g., log transform) may be useful to reduce skewness and bring values closer to normality.\n\nFor RADIO_PR (processed/normalized values)\nHistogram:\n\nThe distribution is more uniform and relatively symmetric compared to RADIO\n\nValues spread between 0 and 500, with no extreme concentration at the low end\n\nThe central tendency seems more balanced around 200\n\nBoxplot:\n\nShows a more symmetric spread with a wider interquartile range\n\nOnly one mild outlier (~500)\n\nMedian is near the center of the box, indicating less skewness than RADIO\n\n\nInterpretation: RADIO_PR is much more normalized and balanced compared to RADIO. The transformation/processing (likely standardization or scaling) effectively reduced skewness and limited extreme outlier effects.\n\n\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\n# Multiple histograms: create one plot per ICT rate ----------------------------\nradio    &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`))    + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ntv       &lt;- ggplot(ict_derived, aes(x = `TV_PR`))       + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\nllphone  &lt;- ggplot(ict_derived, aes(x = `LLPHONE_PR`))  + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\nmphone   &lt;- ggplot(ict_derived, aes(x = `MPHONE_PR`))   + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ncomputer &lt;- ggplot(ict_derived, aes(x = `COMPUTER_PR`)) + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ninternet &lt;- ggplot(ict_derived, aes(x = `INTERNET_PR`)) + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggpubr::ggarrange(radio, tv, llphone, mphone, computer, internet,  # arrange 6 histograms\n                   ncol = 3, nrow = 2)                             # arrange plots in grid 3x2 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Join geospatial (shan_sf) with aspatial (ict_derived) by TS_PCODE -------------\nshan_sf &lt;- dplyr::left_join(shan_sf, ict_derived, by = c(\"TS_PCODE\" = \"TS_PCODE\"))  # enrich polygons with ICT\n\n\n# Persist and reload  -----------------------------\n\nreadr::write_rds(shan_sf, \"data/rds/shan_sf.rds\")  # save result to RDS\n\nshan_sf &lt;- readr::read_rds(\"data/rds/shan_sf.rds\") # reload to ensure reproducibility\n\n\n# Quick choropleth of RADIO_PR using tmap (qtm) --------------------------------\nqtm(shan_sf, \"RADIO_PR\")      # quick thematic map for a single variable\n\n\n\n\n\n\n\n\n\n# Compare raw totals vs rate using two side-by-side choropleths -----------------\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) +                            # base shape: township polygons\n  tm_fill(col = \"TT_HOUSEHOLDS\", n = 5, style = \"jenks\", title = \"Total households\") +\n  tm_borders(alpha = 0.5)                                           # light borders as in notes\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\nRADIO.map &lt;- tm_shape(shan_sf) +                                   # second map for raw RADIO counts\n  tm_fill(col = \"RADIO\", n = 5, style = \"jenks\", title = \"Number Radio \") +\n  tm_borders(alpha = 0.5)\n\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map, asp = NA, ncol = 2)     # view side-by-side\n\n\n\n\n\n\n\n\n\n# Faceted choropleth for TT_HOUSEHOLDS and RADIO_PR ----------------------------\ntm_shape(shan_sf) +\n  tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"), style = \"jenks\") +   # show both variables as facets\n  tm_facets(sync = TRUE, ncol = 2) +                               # same breaks/legends aligned\n  tm_legend(legend.position = c(\"right\", \"bottom\")) +              # legend position\n  tm_layout(outer.margins = 0, asp = 0)                             # fill layout; ignore aspect lock\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_legend()`: use 'tm_legend()' inside a layer function, e.g.\n'tm_polygons(..., fill.legend = tm_legend())'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCan you identify the differences?\n\n\n\nWe use style=\"jenks\" so that the classification adapts to the real distribution of households and penetration rates. This produces more meaningful groupings and reveals disparities that fixed intervals might hide. That’s why the bottom maps (with Jenks) highlight differences in adoption (penetration rate) more clearly than the top maps (absolute counts with equal breaks).\n\n\n\n\n\n\n\n\n# Compute correlation among penetration-rate variables -------------------------\ncluster_vars.cor &lt;- stats::cor(ict_derived[, 12:17]) # columns 12..17 are *_PR variables\n\n\n# Mixed correlation plot (numbers + ellipses) ----------------------------------\ncorrplot::corrplot.mixed(cluster_vars.cor, # matrix of correlations\nlower = \"ellipse\", # lower triangle as ellipses\nupper = \"number\", # upper triangle shows numeric values\ntl.pos = \"lt\", # variable labels at left-top\ndiag = \"l\", # show diagonal line\ntl.col = \"black\") # black label text\n\n\n\n\n\n\n\n\n\nThe plot usually shows COMPUTER_PR and INTERNET_PR as highly correlated; we will keep only one (COMPUTER_PR) for clustering to avoid redundancy.\n\n\n\n\n\n\n\n# Pull clustering variables from the joined sf and drop geometry ----------------\ncluster_vars &lt;- shan_sf %&gt;%\nsf::st_set_geometry(NULL) %&gt;% # remove geometry columns\ndplyr::select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", # keep town name + selected *_PR vars\n\"MPHONE_PR\", \"COMPUTER_PR\")\n\nhead(cluster_vars, 10) # preview first 10 rows\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nNoteObservation\n\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\n\n\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\n# Set township names as row names for clustering display -----------------------\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\" # dendrogram labels use township names\n\nhead(cluster_vars, 10) # verify row names applied\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nNoteObservation\n\n\n\nNotice that the row number has been replaced into the township name.\n\n\nNow, we will delete the TS.x field by using the code chunk below.\n\n# Remove the name column from the clustering data.frame ------------------------\n\nshan_ict &lt;- dplyr::select(cluster_vars, c(2:6)) # keep only numeric *_PR fields\n\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n# Persist to RDS to follow notes ------------------------------------------------\nreadr::write_rds(shan_ict, \"data/rds/shan_ict.rds\") # save numeric matrix as tibble\n\n\n\n\nMultiple variables have different ranges; we standardise before computing distances.\n\n\n\n\n# Min–Max scale each column to [0,1] using heatmaply::normalize -----------------\nshan_ict.std &lt;- heatmaply::normalize(shan_ict) # returns scaled data.frame\nsummary(shan_ict.std) # confirm min=0, max=1 per variable\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\n\n\n\n# Z-score scale (mean=0, sd=1) -------------------------------------------------\nshan_ict.z &lt;- scale(shan_ict) # matrix with standardized columns\npsych::describe(shan_ict.z) # convenient table incl. sd, skew, kurtosis\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\n\n\n\n\n\nNote\n\n\n\ndescribe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nZ-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\n# Compare histograms for RADIO_PR across raw/minmax/z-score --------------------\nr &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`)) + # raw rate distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std) # cast to data.frame for ggplot\ns &lt;- ggplot(shan_ict_s_df, aes(x = `RADIO_PR`)) + # min-max scaled distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Min–Max Standardisation\")\n\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z) # z-scored matrix to data.frame\nz &lt;- ggplot(shan_ict_z_df, aes(x = `RADIO_PR`)) + # z-score distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Z-score Standardisation\")\n\n\nggpubr::ggarrange(r, s, z, ncol = 3, nrow = 1) # 3 histograms in one row\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical conclusion can you draw from the histograms above?\n\n\n\nImplications of Standardisation Methods\n\nRaw Values\n\nKeeping raw values may be fine if all variables are measured on the same scale (e.g., all in dollars).\n\nHowever, when variables differ in scale (e.g., RADIO_PR in hundreds vs TV_PR in thousands), models may be biased toward the larger-scale variable.\n\nNot suitable for distance-based methods (kNN, clustering) or optimization algorithms (gradient descent), because large values dominate.\n\nMin–Max Standardisation\n\nScales all values to the fixed interval [0,1].\n\nPreserves the distribution shape and relative distances.\n\nWorks well when features are bounded and we want to keep proportionality (e.g., image pixels, probabilities).\n\nSensitive to outliers: a single extreme value can stretch the range and compress the majority of the data.\n\nZ-score Standardisation\n\nCenters data at mean = 0, scales variance to 1.\n\nUseful for comparing across variables with different scales or units (e.g., comparing exam scores out of 100 vs heights in cm).\n\nLess affected by outliers compared to Min–Max (though still sensitive if outliers are extreme).\n\nParticularly suitable for statistical methods assuming normality or measuring relative deviations (e.g., regression, PCA, clustering).\n\n\nWhen to use which method?\n\nUse Raw Values only if all features are already comparable in scale.\n\nUse Min–Max when working with bounded ranges (e.g., neural networks, image processing).\n\nUse Z-score when the goal is comparability across different units, or when methods assume standardized input (e.g., PCA, k-means, regression).\n\nTakeaway:\nThe histograms confirm that standardisation changes scale but not distributional shape. The choice of method depends on the statistical technique and the role of the variable in the model.\n\n\n\n# Density comparison (same three panels) ---------------------------------------\nr &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Raw values without standardisation\")\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std); s &lt;- ggplot(shan_ict_s_df, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Min–Max Standardisation\")\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z); z &lt;- ggplot(shan_ict_z_df, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Z-score Standardisation\")\n\n\nggpubr::ggarrange(r, s, z, ncol = 3, nrow = 1) # density panels\n\n\n\n\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\n# Euclidean distance among townships based on *_PR variables --------------------\nproxmat &lt;- stats::dist(shan_ict, method = 'euclidean') # produces a 'dist' object\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\n\n# Ward.D agglomerative hierarchical clustering ---------------------------------\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D') # build dendrogram with Ward’s method\n\n\n# Plot dendrogram (smaller labels) ---------------------------------------------\nplot(hclust_ward, cex = 0.6) # visual tree of township similarity\n\n\n\n\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function we can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n# Compare agglomerative coefficients across linkage methods --------------------\nm &lt;- c(\"average\", \"single\", \"complete\", \"ward\") # candidate methods\nnames(m) &lt;- c(\"average\", \"single\", \"complete\", \"ward\") # name the vector for map_dbl\n\n\nac &lt;- function(x) { cluster::agnes(shan_ict, method = x)$ac } # function returning agglomerative coef\n\n\npurrr::map_dbl(m, ac) # higher (~1) → stronger clustering\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\n\n\n\n\n\n\nNoteObservations:\n\n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\n\nAverage Silhouette Method\n\nGap Statistic Method\n\n\n\nThe gap statisticcompares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\n# Gap statistic using hcut() from factoextra -----------------------------------\nset.seed(12345) # reproducibility as per notes\ngap_stat &lt;- cluster::clusGap(shan_ict, FUN = factoextra::hcut, # wrapper around hclust\nnstart = 25, K.max = 10, B = 50) # 25 random starts; up to 10 clusters\n\n\nprint(gap_stat, method = \"firstmax\") # print suggested k (often 1, next best ~6)\n\nClustering Gap statistic [\"clusGap\"] from call:\ncluster::clusGap(x = shan_ict, FUNcluster = factoextra::hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\n# Visualise gap statistic curve -------------------------------------------------\nfactoextra::fviz_gap_stat(gap_stat) # plot with error bars\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\n\n\n\n\n\n\nNote\n\n\n\nIn addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\n# Draw rectangles to highlight k = 6 clusters on Ward dendrogram ----------------\nplot(hclust_ward, cex = 0.6) # redraw base dendrogram\nrect.hclust(hclust_ward, k = 6, border = 2:5) # coloured boxes for six clusters\n\n\n\n\n\n\n\n\n\n\n\n\n# Convert to matrix and draw an interactive cluster heatmap ---------------------\nshan_ict_mat &lt;- data.matrix(shan_ict) # matrix required by heatmaply\n\n\nheatmaply::heatmaply(heatmaply::normalize(shan_ict_mat), # normalise columns to [0,1]\nColv = NA, # keep variables order\ndist_method = \"euclidean\", # distance for rows\nhclust_method = \"ward.D\", # Ward linkage for rows\nseriate = \"OLO\", # optimal leaf ordering\ncolors = Blues, # colour palette per notes\nk_row = 6, # show 6 row clusters\nmargins = c(NA, 200, 60, NA), # wider left/bottom margins for labels\nfontsize_row = 4, # small text for many rows\nfontsize_col = 5, # slightly larger for cols\nmain = \"Geographic Segmentation of Shan State by ICT indicators\",\nxlab = \"ICT Indicators\",\nylab = \"Townships of Shan State\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the dendextend package.\n  Please report the issue at &lt;https://github.com/talgalili/dendextend/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n# Cut the Ward dendrogram into 6 groups and append to sf -----------------------\ngroups &lt;- as.factor(cutree(hclust_ward, k = 6)) # factor labels 1..6\n\ngroups\n\n   Mongmit    Pindaya    Ywangan   Pinlaung     Mabein      Kalaw      Pekon \n         1          1          2          1          3          3          1 \n  Lawksawk  Nawnghkio    Kyaukme       Muse     Laihka    Mongnai    Mawkmai \n         3          3          3          4          1          1          5 \n    Kutkai    Mongton    Mongyai  Mongkaing     Lashio    Mongpan     Matman \n         1          1          5          2          3          3          2 \n Tachileik    Narphan   Mongkhet     Hsipaw   Monghsat    Mongmao    Nansang \n         4          5          5          1          5          6          1 \n Laukkaing   Pangsang      Namtu  Monghpyak    Konkyan   Mongping     Hopong \n         4          6          1          3          5          5          1 \nNyaungshwe   Hsihseng     Mongla      Hseni    Kunlong     Hopang    Namhkan \n         3          1          4          3          1          6          4 \n  Kengtung    Langkho    Monghsu   Taunggyi   Pangwaun     Kyethi     Loilen \n         3          3          1          4          6          1          1 \n    Manton   Mongyang    Kunhing  Mongyawng    Tangyan    Namhsan \n         2          6          1          3          1          1 \nLevels: 1 2 3 4 5 6\n\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # bind as new column\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`) # rename to CLUSTER (exact style)\n\n\n# Map the non-spatial hierarchical clusters ------------------------------------\nqtm(shan_sf_cluster, \"CLUSTER\") # categorical choropleth of clusters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservations:\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.\n\n\n\n\n\n\nIn this section, we will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\n\n# SKATER expects 'sp' polygons; convert sf → sp --------------------------------\nshan_sp &lt;- sf::as_Spatial(shan_sf) # SpatialPolygonsDataFrame object\n\n\n\n\n\n# Build contiguity neighbours (queen) from polygons ----------------------------\n\nshan.nb &lt;- spdep::poly2nb(shan_sp) # list of neighbours by shared borders\n\nsummary(shan.nb) # report links & degree distribution\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\n\n# Plot neighbours atop township boundaries -------------------------------------\ncoords &lt;- sf::st_coordinates(sf::st_centroid(sf::st_geometry(shan_sf))) # centroid coords for each polygon\n\n\nplot(sf::st_geometry(shan_sf), border = grey(.5)) # draw boundaries first (avoid clipping)\nplot(shan.nb, coords, col = \"blue\", add = TRUE) # overlay neighbour graph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if we plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\n\n\n\n# Edge costs as attribute-space dissimilarity between neighbouring townships ----\nlcosts &lt;- spdep::nbcosts(shan.nb, shan_ict)\n\nglimpse(lcosts)\n\nList of 55\n $ : num [1:4] 263 144 431 238\n $ : num [1:3] 257 303 204\n $ : num [1:2] 257 432\n $ : num [1:3] 182 94.6 138.1\n $ : num [1:2] 263 674\n $ : num [1:5] 302.5 182 140 95.7 252.3\n $ : num [1:2] 94.6 139.3\n $ : num [1:9] 204.3 432.2 140 90.8 186.3 ...\n $ : num [1:2] 90.8 157\n $ : num [1:5] 144 186 157 164 348\n $ : num [1:3] 593 705 239\n $ : num [1:5] 523 78.8 157.5 255.8 59.7\n $ : num [1:7] 374.5 131.7 178.1 92.8 203 ...\n $ : num [1:5] 375 462 329 531 389\n $ : num [1:7] 593 580 311 229 205 ...\n $ : num [1:5] 132 200 151 120 237\n $ : num [1:4] 466 177 138 109\n $ : num [1:5] 625 523 424 379 352\n $ : num [1:7] 466.4 290.9 365.9 79.4 170.6 ...\n $ : num [1:3] 178 200 95\n $ : num [1:6] 203 506 202 308 586 ...\n $ : num [1:3] 677 444 432\n $ : num [1:3] 331 316 265\n $ : num [1:5] 203 114 574 531 445\n $ : num [1:8] 249 164 177 424 291 ...\n $ : num [1:5] 151 677 357 141 384\n $ : num [1:5] 331.4 57.6 78.3 187.4 347.1\n $ : num [1:6] 78.8 92.8 462.4 273 94.2 ...\n $ : num [1:4] 580 636 522 332\n $ : num [1:5] 506.3 316.3 57.6 108.4 364.8\n $ : num [1:4] 366 229 449 241\n $ : num [1:5] 444 357 408 221 286\n $ : num [1:3] 705 311 636\n $ : num [1:8] 203 120 202 114 141 ...\n $ : num [1:6] 274.9 157.5 379.4 91.7 513.8 ...\n $ : num [1:5] 138.1 95.7 139.3 225.8 325.1\n $ : num [1:5] 329.3 91.7 225.8 528.1 158.8\n $ : num [1:5] 574 408 202 147 316\n $ : num [1:5] 229.4 79.4 276.2 162.8 584.6\n $ : num [1:4] 205 522 276 271\n $ : num [1:6] 170.6 78.3 331.7 162.8 271.3 ...\n $ : num [1:3] 239 392 722\n $ : num [1:5] 531 384 221 443 202\n $ : num [1:4] 175 531 237 95\n $ : num [1:5] 308 159 147 147 127\n $ : num [1:5] 252 305 514 325 528\n $ : num [1:2] 265 187\n $ : num [1:8] 256 138 352 195 273 ...\n $ : num [1:5] 59.7 388.7 94.2 124.7 158.8\n $ : num [1:8] 431 674 362 647 449 ...\n $ : num [1:4] 586 445 108 147\n $ : num [1:5] 111 128 213 147 279\n $ : num [1:3] 432 286 316\n $ : num [1:8] 109 430 243 347 365 ...\n $ : num [1:5] 238 348 194 241 210\n - attr(*, \"call\")= language spdep::nbcosts(nb = shan.nb, data = shan_ict)\n - attr(*, \"class\")= chr \"nbdist\"\n\n\n\n# Convert neighbour list + costs into a weights list object --------------------\nshan.w &lt;- spdep::nb2listw(shan.nb, lcosts, style = \"B\") # 'B' keeps raw costs (no row standardise)\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\n\n# Compute Minimum Spanning Tree (MST) over the neighbour graph with edge costs --------------------------\nshan.mst &lt;- spdep::mstree(shan.w) # returns an 'mst' matrix (n-1 edges)\n\n\n# Check its class \nclass(shan.mst); \n\n[1] \"mst\"    \"matrix\"\n\n\n\n# Check its dimension \ndim(shan.mst)\n\n[1] 54  3\n\n\n\nThe dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\n\n\n# Inspect the content of shan.mst\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\n\n# Plot MST atop map -------------------------------------------------------------\nplot(sf::st_geometry(shan_sf), border = gray(.5)) # base map\nspdep::plot.mst(shan.mst, coords, col = \"blue\", cex.lab = 0.7, # draw MST links + node ids\ncex.circles = 0.005, add = TRUE)\n\n\n\n\n\n\n\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\n\n\n\n\n# Cut MST into k-1 edges (ncuts = 5 → k = 6 clusters) --------------------------\nclust6 &lt;- spdep::skater(edges = shan.mst[, 1:2], # first 2 columns are node indices\ndata = shan_ict, # attribute data to update SSW\nmethod = \"euclidean\", # distance in attribute space\nncuts = 5) # 6 clusters (k = ncuts + 1)\n\n\n\n\n\n\n\nNote\n\n\n\nThe skater() takes three mandatory arguments:\n\nthe first two columns of the MST matrix (i.e. not the cost),\n\nthe data matrix (to update the costs as units are being grouped), and\nthe number of cuts.\n\nNote: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\n\n\n\nstr(clust6) # list; includes $groups (cluster labels)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\n\nWe can check the cluster assignment by using the conde chunk below.\n\n# Check the cluster assignment\ncc6 &lt;- clust6$groups; cc6 # vector of cluster assignments\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\ncc6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(cc6) # size of each cluster\n\ncc6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\n# Visualise the pruned tree coloured by groups ---------------------------------\nplot(sf::st_geometry(shan_sf), border = gray(.5)) # background polygons\nplot(clust6, coords, cex.lab = .7, # SKATER plotting helper\ngroups.colors = c(\"red\", \"green\", \"blue\", \"brown\", \"pink\"),\ncex.circles = 0.005, add = TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\n# Append SKATER groups to sf and map -------------------------------------------\ngroups_mat &lt;- as.matrix(clust6$groups) # coerce to matrix for cbind\n\n\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;% # bind to previous sf\ndplyr::rename(`SP_CLUSTER` = `as.factor.groups_mat.`) # new field name per notes\n\n\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\") # map spatially constrained clusters\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\n# Side-by-side comparison of non-spatial vs spatially constrained clusters -----\nhclust.map &lt;- qtm(shan_sf_cluster, \"CLUSTER\") + tm_borders(alpha = 0.5) # non-spatial clusters\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\nshclust.map &lt;- qtm(shan_sf_spatialcluster, \"SP_CLUSTER\") + tm_borders(alpha = 0.5) # SKATER clusters\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\ntmap_arrange(hclust.map, shclust.map, asp = NA, ncol = 2) # compare fragmentation vs contiguity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustGeo implements Ward‑like hierarchical clustering with a mixing parameter alpha ∈ [0,1] combining attribute dissimilarities (D0) and spatial dissimilarities (D1). Use choicealpha() to pick alpha balancing contiguity and attribute fit.\n\n\n\n\n# Run hclustgeo() with attribute-space dissimilarity only -----------------------\nnongeo_cluster &lt;- ClustGeo::hclustgeo(proxmat) # same D0 as from dist()\n\n\nplot(nongeo_cluster, cex = 0.5) # dendrogram\nrect.hclust(nongeo_cluster, k = 6, border = 2:5) # highlight 6 clusters\n\n\n\n\n\n\n\n# Map the non-spatial ClustGeo clusters ----------------------------------------\ngroups &lt;- as.factor(cutree(nongeo_cluster, k = 6)) # cut into 6 groups\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # bind to polygons\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\") # categorical map\n\n\n\n\n\n\n\n\n\n\n\n\n# Build spatial distance matrix between polygon centroids ----------------------\ndist &lt;- sf::st_distance(shan_sf, shan_sf) # pairwise great-circle distances\n\n\ndistmat &lt;- stats::as.dist(dist) # convert to 'dist' object for ClustGeo (convert dataframe into matrix)\n\n\n# Choose alpha that trades off contiguity vs attribute fit ---------------------\ncr &lt;- ClustGeo::choicealpha(proxmat, distmat, # D0 and D1 matrices\nrange.alpha = seq(0, 1, 0.1), K = 6, # evaluate alpha from 0..1 for K=6\ngraph = TRUE) # display criterion curves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# With reference to the plot above, adopt alpha = 0.2 -------------------------------------\nclustG &lt;- ClustGeo::hclustgeo(proxmat, distmat, alpha = 0.2) # combined D0/D1 with alpha=0.2\n\n\n# Cut into 6 groups and map ----------------------------------------------------\ngroups &lt;- as.factor(cutree(clustG, k = 6)) # labels 1..6\n\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # append to polygons\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_Gcluster, \"CLUSTER\") # spatially constrained (ClustGeo) map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Boxplot of RADIO_PR by cluster (non-spatial ClustGeo example) ----------------\nggplot(data = shan_sf_ngeo_cluster, # use non-spatial clusters for example\naes(x = CLUSTER, y = RADIO_PR)) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n\n\n\n# Parallel coordinates (GGally) to compare all ICT rates by cluster -------------\nggparcoord(data = shan_sf_ngeo_cluster,       # data with cluster labels\n           columns = c(17:21),                # columns of *_PR variables (as in notes)\n           scale = \"globalminmax\",            # same vertical scale 0..1 per global range\n           alphaLines = 0.2,                  # faint lines \n           boxplot = TRUE,                    # add per-variable boxplots in background\n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) +                     # one panel per cluster\n  theme(axis.text.x = element_text(angle = 30))  # improve x-axis label readability\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\n\nrobust: univariately, subtract median and divide by median absolute deviation.\n\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\n\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\n\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\n\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\n# Compute cluster-wise means to complement visual inspection -------------------\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%                         # work on attributes only\n  group_by(CLUSTER) %&gt;%                             # aggregate by cluster label\n  summarise(mean_RADIO_PR = mean(RADIO_PR),         # mean Radio per 1000 households\n            mean_TV_PR = mean(TV_PR),               # mean TV per 1000 households\n            mean_LLPHONE_PR = mean(LLPHONE_PR),     # mean Landline per 1000 households\n            mean_MPHONE_PR = mean(MPHONE_PR),       # mean Mobile per 1000 households\n            mean_COMPUTER_PR = mean(COMPUTER_PR))   # mean Computer per 1000 households\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#overview",
    "href": "Hands-on_Ex06/hand-on_ex06.html#overview",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "In this hands‑on exercise, we will learn how to delineate homogeneous regions by using geographically referenced multivariate data. Two major analyses are covered:\n\nhierarchical cluster analysis; and\n\nspatially constrained cluster analysis.\n\n\n\nBy the end of this exercise, we will be able to:\n\nconvert GIS polygon data into R’s simple feature data.frame using sf;\nconvert a simple feature data.frame into SpatialPolygonsDataFrame using sf → sp conversion for SKATER;\nperform cluster analysis using hclust() (Base R) and hclustgeo() (ClustGeo);\nperform spatially constrained clustering using skater() (spdep) and hclustgeo() with spatial dissimilarities; and\nvisualise outputs using ggplot2 and tmap."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#getting-started",
    "href": "Hands-on_Ex06/hand-on_ex06.html#getting-started",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "Segment Shan State, Myanmar into homogeneous regions at the township level using multiple ICT indicators: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#the-data",
    "href": "Hands-on_Ex06/hand-on_ex06.html#the-data",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "Two datasets are used:\n\nMyanmar Township Boundary Data (mynamar_township_boundaries) — ESRI Shapefile; polygons at township level.\nShan-ICT.csv — extract of The 2014 Myanmar Population and Housing Census Myanmar at township level.\n\n\n\n\n# Install and load all packages used in one call -----------------\npacman::p_load(spdep, tmap, sf, ClustGeo, ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)  # loads if installed; installs if missing\n\n\n\n\n\n\n\nNote\n\n\n\nWith tidyverse, we get readr, ggplot2, dplyr, purrr, etc."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#data-import-and-preparation",
    "href": "Hands-on_Ex06/hand-on_ex06.html#data-import-and-preparation",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "# Read township boundaries shapefile (sf) ---------------------------------------\nshan_sf &lt;- st_read(dsn = \"data/geospatial\",      # folder containing the shapefile\n                   layer = \"myanmar_township_boundaries\") %&gt;%     # shapefile layer name (without .shp)\n  dplyr::filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%  # keep only Shan State parts\n  dplyr::select(c(2:7))                          # keep fields 2..7\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n# Inspect the resulting simple feature data.frame --------------------------------\nshan_sf                                          # prints sf summary (rows, cols, bbox)\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\nglimpse(shan_sf)  \n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\n\n# Read the ICT attributes table (CSV) ------------------------------------------\nict &lt;- readr::read_csv(\"data/aspatial/Shan-ICT.csv\")  # loads as a tibble data.frame\n\nRows: 55 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Review the summary statistics of raw counts ----------------------------------\nsummary(ict)         \n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\nWe convert raw household counts to per‑thousand‑household penetration rates to remove bias due to township size.\n\n# Derive penetration rates and tidy names --------------------------------------\nict_derived &lt;- ict %&gt;%\n  dplyr::mutate(RADIO_PR    = `Radio`/`Total households` * 1000) %&gt;%            # Radio per 1000 households\n  dplyr::mutate(TV_PR       = `Television`/`Total households` * 1000) %&gt;%       # TV per 1000 households\n  dplyr::mutate(LLPHONE_PR  = `Land line phone`/`Total households` * 1000) %&gt;%  # Landline per 1000 households\n  dplyr::mutate(MPHONE_PR   = `Mobile phone`/`Total households` * 1000) %&gt;%     # Mobile per 1000 households\n  dplyr::mutate(COMPUTER_PR = `Computer`/`Total households` * 1000) %&gt;%         # Computer per 1000 households\n  dplyr::mutate(INTERNET_PR = `Internet at home`/`Total households` * 1000) %&gt;% # Internet per 1000 households\n  dplyr::rename(`DT_PCODE`     = `District Pcode`,                              # harmonise id fields to tidy names\n                `DT`           = `District Name`,\n                `TS_PCODE`     = `Township Pcode`,\n                `TS`           = `Township Name`,\n                `TT_HOUSEHOLDS`= `Total households`,\n                `RADIO`        = `Radio`,\n                `TV`           = `Television`,\n                `LLPHONE`      = `Land line phone`,\n                `MPHONE`       = `Mobile phone`,\n                `COMPUTER`     = `Computer`,\n                `INTERNET`     = `Internet at home`)\n\n\n# Review penetration rates after derivation ------------------------------------\nsummary(ict_derived)     # confirms new *_PR fields       \n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex06/hand-on_ex06.html#exploratory-data-analysis-eda",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "The code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\n# Histogram and boxplot for RADIO (raw counts) ---------------------------------\nggplot(data = ict_derived, aes(x = `RADIO`)) +          # choose RADIO (raw) for distribution\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")  # 20 bins; black border; light blue fill\n\n\n\n\n\n\n\nggplot(data = ict_derived, aes(x = `RADIO`)) +          # same variable for outlier check\n  geom_boxplot(color = \"black\", fill = \"light blue\")    # boxplot style\n\n\n\n\n\n\n\n# Histogram and boxplot for RADIO_PR (per-thousand) ----------------------------\nggplot(data = ict_derived, aes(x = `RADIO_PR`)) +       # scaled rate variable\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")  # distribution after scaling\n\n\n\n\n\n\n\nggplot(data = ict_derived, aes(x = `RADIO_PR`)) +       # outlier check on rate\n  geom_boxplot(color = \"black\", fill = \"light blue\")    # single extreme points easily seen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat can you observed from the distributions reveal in the histogram and boxplot?\n\n\n\nFor RADIO (original values)\nHistogram:\n\nThe distribution is highly right-skewed (positively skewed)\n\nMost values are concentrated at the lower end (close to 0–5000)\n\nThere are a few very large values (e.g., ~30,000), suggesting the presence of extreme outliers\n\nBoxplot:\n\nConfirms the positive skewness seen in the histogram\n\nA long whisker extends to the right, and several outliers (dots) appear at high values (&gt;10,000, &gt;30,000).\nThe median is much closer to the lower quartile, reinforcing that most data is clustered at the low end.\n\n\nInterpretation: The RADIO variable has extreme variation with a few very large outliers dominating the scale. Data transformation (e.g., log transform) may be useful to reduce skewness and bring values closer to normality.\n\nFor RADIO_PR (processed/normalized values)\nHistogram:\n\nThe distribution is more uniform and relatively symmetric compared to RADIO\n\nValues spread between 0 and 500, with no extreme concentration at the low end\n\nThe central tendency seems more balanced around 200\n\nBoxplot:\n\nShows a more symmetric spread with a wider interquartile range\n\nOnly one mild outlier (~500)\n\nMedian is near the center of the box, indicating less skewness than RADIO\n\n\nInterpretation: RADIO_PR is much more normalized and balanced compared to RADIO. The transformation/processing (likely standardization or scaling) effectively reduced skewness and limited extreme outlier effects.\n\n\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\n# Multiple histograms: create one plot per ICT rate ----------------------------\nradio    &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`))    + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ntv       &lt;- ggplot(ict_derived, aes(x = `TV_PR`))       + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\nllphone  &lt;- ggplot(ict_derived, aes(x = `LLPHONE_PR`))  + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\nmphone   &lt;- ggplot(ict_derived, aes(x = `MPHONE_PR`))   + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ncomputer &lt;- ggplot(ict_derived, aes(x = `COMPUTER_PR`)) + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\ninternet &lt;- ggplot(ict_derived, aes(x = `INTERNET_PR`)) + geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggpubr::ggarrange(radio, tv, llphone, mphone, computer, internet,  # arrange 6 histograms\n                   ncol = 3, nrow = 2)                             # arrange plots in grid 3x2 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Join geospatial (shan_sf) with aspatial (ict_derived) by TS_PCODE -------------\nshan_sf &lt;- dplyr::left_join(shan_sf, ict_derived, by = c(\"TS_PCODE\" = \"TS_PCODE\"))  # enrich polygons with ICT\n\n\n# Persist and reload  -----------------------------\n\nreadr::write_rds(shan_sf, \"data/rds/shan_sf.rds\")  # save result to RDS\n\nshan_sf &lt;- readr::read_rds(\"data/rds/shan_sf.rds\") # reload to ensure reproducibility\n\n\n# Quick choropleth of RADIO_PR using tmap (qtm) --------------------------------\nqtm(shan_sf, \"RADIO_PR\")      # quick thematic map for a single variable\n\n\n\n\n\n\n\n\n\n# Compare raw totals vs rate using two side-by-side choropleths -----------------\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) +                            # base shape: township polygons\n  tm_fill(col = \"TT_HOUSEHOLDS\", n = 5, style = \"jenks\", title = \"Total households\") +\n  tm_borders(alpha = 0.5)                                           # light borders as in notes\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\nRADIO.map &lt;- tm_shape(shan_sf) +                                   # second map for raw RADIO counts\n  tm_fill(col = \"RADIO\", n = 5, style = \"jenks\", title = \"Number Radio \") +\n  tm_borders(alpha = 0.5)\n\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map, asp = NA, ncol = 2)     # view side-by-side\n\n\n\n\n\n\n\n\n\n# Faceted choropleth for TT_HOUSEHOLDS and RADIO_PR ----------------------------\ntm_shape(shan_sf) +\n  tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"), style = \"jenks\") +   # show both variables as facets\n  tm_facets(sync = TRUE, ncol = 2) +                               # same breaks/legends aligned\n  tm_legend(legend.position = c(\"right\", \"bottom\")) +              # legend position\n  tm_layout(outer.margins = 0, asp = 0)                             # fill layout; ignore aspect lock\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_legend()`: use 'tm_legend()' inside a layer function, e.g.\n'tm_polygons(..., fill.legend = tm_legend())'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCan you identify the differences?\n\n\n\nWe use style=\"jenks\" so that the classification adapts to the real distribution of households and penetration rates. This produces more meaningful groupings and reveals disparities that fixed intervals might hide. That’s why the bottom maps (with Jenks) highlight differences in adoption (penetration rate) more clearly than the top maps (absolute counts with equal breaks)."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#correlation-analysis",
    "href": "Hands-on_Ex06/hand-on_ex06.html#correlation-analysis",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "# Compute correlation among penetration-rate variables -------------------------\ncluster_vars.cor &lt;- stats::cor(ict_derived[, 12:17]) # columns 12..17 are *_PR variables\n\n\n# Mixed correlation plot (numbers + ellipses) ----------------------------------\ncorrplot::corrplot.mixed(cluster_vars.cor, # matrix of correlations\nlower = \"ellipse\", # lower triangle as ellipses\nupper = \"number\", # upper triangle shows numeric values\ntl.pos = \"lt\", # variable labels at left-top\ndiag = \"l\", # show diagonal line\ntl.col = \"black\") # black label text\n\n\n\n\n\n\n\n\n\nThe plot usually shows COMPUTER_PR and INTERNET_PR as highly correlated; we will keep only one (COMPUTER_PR) for clustering to avoid redundancy."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex06/hand-on_ex06.html#hierarchy-cluster-analysis",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "# Pull clustering variables from the joined sf and drop geometry ----------------\ncluster_vars &lt;- shan_sf %&gt;%\nsf::st_set_geometry(NULL) %&gt;% # remove geometry columns\ndplyr::select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", # keep town name + selected *_PR vars\n\"MPHONE_PR\", \"COMPUTER_PR\")\n\nhead(cluster_vars, 10) # preview first 10 rows\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nNoteObservation\n\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\n\n\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\n# Set township names as row names for clustering display -----------------------\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\" # dendrogram labels use township names\n\nhead(cluster_vars, 10) # verify row names applied\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nNoteObservation\n\n\n\nNotice that the row number has been replaced into the township name.\n\n\nNow, we will delete the TS.x field by using the code chunk below.\n\n# Remove the name column from the clustering data.frame ------------------------\n\nshan_ict &lt;- dplyr::select(cluster_vars, c(2:6)) # keep only numeric *_PR fields\n\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n# Persist to RDS to follow notes ------------------------------------------------\nreadr::write_rds(shan_ict, \"data/rds/shan_ict.rds\") # save numeric matrix as tibble\n\n\n\n\nMultiple variables have different ranges; we standardise before computing distances.\n\n\n\n\n# Min–Max scale each column to [0,1] using heatmaply::normalize -----------------\nshan_ict.std &lt;- heatmaply::normalize(shan_ict) # returns scaled data.frame\nsummary(shan_ict.std) # confirm min=0, max=1 per variable\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\n\n\n\n# Z-score scale (mean=0, sd=1) -------------------------------------------------\nshan_ict.z &lt;- scale(shan_ict) # matrix with standardized columns\npsych::describe(shan_ict.z) # convenient table incl. sd, skew, kurtosis\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\n\n\n\n\n\nNote\n\n\n\ndescribe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nZ-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\n# Compare histograms for RADIO_PR across raw/minmax/z-score --------------------\nr &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`)) + # raw rate distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std) # cast to data.frame for ggplot\ns &lt;- ggplot(shan_ict_s_df, aes(x = `RADIO_PR`)) + # min-max scaled distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Min–Max Standardisation\")\n\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z) # z-scored matrix to data.frame\nz &lt;- ggplot(shan_ict_z_df, aes(x = `RADIO_PR`)) + # z-score distribution\ngeom_histogram(bins = 20, color = \"black\", fill = \"light blue\") +\nggtitle(\"Z-score Standardisation\")\n\n\nggpubr::ggarrange(r, s, z, ncol = 3, nrow = 1) # 3 histograms in one row\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical conclusion can you draw from the histograms above?\n\n\n\nImplications of Standardisation Methods\n\nRaw Values\n\nKeeping raw values may be fine if all variables are measured on the same scale (e.g., all in dollars).\n\nHowever, when variables differ in scale (e.g., RADIO_PR in hundreds vs TV_PR in thousands), models may be biased toward the larger-scale variable.\n\nNot suitable for distance-based methods (kNN, clustering) or optimization algorithms (gradient descent), because large values dominate.\n\nMin–Max Standardisation\n\nScales all values to the fixed interval [0,1].\n\nPreserves the distribution shape and relative distances.\n\nWorks well when features are bounded and we want to keep proportionality (e.g., image pixels, probabilities).\n\nSensitive to outliers: a single extreme value can stretch the range and compress the majority of the data.\n\nZ-score Standardisation\n\nCenters data at mean = 0, scales variance to 1.\n\nUseful for comparing across variables with different scales or units (e.g., comparing exam scores out of 100 vs heights in cm).\n\nLess affected by outliers compared to Min–Max (though still sensitive if outliers are extreme).\n\nParticularly suitable for statistical methods assuming normality or measuring relative deviations (e.g., regression, PCA, clustering).\n\n\nWhen to use which method?\n\nUse Raw Values only if all features are already comparable in scale.\n\nUse Min–Max when working with bounded ranges (e.g., neural networks, image processing).\n\nUse Z-score when the goal is comparability across different units, or when methods assume standardized input (e.g., PCA, k-means, regression).\n\nTakeaway:\nThe histograms confirm that standardisation changes scale but not distributional shape. The choice of method depends on the statistical technique and the role of the variable in the model.\n\n\n\n# Density comparison (same three panels) ---------------------------------------\nr &lt;- ggplot(ict_derived, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Raw values without standardisation\")\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std); s &lt;- ggplot(shan_ict_s_df, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Min–Max Standardisation\")\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z); z &lt;- ggplot(shan_ict_z_df, aes(x = `RADIO_PR`)) + geom_density(color = \"black\", fill = \"light blue\") + ggtitle(\"Z-score Standardisation\")\n\n\nggpubr::ggarrange(r, s, z, ncol = 3, nrow = 1) # density panels\n\n\n\n\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\n# Euclidean distance among townships based on *_PR variables --------------------\nproxmat &lt;- stats::dist(shan_ict, method = 'euclidean') # produces a 'dist' object\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\n\n# Ward.D agglomerative hierarchical clustering ---------------------------------\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D') # build dendrogram with Ward’s method\n\n\n# Plot dendrogram (smaller labels) ---------------------------------------------\nplot(hclust_ward, cex = 0.6) # visual tree of township similarity\n\n\n\n\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function we can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n# Compare agglomerative coefficients across linkage methods --------------------\nm &lt;- c(\"average\", \"single\", \"complete\", \"ward\") # candidate methods\nnames(m) &lt;- c(\"average\", \"single\", \"complete\", \"ward\") # name the vector for map_dbl\n\n\nac &lt;- function(x) { cluster::agnes(shan_ict, method = x)$ac } # function returning agglomerative coef\n\n\npurrr::map_dbl(m, ac) # higher (~1) → stronger clustering\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\n\n\n\n\n\n\nNoteObservations:\n\n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\n\nAverage Silhouette Method\n\nGap Statistic Method\n\n\n\nThe gap statisticcompares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\n# Gap statistic using hcut() from factoextra -----------------------------------\nset.seed(12345) # reproducibility as per notes\ngap_stat &lt;- cluster::clusGap(shan_ict, FUN = factoextra::hcut, # wrapper around hclust\nnstart = 25, K.max = 10, B = 50) # 25 random starts; up to 10 clusters\n\n\nprint(gap_stat, method = \"firstmax\") # print suggested k (often 1, next best ~6)\n\nClustering Gap statistic [\"clusGap\"] from call:\ncluster::clusGap(x = shan_ict, FUNcluster = factoextra::hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\n# Visualise gap statistic curve -------------------------------------------------\nfactoextra::fviz_gap_stat(gap_stat) # plot with error bars\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\n\n\n\n\n\n\nNote\n\n\n\nIn addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\n# Draw rectangles to highlight k = 6 clusters on Ward dendrogram ----------------\nplot(hclust_ward, cex = 0.6) # redraw base dendrogram\nrect.hclust(hclust_ward, k = 6, border = 2:5) # coloured boxes for six clusters\n\n\n\n\n\n\n\n\n\n\n\n\n# Convert to matrix and draw an interactive cluster heatmap ---------------------\nshan_ict_mat &lt;- data.matrix(shan_ict) # matrix required by heatmaply\n\n\nheatmaply::heatmaply(heatmaply::normalize(shan_ict_mat), # normalise columns to [0,1]\nColv = NA, # keep variables order\ndist_method = \"euclidean\", # distance for rows\nhclust_method = \"ward.D\", # Ward linkage for rows\nseriate = \"OLO\", # optimal leaf ordering\ncolors = Blues, # colour palette per notes\nk_row = 6, # show 6 row clusters\nmargins = c(NA, 200, 60, NA), # wider left/bottom margins for labels\nfontsize_row = 4, # small text for many rows\nfontsize_col = 5, # slightly larger for cols\nmain = \"Geographic Segmentation of Shan State by ICT indicators\",\nxlab = \"ICT Indicators\",\nylab = \"Townships of Shan State\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the dendextend package.\n  Please report the issue at &lt;https://github.com/talgalili/dendextend/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n# Cut the Ward dendrogram into 6 groups and append to sf -----------------------\ngroups &lt;- as.factor(cutree(hclust_ward, k = 6)) # factor labels 1..6\n\ngroups\n\n   Mongmit    Pindaya    Ywangan   Pinlaung     Mabein      Kalaw      Pekon \n         1          1          2          1          3          3          1 \n  Lawksawk  Nawnghkio    Kyaukme       Muse     Laihka    Mongnai    Mawkmai \n         3          3          3          4          1          1          5 \n    Kutkai    Mongton    Mongyai  Mongkaing     Lashio    Mongpan     Matman \n         1          1          5          2          3          3          2 \n Tachileik    Narphan   Mongkhet     Hsipaw   Monghsat    Mongmao    Nansang \n         4          5          5          1          5          6          1 \n Laukkaing   Pangsang      Namtu  Monghpyak    Konkyan   Mongping     Hopong \n         4          6          1          3          5          5          1 \nNyaungshwe   Hsihseng     Mongla      Hseni    Kunlong     Hopang    Namhkan \n         3          1          4          3          1          6          4 \n  Kengtung    Langkho    Monghsu   Taunggyi   Pangwaun     Kyethi     Loilen \n         3          3          1          4          6          1          1 \n    Manton   Mongyang    Kunhing  Mongyawng    Tangyan    Namhsan \n         2          6          1          3          1          1 \nLevels: 1 2 3 4 5 6\n\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # bind as new column\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`) # rename to CLUSTER (exact style)\n\n\n# Map the non-spatial hierarchical clusters ------------------------------------\nqtm(shan_sf_cluster, \"CLUSTER\") # categorical choropleth of clusters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservations:\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex06/hand-on_ex06.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "In this section, we will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\n\n# SKATER expects 'sp' polygons; convert sf → sp --------------------------------\nshan_sp &lt;- sf::as_Spatial(shan_sf) # SpatialPolygonsDataFrame object\n\n\n\n\n\n# Build contiguity neighbours (queen) from polygons ----------------------------\n\nshan.nb &lt;- spdep::poly2nb(shan_sp) # list of neighbours by shared borders\n\nsummary(shan.nb) # report links & degree distribution\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\n\n# Plot neighbours atop township boundaries -------------------------------------\ncoords &lt;- sf::st_coordinates(sf::st_centroid(sf::st_geometry(shan_sf))) # centroid coords for each polygon\n\n\nplot(sf::st_geometry(shan_sf), border = grey(.5)) # draw boundaries first (avoid clipping)\nplot(shan.nb, coords, col = \"blue\", add = TRUE) # overlay neighbour graph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if we plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\n\n\n\n# Edge costs as attribute-space dissimilarity between neighbouring townships ----\nlcosts &lt;- spdep::nbcosts(shan.nb, shan_ict)\n\nglimpse(lcosts)\n\nList of 55\n $ : num [1:4] 263 144 431 238\n $ : num [1:3] 257 303 204\n $ : num [1:2] 257 432\n $ : num [1:3] 182 94.6 138.1\n $ : num [1:2] 263 674\n $ : num [1:5] 302.5 182 140 95.7 252.3\n $ : num [1:2] 94.6 139.3\n $ : num [1:9] 204.3 432.2 140 90.8 186.3 ...\n $ : num [1:2] 90.8 157\n $ : num [1:5] 144 186 157 164 348\n $ : num [1:3] 593 705 239\n $ : num [1:5] 523 78.8 157.5 255.8 59.7\n $ : num [1:7] 374.5 131.7 178.1 92.8 203 ...\n $ : num [1:5] 375 462 329 531 389\n $ : num [1:7] 593 580 311 229 205 ...\n $ : num [1:5] 132 200 151 120 237\n $ : num [1:4] 466 177 138 109\n $ : num [1:5] 625 523 424 379 352\n $ : num [1:7] 466.4 290.9 365.9 79.4 170.6 ...\n $ : num [1:3] 178 200 95\n $ : num [1:6] 203 506 202 308 586 ...\n $ : num [1:3] 677 444 432\n $ : num [1:3] 331 316 265\n $ : num [1:5] 203 114 574 531 445\n $ : num [1:8] 249 164 177 424 291 ...\n $ : num [1:5] 151 677 357 141 384\n $ : num [1:5] 331.4 57.6 78.3 187.4 347.1\n $ : num [1:6] 78.8 92.8 462.4 273 94.2 ...\n $ : num [1:4] 580 636 522 332\n $ : num [1:5] 506.3 316.3 57.6 108.4 364.8\n $ : num [1:4] 366 229 449 241\n $ : num [1:5] 444 357 408 221 286\n $ : num [1:3] 705 311 636\n $ : num [1:8] 203 120 202 114 141 ...\n $ : num [1:6] 274.9 157.5 379.4 91.7 513.8 ...\n $ : num [1:5] 138.1 95.7 139.3 225.8 325.1\n $ : num [1:5] 329.3 91.7 225.8 528.1 158.8\n $ : num [1:5] 574 408 202 147 316\n $ : num [1:5] 229.4 79.4 276.2 162.8 584.6\n $ : num [1:4] 205 522 276 271\n $ : num [1:6] 170.6 78.3 331.7 162.8 271.3 ...\n $ : num [1:3] 239 392 722\n $ : num [1:5] 531 384 221 443 202\n $ : num [1:4] 175 531 237 95\n $ : num [1:5] 308 159 147 147 127\n $ : num [1:5] 252 305 514 325 528\n $ : num [1:2] 265 187\n $ : num [1:8] 256 138 352 195 273 ...\n $ : num [1:5] 59.7 388.7 94.2 124.7 158.8\n $ : num [1:8] 431 674 362 647 449 ...\n $ : num [1:4] 586 445 108 147\n $ : num [1:5] 111 128 213 147 279\n $ : num [1:3] 432 286 316\n $ : num [1:8] 109 430 243 347 365 ...\n $ : num [1:5] 238 348 194 241 210\n - attr(*, \"call\")= language spdep::nbcosts(nb = shan.nb, data = shan_ict)\n - attr(*, \"class\")= chr \"nbdist\"\n\n\n\n# Convert neighbour list + costs into a weights list object --------------------\nshan.w &lt;- spdep::nb2listw(shan.nb, lcosts, style = \"B\") # 'B' keeps raw costs (no row standardise)\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\n\n# Compute Minimum Spanning Tree (MST) over the neighbour graph with edge costs --------------------------\nshan.mst &lt;- spdep::mstree(shan.w) # returns an 'mst' matrix (n-1 edges)\n\n\n# Check its class \nclass(shan.mst); \n\n[1] \"mst\"    \"matrix\"\n\n\n\n# Check its dimension \ndim(shan.mst)\n\n[1] 54  3\n\n\n\nThe dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\n\n\n# Inspect the content of shan.mst\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\n\n# Plot MST atop map -------------------------------------------------------------\nplot(sf::st_geometry(shan_sf), border = gray(.5)) # base map\nspdep::plot.mst(shan.mst, coords, col = \"blue\", cex.lab = 0.7, # draw MST links + node ids\ncex.circles = 0.005, add = TRUE)\n\n\n\n\n\n\n\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\n\n\n\n\n# Cut MST into k-1 edges (ncuts = 5 → k = 6 clusters) --------------------------\nclust6 &lt;- spdep::skater(edges = shan.mst[, 1:2], # first 2 columns are node indices\ndata = shan_ict, # attribute data to update SSW\nmethod = \"euclidean\", # distance in attribute space\nncuts = 5) # 6 clusters (k = ncuts + 1)\n\n\n\n\n\n\n\nNote\n\n\n\nThe skater() takes three mandatory arguments:\n\nthe first two columns of the MST matrix (i.e. not the cost),\n\nthe data matrix (to update the costs as units are being grouped), and\nthe number of cuts.\n\nNote: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\n\n\n\nstr(clust6) # list; includes $groups (cluster labels)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\n\nWe can check the cluster assignment by using the conde chunk below.\n\n# Check the cluster assignment\ncc6 &lt;- clust6$groups; cc6 # vector of cluster assignments\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\ncc6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(cc6) # size of each cluster\n\ncc6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\n# Visualise the pruned tree coloured by groups ---------------------------------\nplot(sf::st_geometry(shan_sf), border = gray(.5)) # background polygons\nplot(clust6, coords, cex.lab = .7, # SKATER plotting helper\ngroups.colors = c(\"red\", \"green\", \"blue\", \"brown\", \"pink\"),\ncex.circles = 0.005, add = TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\n# Append SKATER groups to sf and map -------------------------------------------\ngroups_mat &lt;- as.matrix(clust6$groups) # coerce to matrix for cbind\n\n\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;% # bind to previous sf\ndplyr::rename(`SP_CLUSTER` = `as.factor.groups_mat.`) # new field name per notes\n\n\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\") # map spatially constrained clusters\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\n# Side-by-side comparison of non-spatial vs spatially constrained clusters -----\nhclust.map &lt;- qtm(shan_sf_cluster, \"CLUSTER\") + tm_borders(alpha = 0.5) # non-spatial clusters\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\nshclust.map &lt;- qtm(shan_sf_spatialcluster, \"SP_CLUSTER\") + tm_borders(alpha = 0.5) # SKATER clusters\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\ntmap_arrange(hclust.map, shclust.map, asp = NA, ncol = 2) # compare fragmentation vs contiguity"
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex06/hand-on_ex06.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "ClustGeo implements Ward‑like hierarchical clustering with a mixing parameter alpha ∈ [0,1] combining attribute dissimilarities (D0) and spatial dissimilarities (D1). Use choicealpha() to pick alpha balancing contiguity and attribute fit.\n\n\n\n\n# Run hclustgeo() with attribute-space dissimilarity only -----------------------\nnongeo_cluster &lt;- ClustGeo::hclustgeo(proxmat) # same D0 as from dist()\n\n\nplot(nongeo_cluster, cex = 0.5) # dendrogram\nrect.hclust(nongeo_cluster, k = 6, border = 2:5) # highlight 6 clusters\n\n\n\n\n\n\n\n# Map the non-spatial ClustGeo clusters ----------------------------------------\ngroups &lt;- as.factor(cutree(nongeo_cluster, k = 6)) # cut into 6 groups\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # bind to polygons\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\") # categorical map\n\n\n\n\n\n\n\n\n\n\n\n\n# Build spatial distance matrix between polygon centroids ----------------------\ndist &lt;- sf::st_distance(shan_sf, shan_sf) # pairwise great-circle distances\n\n\ndistmat &lt;- stats::as.dist(dist) # convert to 'dist' object for ClustGeo (convert dataframe into matrix)\n\n\n# Choose alpha that trades off contiguity vs attribute fit ---------------------\ncr &lt;- ClustGeo::choicealpha(proxmat, distmat, # D0 and D1 matrices\nrange.alpha = seq(0, 1, 0.1), K = 6, # evaluate alpha from 0..1 for K=6\ngraph = TRUE) # display criterion curves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# With reference to the plot above, adopt alpha = 0.2 -------------------------------------\nclustG &lt;- ClustGeo::hclustgeo(proxmat, distmat, alpha = 0.2) # combined D0/D1 with alpha=0.2\n\n\n# Cut into 6 groups and map ----------------------------------------------------\ngroups &lt;- as.factor(cutree(clustG, k = 6)) # labels 1..6\n\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% # append to polygons\ndplyr::rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_Gcluster, \"CLUSTER\") # spatially constrained (ClustGeo) map"
  },
  {
    "objectID": "Hands-on_Ex06/hand-on_ex06.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex06/hand-on_ex06.html#visual-interpretation-of-clusters",
    "title": "Hands-on Ex06",
    "section": "",
    "text": "# Boxplot of RADIO_PR by cluster (non-spatial ClustGeo example) ----------------\nggplot(data = shan_sf_ngeo_cluster, # use non-spatial clusters for example\naes(x = CLUSTER, y = RADIO_PR)) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n\n\n\n# Parallel coordinates (GGally) to compare all ICT rates by cluster -------------\nggparcoord(data = shan_sf_ngeo_cluster,       # data with cluster labels\n           columns = c(17:21),                # columns of *_PR variables (as in notes)\n           scale = \"globalminmax\",            # same vertical scale 0..1 per global range\n           alphaLines = 0.2,                  # faint lines \n           boxplot = TRUE,                    # add per-variable boxplots in background\n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) +                     # one panel per cluster\n  theme(axis.text.x = element_text(angle = 30))  # improve x-axis label readability\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\n\nrobust: univariately, subtract median and divide by median absolute deviation.\n\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\n\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\n\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\n\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\n# Compute cluster-wise means to complement visual inspection -------------------\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%                         # work on attributes only\n  group_by(CLUSTER) %&gt;%                             # aggregate by cluster label\n  summarise(mean_RADIO_PR = mean(RADIO_PR),         # mean Radio per 1000 households\n            mean_TV_PR = mean(TV_PR),               # mean TV per 1000 households\n            mean_LLPHONE_PR = mean(LLPHONE_PR),     # mean Landline per 1000 households\n            mean_MPHONE_PR = mean(MPHONE_PR),       # mean Mobile per 1000 households\n            mean_COMPUTER_PR = mean(COMPUTER_PR))   # mean Computer per 1000 households\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#introduction",
    "href": "Take-home_Ex02/take-home_ex02.html#introduction",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "1 Introduction",
    "text": "1 Introduction\nPublic transport is the backbone of Singapore’s urban mobility system, linking homes, workplaces and commercial centres into an integrated city network. Within this system, public buses remain the most extensive and inclusive mode, serving both central and peripheral areas. As Singapore advances toward the Land Transport Master Plan 2040, analysing spatial and temporal variations in bus passenger movement is essential for informed planning. Descriptive statistics and heat maps offer only surface patterns and cannot reveal the deeper spatial relationships among travel zones.\nThis study applies Global and Local Measures of Spatial Autocorrelation (G/LMSA), including Global and Local Moran I, and Getis Ord Gi Star, to identify clusters and outliers in bus trip intensity across Singapore. These indicators reveal neighbourhoods of consistently high or low ridership and pinpoint areas that diverge from surrounding conditions. To incorporate the temporal dimension, the study employs Emerging Hot Spot Analysis (EMSA) using the Gi Star statistic together with the Mann Kendall (KM) trend test to assess how hot and cold spots evolve during morning, evening and weekend periods.\nThe results will provide empirical evidence for improving bus network design, enhancing accessibility and promoting a more efficient and equitable transport system that supports sustainable urban development."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#research-questions",
    "href": "Take-home_Ex02/take-home_ex02.html#research-questions",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "2 Research Questions",
    "text": "2 Research Questions\nThe present study aims to examine spatial and temporal patterns of public bus mobility in Singapore through the application of local spatial statistical methods. Building upon the objectives outlined in the exercise, the research is guided by four interrelated questions that connect theoretical analysis with practical urban transport planning.\nRQ1 – Spatial Distribution: Where do bus trips concentrate across the city during weekday morning, weekday evening and weekend periods, and what does this reveal about overall mobility intensity?\nRQ2 – Local Spatial Clustering: Which areas display statistically significant clusters or outliers of bus trip intensity as detected by Local Moran I, and Getis Ord Gi Star (Gi*)?\nRQ3 – Temporal Evolution: How do the identified hot and cold spots change over time, and what categories of emerging, intensifying, diminishing or sporadic patterns can be observed through the Mann Kendall trend analysis?\nRQ4 – Policy and Planning Implications: How can the identified spatial and temporal patterns of bus ridership inform the design of transport policies, allocation of resources and long-term planning for an efficient and inclusive public transport network?\nTogether these questions structure the analytical framework and ensure direct alignment between the study objectives and subsequent methods."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#the-data",
    "href": "Take-home_Ex02/take-home_ex02.html#the-data",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "3 The Data",
    "text": "3 The Data\nThis study uses official datasets from national agencies to analyse spatial and temporal patterns of bus mobility in Singapore. The Land Transport Authority (LTA) DataMall Passenger Volume OD Bus dataset provides detailed records of passenger trips between bus stops, including origin code, destination code, day type, hour and total trip volume. The LTA Bus Stop layer supplies the geographic locations and attributes of all active bus stops, while the Urban Redevelopment Authority (URA) Master Plan 2019 Subzone Boundary defines the official spatial units for aggregation and mapping.\nAll spatial data are projected to the Singapore SVY21 coordinate system (EPSG 3414) to preserve distance accuracy. The datasets are cleaned and joined in the R environment to generate analytical hexagonal zones containing at least one bus stop. These integrated data support the computation of local spatial statistics and the identification of emerging patterns in public bus ridership.\n\n# --- 3.1 Create data source table --------------------------------------------\n\n# Load knitr (already part of pacman::p_load in setup)\npacman::p_load(knitr)\n\n# Create a data frame listing all datasets used in the study\ndata_sources &lt;- data.frame(\n  Dataset_Name = c(\n    \"LTA Passenger Volume OD Bus\",\n    \"LTA Bus Stop Location\",\n    \"URA Master Plan 2019 Subzone Boundary\",\n    \"Coordinate Reference System\"\n  ),\n  Description = c(\n    \"Passenger trips by origin, destination, day type and hour.\",\n    \"Geographic location and attributes of all operational bus stops.\",\n    \"Official planning boundaries for spatial aggregation and mapping.\",\n    \"Projected Singapore SVY21 system ensuring metric distance accuracy.\"\n  ),\n  Format = c(\"CSV\", \"Shapefile\", \"KML / GeoJSON\", \"EPSG 3414\"),\n  Source = c(\n    \"LTA DataMall\",\n    \"LTA DataMall\",\n    \"data.gov.sg (URA)\",\n    \"Singapore Land Authority\"\n  )\n)\n\n# Render the table in Quarto / R Markdown output\nkable(\n  data_sources,\n  caption = \"Table 1: Data Sources for Bus Mobility Analysis in Singapore\",\n  align = \"llll\"\n)\n\n\nTable 1: Data Sources for Bus Mobility Analysis in Singapore\n\n\n\n\n\n\n\n\nDataset_Name\nDescription\nFormat\nSource\n\n\n\n\nLTA Passenger Volume OD Bus\nPassenger trips by origin, destination, day type and hour.\nCSV\nLTA DataMall\n\n\nLTA Bus Stop Location\nGeographic location and attributes of all operational bus stops.\nShapefile\nLTA DataMall\n\n\nURA Master Plan 2019 Subzone Boundary\nOfficial planning boundaries for spatial aggregation and mapping.\nKML / GeoJSON\ndata.gov.sg (URA)\n\n\nCoordinate Reference System\nProjected Singapore SVY21 system ensuring metric distance accuracy.\nEPSG 3414\nSingapore Land Authority"
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#setup-the-environment",
    "href": "Take-home_Ex02/take-home_ex02.html#setup-the-environment",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "4 Setup the Environment",
    "text": "4 Setup the Environment\nA consistent analytical environment ensures reproducibility and transparency in spatial data analysis. This section defines the R environment used in the study. All scripts were executed in RStudio using packages that support data wrangling, spatial statistics and visualisation. The pacman package is used to automate installation and loading of required libraries. Each library serves a specific purpose within the analytical workflow, and a random seed is set to guarantee consistent statistical outputs across repeated runs.\n\n# --- 4.1 Load and install required packages ----------------------------------\n\nif (!require(pacman)) install.packages(\"pacman\")  \n# Checks whether 'pacman' is already installed. \n# If missing, it installs the package so that subsequent commands can run.\n\npacman::p_load(                           \n  tidyverse,  # Provides data manipulation and plotting tools (dplyr, ggplot2, readr).\n  stats,      # Base R toolbox for statistical tests, distributions, and modelling.\n  sf,         # Handles spatial vector data such as points, lines, and polygons.\n  spdep,      # Core spatial dependence utilities for spatial neighbour weights, and models.\n  sfdep,      # Performs spatial dependence analysis (Moran I, Gi* and related statistics).\n  tmap,       # Creates static and interactive thematic maps for visualisation.\n  ggplot2,    # Grammar of graphics plotting used for your diagnostics and faceted bar charts.\n  plotly,     # Adds interactivity to ggplot maps and statistical plots.\n  Kendall,    # Performs the Mann Kendall trend test for temporal trend detection.\n  classInt,   # Breaks generators for choropleths.\n  tibble,     # Modern data frame with cleaner printing and safer subsetting for pipelines.\n  purrr,      # Functional programming mappers like map and pmap to run EHSA across multiple periods.\n  knitr,      # Enables dynamic report generation in Quarto or R Markdown.\n  kableExtra  # Enhances table formatting for publication-quality outputs.\n)\n\n# --- 4.2 Reproducibility setting ---------------------------------------------\n\nset.seed(626)  \n# Establishes a fixed random seed so that random processes \n# (for example, simulation-based significance tests) produce identical results on each run."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#section",
    "href": "Take-home_Ex02/take-home_ex02.html#section",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "7 ",
    "text": "7"
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html",
    "href": "Hands-on_Ex05/hand-on_ex05b.html",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "We will import geospatial and aspatial data for Hunan Province (county level), join attributes, visualise a regional indicator (GDP per capita, GDPPC), build contiguity and distance spatial weights, and compute two local spatial statistics:\n\nLocal Moran’s I (LISA) for cluster/outlier detection.\nGetis–Ord Gi* for Hot/Cold Spot analysis (HCSA).\n\nWe will then create choropleths for I values, \\(p\\)-values, LISA clusters (\\(p &lt; 0.05%\\)), and HCSA clusters (\\(p &lt; 0.05\\)), plus Moran scatterplots (raw and standardised).\n\n\n\n\n\nIn spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level (Geospatial). This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv (Aspatial): This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\n\n# load packages (installs missing ones, then attaches)\npacman::p_load(sf, sfdep, tmap, tidyverse)  # sf for spatial data; sfdep for spatial stats;\n                                            # tmap for mapping; tidyverse for wrangling/ggplot\n\n\n\n\n\n\n# --- 10.3.1 Import shapefile into R environment -----------------------------\n\nhunan &lt;- \n  st_read(                      # read a spatial layer from disk\n    dsn   = \"data/geospatial\",  # folder containing the shapefile set\n    layer = \"Hunan\"             # layer name (without .shp)\n  ) %&gt;% \n  \n  # reproject from WGS84 to UTM zone 50N (EPSG:32650)\n  # (projected CRS is recommended for spatial analysis)\n  st_transform(crs = 32650)        \n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe raw data is in WGS 84 geographic coordinates system. For geospatial analysis, it is appropriate to use projected coordinates system. In the code chunk above, st_transform() is used to transform Hunan geospatial data from WGS 84 to UTM zone 50N (i.e. EPSG: 32650).\n\n\n\n# --- 10.3.2 Import CSV into R environment ----------------------------------\n\nhunan2012 &lt;- \n  readr::read_csv(\"data/aspatial/Hunan_2012.csv\")  # read the attribute table (tabular CSV)\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# --- 10.3.3 Join attributes into the sf object ------------------------------\n\nhunan &lt;- \n  left_join(hunan, hunan2012) %&gt;%  # left join by the shared key column(s) (kept from shapefile)\n  select(1:4, 7, 15)               # keep only the columns used in this exercise (as per lesson)\n\nJoining with `by = join_by(County)`\n\n\n\n\n\nequal &lt;- tm_shape(hunan) +\n  tm_polygons(fill = \"GDPPC\",\n              fill.scale = tm_scale_intervals(\n                style = \"equal\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"GDPPC\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_polygons(fill = \"GDPPC\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"GDPPC\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Quantile interval classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDoes the plot above reveal any outliers or clusters?\n\n\n\n\nThe Equal interval map (left) spreads values evenly across 5 fixed ranges. Because the distribution of GDPPC is skewed, many counties fall into the lower classes, and the map appears very light with only a few darker areas. This makes it difficult to visually spot clear clusters or outliers, since most counties look similar and variation is compressed.\nThe Quantile map (right) divides the counties into 5 groups with equal counts per group. Here, differences are more apparent — some dark blue areas (highest quantile) are grouped together, while very light areas (lowest quantile) are also visible. This gives a stronger sense that there may be regional clustering of high and low GDPPC, but it’s only suggestive.\n\nAnswer: The plots hint at spatial variation, but do not confirm outliers or clusters statistically. Especially in the quantile map, you can see that richer counties cluster in the east, while poorer ones appear in the west/south. Still, formal spatial autocorrelation tests (Moran’s I, LISA) are needed to verify.\n\n\n\n\n\n\n\n\nNoteDoes the plot above indicate the presence of hot spots or cold spots?\n\n\n\n\nBy visual impression only, the dark blue patches in the quantile map (eastern and central counties) could be potential hot spots (high GDPPC areas adjacent to each other).\nConversely, lighter patches (western/southern counties) could be cold spots (low GDPPC areas grouped together).\nHowever, these impressions are not reliable evidence — classification maps can exaggerate or understate differences depending on method.\n\nAnswer: The quantile map suggests possible hot spots in central/eastern Hunan and cold spots in western/southern areas, but the equal interval map is less informative. These are hypotheses only — the next step is to compute Local Moran’s I and Gi* to confirm whether these patterns are statistically significant.\n\n\n\nIn summary: The classification maps are useful exploratory tools. Quantile classification reveals stronger visual contrasts, hinting at clusters of rich and poor counties. But to answer definitively about outliers or hot/cold spots, we must proceed to LISA (Local Moran’s I) and HCSA (Getis–Ord Gi) analyses.\n\n\n\n\n\n\n\n\n# Create Queen-contiguity neighbours and W-style weights (row-standardised)\nwm_q &lt;- hunan %&gt;% \n  mutate(\n    nb = st_contiguity(geometry),   # build a neighbours list from touching polygons (Queen)\n    wt = st_weights(nb, style = \"W\"), # row-standardised weights (sum of weights = 1 per region)\n    .before = 1                     # place the new columns at the beginning (professor's style)\n  )\n\n# Inspect the neighbour structure (how many neighbours per county)\nsummary(wm_q$nb)  # prints count distribution, min/max, and average links\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbour.\n\n\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nlocal_moran() function returns a matrix of values whose columns are:\n\nli: the local Moran’s I statistics\nE.li: the expectation of local moran statistic under the randomisation hypothesis\nVar.li: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\nglimpse(lisa)\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; -8.148464e-04, -9.932206e-03, -2.929856e-02, -9.398322e-0…\n$ var_ii       &lt;dbl&gt; 4.589734e-04, 1.136942e-02, 9.800257e-02, 4.136778e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.03050932, 0.33584570, 0.05529697, 0.54891935, 0.270022…\n$ p_ii         &lt;dbl&gt; 9.756609e-01, 7.369872e-01, 9.559019e-01, 5.830608e-01, 7…\n$ p_ii_sim     &lt;dbl&gt; 0.84, 0.98, 0.68, 0.52, 0.68, 0.86, 0.06, 0.06, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.42, 0.49, 0.34, 0.26, 0.34, 0.43, 0.03, 0.03, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.6165755, -0.9102915, 0.7574819, 0.9515344, 0.7445666, …\n$ kurtosis     &lt;dbl&gt; -0.5321037, 0.4113895, -0.1561211, 0.8700892, 0.1347678, …\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((22320.48 3301894,..., POLYGON ((35522.9…\n\n\n\n\n\ntm_shape(lisa) +\n  tm_polygons(fill = \"ii\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"brewer.RdBu\"),\n              fill.legend = tm_legend(\n                title = \"Local Morans'I\",\n                position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Loal Morans'I of GDPPC (Queen's method)\")\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(lisa) +\n  tm_polygons(fill = \"p_ii\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of Loal Moran's I of GDPPC (Queen's method)\")\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nii.map &lt;- tm_shape(lisa) +\n  tm_polygons(fill = \"ii\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"brewer.RdBu\"),\n              fill.legend = tm_legend(\n                title = \"Local Moran's I\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Loal Moran's I of GDPPC (Queen's method)\")\n\np_ii.map &lt;- tm_shape(lisa) +\n  tm_polygons(fill = \"p_ii\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_in(\"left\", \"bottom\")\n    )) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of Loal Moran's I of GDPPC (Queen's method)\")\n\ntmap_arrange(ii.map, p_ii.map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# --- Compute spatial lag for GDPPC (needed for Moran scatterplot) -----------\n\nlisa &lt;- lisa %&gt;%\n  mutate(lag_GDPPC = st_lag(\n    GDPPC, nb, wt),\n    .before = 1) %&gt;%\n  unnest(lag_GDPPC)\n\n\n# --- Moran scatterplot (raw values) ----------------------------------------\n\nggplot(data = lisa, \n       aes(x = GDPPC, \n           y = lag_GDPPC)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"red\") +\n  labs(x = \"GDPPC\",\n       y = \"Spatial Lag of GDPPC\",\n       title = \"Moran Scatterplot\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data = lisa, \n       aes(x = GDPPC, \n           y = lag_GDPPC, \n           color = mean)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\") +\n  geom_hline(yintercept=mean(lisa$lag_GDPPC), lty=2) + \n    geom_vline(xintercept=mean(lisa$GDPPC), lty=2) +\n  scale_color_manual(\n    values = c(\"High-High\" = \"red\", \n               \"Low-Low\" = \"blue\",\n               \"Low-High\" = \"lightblue\", \n               \"High-Low\" = \"pink\")) +\n  labs(x = \"GDPPC\",\n       y = \"Spatial Lag of GDPPC\",\n       title = \"Moran Scatterplot with LISA Quadrants\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Standardise GDPPC and its spatial lag (z-scores)\nlisa &lt;- lisa %&gt;%\n  mutate(\n    z_GDPPC     = scale(GDPPC),                # centre & scale GDPPC\n    z_lag_GDPPC = scale(lag_GDPPC),            # centre & scale spatial lag\n    .before = 1\n  )\n\n\n# Standardised Moran scatterplot with LISA quadrants\nggplot(data = lisa,\n       aes(x = z_GDPPC, y = z_lag_GDPPC, color = mean)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_hline(yintercept = mean(lisa$z_lag_GDPPC), lty = 2) +\n  geom_vline(xintercept = mean(lisa$z_GDPPC),      lty = 2) +\n  scale_color_manual(values = c(\"High-High\" = \"red\",\n                                \"Low-Low\"  = \"blue\",\n                                \"Low-High\" = \"lightblue\",\n                                \"High-Low\" = \"pink\")) +\n  labs(x = \"Standardised GDPPC\",\n       y = \"Standardised Spatial Lag of GDPPC\",\n       title = \"Standardised Moran Scatterplot with LISA Quadrants\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Significance threshold used by the professor\nsignif &lt;- 0.05\n\n\n\n\n\n# Build the display class for the LISA cluster map (keep only significant locations)\nlisa &lt;- lisa %&gt;%\n  mutate(\n    LISA_cluster = ifelse(p_ii &lt; signif, as.character(mean), \"Insignificant\"),\n    LISA_cluster = factor(LISA_cluster,\n                          levels = c(\"Insignificant\",\"Low-Low\",\"Low-High\",\"High-Low\",\"High-High\"))\n  )\n\n\nlisa_map &lt;- tm_shape(lisa) +\n  tm_polygons(\n    fill = \"LISA_cluster\",\n    fill.scale = tm_scale_categorical(\n      values = c(\"grey80\",  # Insignificant\n                 \"blue\",    # Low-Low\n                 \"lightblue\", # Low-High\n                 \"pink\",    # High-Low\n                 \"red\")     # High-High\n    ),\n    fill.legend = tm_legend(title = \"LISA Cluster\",\n                            position = tm_pos_in(\"left\",\"bottom\"))\n  ) +\n  tm_borders() +\n  tm_title(\"Local Moran's I Clusters (p &lt; 0.05)\")\n\nlisa_map\n\n\n\n\n\n\n\n\n\n# --- Side-by-side visualisation: Local Moran's I map + LISA Cluster map ----\n# Assumes `lisa` already exists from 10.4.3/10.4.4 and contains:\n#  - ii       : Local Moran's I statistic\n#  - p_ii     : permutation p-values\n#  - mean     : LISA quadrant label (High-High, Low-Low, etc.)\n#  - LISA_cluster : factor with levels c(\"Insignificant\",\"Low-Low\",\"Low-High\",\"High-Low\",\"High-High\")\n\n# 1) Choropleth of Local Moran's I (Queen's method), using a diverging palette\nii.map &lt;- tm_shape(lisa) +                                           # provide sf object\n  tm_polygons(fill = \"ii\",                                           # map the I statistic\n              fill.scale = tm_scale_intervals(                       # classing method and palette\n                style = \"pretty\",                                    # 'pretty' breaks (as in slides)\n                n = 5,                                               # 5 classes\n                values = \"brewer.RdBu\"                               # diverging red–blue palette\n              ),\n              fill.legend = tm_legend(                               # legend style\n                title = \"Local Moran's I\",\n                position = tm_pos_in(\"left\",\"bottom\")                # inside, left–bottom\n              )) +\n  tm_borders(fill_alpha = 0.5) +                                     # light border/fill alpha\n  tm_title(\"Loal Moran's I of GDPPC (Queen's method)\")               # title (kept as in slide)\n\n# 2) Choropleth of LISA clusters (significant at p &lt; 0.05)\nlisa_map &lt;- tm_shape(lisa) +                                         # same sf object\n  tm_polygons(\n    fill = \"LISA_cluster\",                                           # categorical cluster label\n    fill.scale = tm_scale_categorical(                               # fixed colours per category\n      values = c(\"grey80\",  # Insignificant\n                 \"blue\",    # Low-Low\n                 \"lightblue\", # Low-High\n                 \"pink\",    # High-Low\n                 \"red\")     # High-High\n    ),\n    fill.legend = tm_legend(                                         # legend style\n      title = \"LISA Cluster\",\n      position = tm_pos_in(\"left\",\"bottom\")\n    )\n  ) +\n  tm_borders() +                                                     # polygon borders\n  tm_title(\"Local Moran's I Clusters (p &lt; 0.05)\")                    # map title\n\n# 3) Arrange the two maps side by side for comparison\ntmap_arrange(ii.map, lisa_map, asp = 1, ncol = 2)                    # equal aspect; two columns\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical observations can you draw from the LISA map above?\n\n\n\nStatistical observations from the LISA map\nThe Local Moran’s I cluster map provides strong evidence of spatial dependence in the distribution of GDP per capita across Hunan Province. Statistically significant High–High clusters are observed in the central–eastern part of the province, where wealthy counties are located adjacent to one another. These red-shaded areas indicate local hot spots, confirming that high economic performance is not evenly dispersed but instead forms concentrated pockets of prosperity. Conversely, Low–Low clusters are identified in the western counties, representing cold spots where underdeveloped regions are spatially concentrated. These blue-shaded areas highlight localised disadvantage, suggesting that peripheral regions may be experiencing persistent economic stagnation.\nIn addition, a small number of outlier counties appear as High–Low or Low–High clusters, where individual counties deviate from the economic profile of their surrounding neighbours. Although fewer in number, these outliers are statistically meaningful, as they reveal local anomalies that would otherwise be masked in global measures of autocorrelation. A substantial portion of the province, however, remains statistically insignificant (grey areas), indicating that GDP variation in these counties does not depart significantly from spatial randomness.\nTaken together, the LISA results reinforce the conclusion that GDP per capita in Hunan is not randomly distributed. Instead, there is evidence of spatial clustering, with distinct patterns of prosperity in the east and central regions and concentrated underdevelopment in the west. These findings highlight the importance of spatial context in understanding regional inequality and provide a quantitative basis for targeted policy interventions.\n\n\n\n\n\n\n\n\n\n\n\n\nct &lt;- critical_threshold(st_geometry(hunan))   # ensure ≥ 1 neighbour\n\nWarning in spdep::knn2nb(spdep::knearneigh(pnts, k)): neighbour object has 25\nsub-graphs\n\nct\n\n[1] 60799.91\n\n\n\n\n\n\nhunan_fdw &lt;- hunan %&gt;%\n  mutate(\n    nb = include_self(st_dist_band(st_geometry(geometry), upper = ct)),\n    wt = st_weights(nb, style = \"W\"),\n    .before = 1\n  )\n\n! Polygon provided. Using point on surface.\n\n\n\n\n\n\nhunan_adw &lt;- hunan %&gt;%\n  mutate(nb = include_self(\n    st_knn(\n      st_geometry(geometry),\n      k = 6)),\n    wt = st_weights(\n      nb, style = \"W\"),\n    .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\n\n\n\n\n\nHCSA_fdw &lt;- hunan_fdw %&gt;%\n  mutate(\n    gistar = local_gstar_perm(\n      GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(gistar)\n\n\n\n\n\n\n\ntm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"gi_star\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 6,\n                values = \"brewer.rd_bu\"),\n              fill.legend = tm_legend(\n                title = \"Gi*\",\n                position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Gi* of GDPPC (Fixed Bandwidth d = 60799.91m)\")\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"p_sim\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"simulated p-value\",\n      position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of local Gi* of GDPPC (Fixed distance)\")\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGi_star_map &lt;- tm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"gi_star\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"-brewer.rd_bu\"),\n              fill.legend = tm_legend(\n                title = \"local Gi*\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Local Gi* of GDPPC\")\n\np_values_map &lt;- tm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"p_sim\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n                values = \"-brewer.reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_in(\"left\", \"bottom\")\n    )) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of local Gi* of GDPPC (fixed distance)\")\n\ntmap_arrange(Gi_star_map, p_values_map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHCSA_fdw &lt;- HCSA_fdw %&gt;%\n  mutate(HCSA_cluster = case_when(\n    p_sim &gt; 0.05 ~ \"Insignificant\",\n    p_sim &lt;= 0.05 & cluster == \"High\" ~ \"Hot spot\",\n    p_sim &lt;= 0.05 & cluster == \"Low\"  ~ \"Cold spot\",\n    TRUE ~ \"Other\"),\n    HCSA_cluster = factor(\n      HCSA_cluster,\n      levels = c(\"Insignificant\", \"Hot spot\", \"Cold spot\")\n    ),\n    .before = 1\n  )\n\n\nHCSA_map &lt;- tm_shape(HCSA_fdw) + \n  tm_polygons(\n    fill = \"HCSA_cluster\",\n    fill.scale = tm_scale_categorical(\n      values = c(\n        \"grey80\",      # Insignificant\n        \"red\",        # Low-Low\n        \"blue\"          # High-High\n      )\n    ),\n    fill.legend = tm_legend(\n      title = \"HSCA Cluster\",\n      position = tm_pos_in(\"left\", \"bottom\"))\n  ) +\n  tm_borders() +\n  tm_title(\"HCSA Clusters (p &lt; 0.05)\")\nHCSA_map\n\n\n\n\n\n\n\n\n\ntmap_arrange(Gi_star_map, HCSA_map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical observations can you draw from the HCSA map above?\n\n\n\nStatistical observations from the HCSA map\nThe Hot Spot and Cold Spot Analysis (HCSA) using the Getis-Ord Gi* statistic reveals statistically significant spatial clusters of GDP per capita across Hunan Province. The hot spots (red areas) are clearly concentrated in the eastern and central counties, where GDP per capita values are significantly higher than would be expected under spatial randomness. These locations are not only economically prosperous on their own but are also surrounded by neighbouring counties with similarly high values, reinforcing spatial clustering of wealth.\nIn contrast, the cold spots (blue areas) are located primarily in the western and southern parts of the province. These areas exhibit significantly low GDP per capita values and are surrounded by other low-value counties, forming concentrated zones of economic disadvantage. The identification of cold spots highlights the persistence of underdevelopment in certain regions, particularly those more remote from the provincial economic core.\nThe remaining counties (grey) are classified as statistically insignificant, meaning their GDP per capita patterns do not deviate sufficiently from spatial randomness to be considered clustered.\nOverall, the Gi* analysis provides complementary evidence to the LISA results: economic prosperity in Hunan is highly clustered in the east and central regions, while underdevelopment is concentrated in the west and periphery. This reinforces the presence of spatial inequality, suggesting the need for regionally targeted development strategies to reduce the widening gap between hot spots of prosperity and cold spots of persistent poverty."
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html#overview",
    "href": "Hands-on_Ex05/hand-on_ex05b.html#overview",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "We will import geospatial and aspatial data for Hunan Province (county level), join attributes, visualise a regional indicator (GDP per capita, GDPPC), build contiguity and distance spatial weights, and compute two local spatial statistics:\n\nLocal Moran’s I (LISA) for cluster/outlier detection.\nGetis–Ord Gi* for Hot/Cold Spot analysis (HCSA).\n\nWe will then create choropleths for I values, \\(p\\)-values, LISA clusters (\\(p &lt; 0.05%\\)), and HCSA clusters (\\(p &lt; 0.05\\)), plus Moran scatterplots (raw and standardised)."
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html#getting-started",
    "href": "Hands-on_Ex05/hand-on_ex05b.html#getting-started",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "In spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level (Geospatial). This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv (Aspatial): This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\n\n# load packages (installs missing ones, then attaches)\npacman::p_load(sf, sfdep, tmap, tidyverse)  # sf for spatial data; sfdep for spatial stats;\n                                            # tmap for mapping; tidyverse for wrangling/ggplot"
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html#getting-the-data-into-r",
    "href": "Hands-on_Ex05/hand-on_ex05b.html#getting-the-data-into-r",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "# --- 10.3.1 Import shapefile into R environment -----------------------------\n\nhunan &lt;- \n  st_read(                      # read a spatial layer from disk\n    dsn   = \"data/geospatial\",  # folder containing the shapefile set\n    layer = \"Hunan\"             # layer name (without .shp)\n  ) %&gt;% \n  \n  # reproject from WGS84 to UTM zone 50N (EPSG:32650)\n  # (projected CRS is recommended for spatial analysis)\n  st_transform(crs = 32650)        \n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe raw data is in WGS 84 geographic coordinates system. For geospatial analysis, it is appropriate to use projected coordinates system. In the code chunk above, st_transform() is used to transform Hunan geospatial data from WGS 84 to UTM zone 50N (i.e. EPSG: 32650).\n\n\n\n# --- 10.3.2 Import CSV into R environment ----------------------------------\n\nhunan2012 &lt;- \n  readr::read_csv(\"data/aspatial/Hunan_2012.csv\")  # read the attribute table (tabular CSV)\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# --- 10.3.3 Join attributes into the sf object ------------------------------\n\nhunan &lt;- \n  left_join(hunan, hunan2012) %&gt;%  # left join by the shared key column(s) (kept from shapefile)\n  select(1:4, 7, 15)               # keep only the columns used in this exercise (as per lesson)\n\nJoining with `by = join_by(County)`\n\n\n\n\n\nequal &lt;- tm_shape(hunan) +\n  tm_polygons(fill = \"GDPPC\",\n              fill.scale = tm_scale_intervals(\n                style = \"equal\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"GDPPC\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_polygons(fill = \"GDPPC\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"GDPPC\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Quantile interval classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDoes the plot above reveal any outliers or clusters?\n\n\n\n\nThe Equal interval map (left) spreads values evenly across 5 fixed ranges. Because the distribution of GDPPC is skewed, many counties fall into the lower classes, and the map appears very light with only a few darker areas. This makes it difficult to visually spot clear clusters or outliers, since most counties look similar and variation is compressed.\nThe Quantile map (right) divides the counties into 5 groups with equal counts per group. Here, differences are more apparent — some dark blue areas (highest quantile) are grouped together, while very light areas (lowest quantile) are also visible. This gives a stronger sense that there may be regional clustering of high and low GDPPC, but it’s only suggestive.\n\nAnswer: The plots hint at spatial variation, but do not confirm outliers or clusters statistically. Especially in the quantile map, you can see that richer counties cluster in the east, while poorer ones appear in the west/south. Still, formal spatial autocorrelation tests (Moran’s I, LISA) are needed to verify.\n\n\n\n\n\n\n\n\nNoteDoes the plot above indicate the presence of hot spots or cold spots?\n\n\n\n\nBy visual impression only, the dark blue patches in the quantile map (eastern and central counties) could be potential hot spots (high GDPPC areas adjacent to each other).\nConversely, lighter patches (western/southern counties) could be cold spots (low GDPPC areas grouped together).\nHowever, these impressions are not reliable evidence — classification maps can exaggerate or understate differences depending on method.\n\nAnswer: The quantile map suggests possible hot spots in central/eastern Hunan and cold spots in western/southern areas, but the equal interval map is less informative. These are hypotheses only — the next step is to compute Local Moran’s I and Gi* to confirm whether these patterns are statistically significant.\n\n\n\nIn summary: The classification maps are useful exploratory tools. Quantile classification reveals stronger visual contrasts, hinting at clusters of rich and poor counties. But to answer definitively about outliers or hot/cold spots, we must proceed to LISA (Local Moran’s I) and HCSA (Getis–Ord Gi) analyses."
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html#local-indicators-of-spatial-association-lisa",
    "href": "Hands-on_Ex05/hand-on_ex05b.html#local-indicators-of-spatial-association-lisa",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "# Create Queen-contiguity neighbours and W-style weights (row-standardised)\nwm_q &lt;- hunan %&gt;% \n  mutate(\n    nb = st_contiguity(geometry),   # build a neighbours list from touching polygons (Queen)\n    wt = st_weights(nb, style = \"W\"), # row-standardised weights (sum of weights = 1 per region)\n    .before = 1                     # place the new columns at the beginning (professor's style)\n  )\n\n# Inspect the neighbour structure (how many neighbours per county)\nsummary(wm_q$nb)  # prints count distribution, min/max, and average links\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbour.\n\n\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nlocal_moran() function returns a matrix of values whose columns are:\n\nli: the local Moran’s I statistics\nE.li: the expectation of local moran statistic under the randomisation hypothesis\nVar.li: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\nglimpse(lisa)\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; -8.148464e-04, -9.932206e-03, -2.929856e-02, -9.398322e-0…\n$ var_ii       &lt;dbl&gt; 4.589734e-04, 1.136942e-02, 9.800257e-02, 4.136778e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.03050932, 0.33584570, 0.05529697, 0.54891935, 0.270022…\n$ p_ii         &lt;dbl&gt; 9.756609e-01, 7.369872e-01, 9.559019e-01, 5.830608e-01, 7…\n$ p_ii_sim     &lt;dbl&gt; 0.84, 0.98, 0.68, 0.52, 0.68, 0.86, 0.06, 0.06, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.42, 0.49, 0.34, 0.26, 0.34, 0.43, 0.03, 0.03, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.6165755, -0.9102915, 0.7574819, 0.9515344, 0.7445666, …\n$ kurtosis     &lt;dbl&gt; -0.5321037, 0.4113895, -0.1561211, 0.8700892, 0.1347678, …\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((22320.48 3301894,..., POLYGON ((35522.9…\n\n\n\n\n\ntm_shape(lisa) +\n  tm_polygons(fill = \"ii\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"brewer.RdBu\"),\n              fill.legend = tm_legend(\n                title = \"Local Morans'I\",\n                position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Loal Morans'I of GDPPC (Queen's method)\")\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(lisa) +\n  tm_polygons(fill = \"p_ii\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of Loal Moran's I of GDPPC (Queen's method)\")\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nii.map &lt;- tm_shape(lisa) +\n  tm_polygons(fill = \"ii\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"brewer.RdBu\"),\n              fill.legend = tm_legend(\n                title = \"Local Moran's I\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Loal Moran's I of GDPPC (Queen's method)\")\n\np_ii.map &lt;- tm_shape(lisa) +\n  tm_polygons(fill = \"p_ii\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_in(\"left\", \"bottom\")\n    )) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of Loal Moran's I of GDPPC (Queen's method)\")\n\ntmap_arrange(ii.map, p_ii.map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# --- Compute spatial lag for GDPPC (needed for Moran scatterplot) -----------\n\nlisa &lt;- lisa %&gt;%\n  mutate(lag_GDPPC = st_lag(\n    GDPPC, nb, wt),\n    .before = 1) %&gt;%\n  unnest(lag_GDPPC)\n\n\n# --- Moran scatterplot (raw values) ----------------------------------------\n\nggplot(data = lisa, \n       aes(x = GDPPC, \n           y = lag_GDPPC)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"red\") +\n  labs(x = \"GDPPC\",\n       y = \"Spatial Lag of GDPPC\",\n       title = \"Moran Scatterplot\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data = lisa, \n       aes(x = GDPPC, \n           y = lag_GDPPC, \n           color = mean)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\") +\n  geom_hline(yintercept=mean(lisa$lag_GDPPC), lty=2) + \n    geom_vline(xintercept=mean(lisa$GDPPC), lty=2) +\n  scale_color_manual(\n    values = c(\"High-High\" = \"red\", \n               \"Low-Low\" = \"blue\",\n               \"Low-High\" = \"lightblue\", \n               \"High-Low\" = \"pink\")) +\n  labs(x = \"GDPPC\",\n       y = \"Spatial Lag of GDPPC\",\n       title = \"Moran Scatterplot with LISA Quadrants\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Standardise GDPPC and its spatial lag (z-scores)\nlisa &lt;- lisa %&gt;%\n  mutate(\n    z_GDPPC     = scale(GDPPC),                # centre & scale GDPPC\n    z_lag_GDPPC = scale(lag_GDPPC),            # centre & scale spatial lag\n    .before = 1\n  )\n\n\n# Standardised Moran scatterplot with LISA quadrants\nggplot(data = lisa,\n       aes(x = z_GDPPC, y = z_lag_GDPPC, color = mean)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_hline(yintercept = mean(lisa$z_lag_GDPPC), lty = 2) +\n  geom_vline(xintercept = mean(lisa$z_GDPPC),      lty = 2) +\n  scale_color_manual(values = c(\"High-High\" = \"red\",\n                                \"Low-Low\"  = \"blue\",\n                                \"Low-High\" = \"lightblue\",\n                                \"High-Low\" = \"pink\")) +\n  labs(x = \"Standardised GDPPC\",\n       y = \"Standardised Spatial Lag of GDPPC\",\n       title = \"Standardised Moran Scatterplot with LISA Quadrants\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Significance threshold used by the professor\nsignif &lt;- 0.05\n\n\n\n\n\n# Build the display class for the LISA cluster map (keep only significant locations)\nlisa &lt;- lisa %&gt;%\n  mutate(\n    LISA_cluster = ifelse(p_ii &lt; signif, as.character(mean), \"Insignificant\"),\n    LISA_cluster = factor(LISA_cluster,\n                          levels = c(\"Insignificant\",\"Low-Low\",\"Low-High\",\"High-Low\",\"High-High\"))\n  )\n\n\nlisa_map &lt;- tm_shape(lisa) +\n  tm_polygons(\n    fill = \"LISA_cluster\",\n    fill.scale = tm_scale_categorical(\n      values = c(\"grey80\",  # Insignificant\n                 \"blue\",    # Low-Low\n                 \"lightblue\", # Low-High\n                 \"pink\",    # High-Low\n                 \"red\")     # High-High\n    ),\n    fill.legend = tm_legend(title = \"LISA Cluster\",\n                            position = tm_pos_in(\"left\",\"bottom\"))\n  ) +\n  tm_borders() +\n  tm_title(\"Local Moran's I Clusters (p &lt; 0.05)\")\n\nlisa_map\n\n\n\n\n\n\n\n\n\n# --- Side-by-side visualisation: Local Moran's I map + LISA Cluster map ----\n# Assumes `lisa` already exists from 10.4.3/10.4.4 and contains:\n#  - ii       : Local Moran's I statistic\n#  - p_ii     : permutation p-values\n#  - mean     : LISA quadrant label (High-High, Low-Low, etc.)\n#  - LISA_cluster : factor with levels c(\"Insignificant\",\"Low-Low\",\"Low-High\",\"High-Low\",\"High-High\")\n\n# 1) Choropleth of Local Moran's I (Queen's method), using a diverging palette\nii.map &lt;- tm_shape(lisa) +                                           # provide sf object\n  tm_polygons(fill = \"ii\",                                           # map the I statistic\n              fill.scale = tm_scale_intervals(                       # classing method and palette\n                style = \"pretty\",                                    # 'pretty' breaks (as in slides)\n                n = 5,                                               # 5 classes\n                values = \"brewer.RdBu\"                               # diverging red–blue palette\n              ),\n              fill.legend = tm_legend(                               # legend style\n                title = \"Local Moran's I\",\n                position = tm_pos_in(\"left\",\"bottom\")                # inside, left–bottom\n              )) +\n  tm_borders(fill_alpha = 0.5) +                                     # light border/fill alpha\n  tm_title(\"Loal Moran's I of GDPPC (Queen's method)\")               # title (kept as in slide)\n\n# 2) Choropleth of LISA clusters (significant at p &lt; 0.05)\nlisa_map &lt;- tm_shape(lisa) +                                         # same sf object\n  tm_polygons(\n    fill = \"LISA_cluster\",                                           # categorical cluster label\n    fill.scale = tm_scale_categorical(                               # fixed colours per category\n      values = c(\"grey80\",  # Insignificant\n                 \"blue\",    # Low-Low\n                 \"lightblue\", # Low-High\n                 \"pink\",    # High-Low\n                 \"red\")     # High-High\n    ),\n    fill.legend = tm_legend(                                         # legend style\n      title = \"LISA Cluster\",\n      position = tm_pos_in(\"left\",\"bottom\")\n    )\n  ) +\n  tm_borders() +                                                     # polygon borders\n  tm_title(\"Local Moran's I Clusters (p &lt; 0.05)\")                    # map title\n\n# 3) Arrange the two maps side by side for comparison\ntmap_arrange(ii.map, lisa_map, asp = 1, ncol = 2)                    # equal aspect; two columns\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"brewer.RdBu\" is\nnamed \"rd_bu\" (in long format \"brewer.rd_bu\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical observations can you draw from the LISA map above?\n\n\n\nStatistical observations from the LISA map\nThe Local Moran’s I cluster map provides strong evidence of spatial dependence in the distribution of GDP per capita across Hunan Province. Statistically significant High–High clusters are observed in the central–eastern part of the province, where wealthy counties are located adjacent to one another. These red-shaded areas indicate local hot spots, confirming that high economic performance is not evenly dispersed but instead forms concentrated pockets of prosperity. Conversely, Low–Low clusters are identified in the western counties, representing cold spots where underdeveloped regions are spatially concentrated. These blue-shaded areas highlight localised disadvantage, suggesting that peripheral regions may be experiencing persistent economic stagnation.\nIn addition, a small number of outlier counties appear as High–Low or Low–High clusters, where individual counties deviate from the economic profile of their surrounding neighbours. Although fewer in number, these outliers are statistically meaningful, as they reveal local anomalies that would otherwise be masked in global measures of autocorrelation. A substantial portion of the province, however, remains statistically insignificant (grey areas), indicating that GDP variation in these counties does not depart significantly from spatial randomness.\nTaken together, the LISA results reinforce the conclusion that GDP per capita in Hunan is not randomly distributed. Instead, there is evidence of spatial clustering, with distinct patterns of prosperity in the east and central regions and concentrated underdevelopment in the west. These findings highlight the importance of spatial context in understanding regional inequality and provide a quantitative basis for targeted policy interventions."
  },
  {
    "objectID": "Hands-on_Ex05/hand-on_ex05b.html#hot-spots-and-cold-spots-analysis-hcsa",
    "href": "Hands-on_Ex05/hand-on_ex05b.html#hot-spots-and-cold-spots-analysis-hcsa",
    "title": "Hands-on Ex5b",
    "section": "",
    "text": "ct &lt;- critical_threshold(st_geometry(hunan))   # ensure ≥ 1 neighbour\n\nWarning in spdep::knn2nb(spdep::knearneigh(pnts, k)): neighbour object has 25\nsub-graphs\n\nct\n\n[1] 60799.91\n\n\n\n\n\n\nhunan_fdw &lt;- hunan %&gt;%\n  mutate(\n    nb = include_self(st_dist_band(st_geometry(geometry), upper = ct)),\n    wt = st_weights(nb, style = \"W\"),\n    .before = 1\n  )\n\n! Polygon provided. Using point on surface.\n\n\n\n\n\n\nhunan_adw &lt;- hunan %&gt;%\n  mutate(nb = include_self(\n    st_knn(\n      st_geometry(geometry),\n      k = 6)),\n    wt = st_weights(\n      nb, style = \"W\"),\n    .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\n\n\n\n\n\nHCSA_fdw &lt;- hunan_fdw %&gt;%\n  mutate(\n    gistar = local_gstar_perm(\n      GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(gistar)\n\n\n\n\n\n\n\ntm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"gi_star\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 6,\n                values = \"brewer.rd_bu\"),\n              fill.legend = tm_legend(\n                title = \"Gi*\",\n                position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Gi* of GDPPC (Fixed Bandwidth d = 60799.91m)\")\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"p_sim\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n                values = \"-brewer.Reds\"),\n              fill.legend = tm_legend(\n                title = \"simulated p-value\",\n      position = tm_pos_out())) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of local Gi* of GDPPC (Fixed distance)\")\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"-brewer.Reds\" is\nnamed \"reds\" (in long format \"brewer.reds\")\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGi_star_map &lt;- tm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"gi_star\",\n              fill.scale = tm_scale_intervals(\n                style = \"pretty\",\n                n = 5,\n                values = \"-brewer.rd_bu\"),\n              fill.legend = tm_legend(\n                title = \"local Gi*\",\n                position = tm_pos_in(\n                  \"left\", \"bottom\"))) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Local Gi* of GDPPC\")\n\np_values_map &lt;- tm_shape(HCSA_fdw) +\n  tm_polygons(fill = \"p_sim\", \n              fill.scale = tm_scale_intervals(\n                breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n                values = \"-brewer.reds\"),\n              fill.legend = tm_legend(\n                title = \"p-value\",\n      position = tm_pos_in(\"left\", \"bottom\")\n    )) + \n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"p-values of local Gi* of GDPPC (fixed distance)\")\n\ntmap_arrange(Gi_star_map, p_values_map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHCSA_fdw &lt;- HCSA_fdw %&gt;%\n  mutate(HCSA_cluster = case_when(\n    p_sim &gt; 0.05 ~ \"Insignificant\",\n    p_sim &lt;= 0.05 & cluster == \"High\" ~ \"Hot spot\",\n    p_sim &lt;= 0.05 & cluster == \"Low\"  ~ \"Cold spot\",\n    TRUE ~ \"Other\"),\n    HCSA_cluster = factor(\n      HCSA_cluster,\n      levels = c(\"Insignificant\", \"Hot spot\", \"Cold spot\")\n    ),\n    .before = 1\n  )\n\n\nHCSA_map &lt;- tm_shape(HCSA_fdw) + \n  tm_polygons(\n    fill = \"HCSA_cluster\",\n    fill.scale = tm_scale_categorical(\n      values = c(\n        \"grey80\",      # Insignificant\n        \"red\",        # Low-Low\n        \"blue\"          # High-High\n      )\n    ),\n    fill.legend = tm_legend(\n      title = \"HSCA Cluster\",\n      position = tm_pos_in(\"left\", \"bottom\"))\n  ) +\n  tm_borders() +\n  tm_title(\"HCSA Clusters (p &lt; 0.05)\")\nHCSA_map\n\n\n\n\n\n\n\n\n\ntmap_arrange(Gi_star_map, HCSA_map, asp=1, ncol=2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat statistical observations can you draw from the HCSA map above?\n\n\n\nStatistical observations from the HCSA map\nThe Hot Spot and Cold Spot Analysis (HCSA) using the Getis-Ord Gi* statistic reveals statistically significant spatial clusters of GDP per capita across Hunan Province. The hot spots (red areas) are clearly concentrated in the eastern and central counties, where GDP per capita values are significantly higher than would be expected under spatial randomness. These locations are not only economically prosperous on their own but are also surrounded by neighbouring counties with similarly high values, reinforcing spatial clustering of wealth.\nIn contrast, the cold spots (blue areas) are located primarily in the western and southern parts of the province. These areas exhibit significantly low GDP per capita values and are surrounded by other low-value counties, forming concentrated zones of economic disadvantage. The identification of cold spots highlights the persistence of underdevelopment in certain regions, particularly those more remote from the provincial economic core.\nThe remaining counties (grey) are classified as statistically insignificant, meaning their GDP per capita patterns do not deviate sufficiently from spatial randomness to be considered clustered.\nOverall, the Gi* analysis provides complementary evidence to the LISA results: economic prosperity in Hunan is highly clustered in the east and central regions, while underdevelopment is concentrated in the west and periphery. This reinforces the presence of spatial inequality, suggesting the need for regionally targeted development strategies to reduce the widening gap between hot spots of prosperity and cold spots of persistent poverty."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html",
    "href": "In-Class_Ex05/in-class_ex05.html",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "Introducing sfdep:\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”\n\n\n\n\n\n\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\npacman::p_load(sf, sfdep, tmap, tidyverse)\n\n\n\n\n\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nhunan &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nImport Hunan_2012.csv into R environment as an tibble data frame.\n\nhunan2012 &lt;- read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)\n\n\n\n\n\nPlot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"Multiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\n\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\n\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\n\n\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\n\n\nIt is always a good practice to use set.seed() before performing simulation. This is o ensure that the computation is reproducible.\n\nset.seed(1234)\n\n\n\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\nThe statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed.\n\n\n\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\n\n\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii     var_ii    z_ii    p_ii p_ii_sim p_folded_sim skewness\n      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.00147  0.00177   0.000418   -0.158  0.874       0.82         0.41   -0.812\n 2  0.0259   0.00641   0.0105      0.190  0.849       0.96         0.48   -1.09 \n 3 -0.0120  -0.0374    0.102       0.0796 0.937       0.76         0.38    0.824\n 4  0.00102 -0.0000349 0.00000437  0.506  0.613       0.64         0.32    1.04 \n 5  0.0148  -0.00340   0.00165     0.449  0.654       0.5          0.25    1.64 \n 6 -0.0388  -0.00339   0.00545    -0.480  0.631       0.82         0.41    0.614\n 7  3.37    -0.198     1.41        3.00   0.00266     0.08         0.04    1.46 \n 8  1.56    -0.265     0.804       2.04   0.0417      0.08         0.04    0.459\n 9  4.42     0.0450    1.79        3.27   0.00108     0.02         0.01    0.746\n10 -0.399   -0.0505    0.0859     -1.19   0.234       0.28         0.14   -0.685\n# ℹ 78 more rows\n# ℹ 13 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, NAME_2 &lt;chr&gt;, ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;,\n#   ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;, geometry &lt;POLYGON [°]&gt;\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of GDPPC\",\n    main.title.size = 2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\n\nFor effective comparison, it will be better for us to plot both maps next to each other.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\ntmap_arrange(map1, map2, ncol = 2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.\n\n\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wts = st_inverse_distance(nb, \n                              geometry, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\nSince we are going to compute Gi* statistics, include_self() is used.\n\n\n\n\n\nNow, we will compute the local Gi* by using the code chunk below.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n    gi_star cluster     e_gi  var_gi std_dev p_value p_sim p_folded_sim skewness\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.261   Low     0.00126  1.07e-7  0.283  7.78e-1  0.66         0.33    0.783\n 2 -0.276   Low     0.000969 4.76e-8 -0.123  9.02e-1  0.98         0.49    0.713\n 3  0.00573 High    0.00156  2.53e-7 -0.0571 9.54e-1  0.78         0.39    0.972\n 4  0.528   High    0.00155  2.97e-7  0.321  7.48e-1  0.56         0.28    0.942\n 5  0.466   High    0.00137  2.76e-7  0.386  7.00e-1  0.52         0.26    1.32 \n 6 -0.445   High    0.000992 7.08e-8 -0.588  5.57e-1  0.68         0.34    0.692\n 7  2.99    High    0.000700 4.05e-8  3.13   1.74e-3  0.04         0.02    0.975\n 8  2.04    High    0.00152  1.58e-7  1.77   7.59e-2  0.16         0.08    1.26 \n 9  4.42    High    0.00130  1.18e-7  4.22   2.39e-5  0.02         0.01    1.20 \n10  1.21    Low     0.00175  1.25e-7  1.49   1.36e-1  0.18         0.09    0.408\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\n\n\nIn the code chunk below, tmap functions are used to plot the local Gi* (i.e. gi_star) at the province level.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, tmap functions are used to plot the p-values of local Gi* (i.e. p_sim) at the province level.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n\nFor effective comparison, you can plot both maps next to each other as shown below.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\ntmap_arrange(map1, map2, ncol = 2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#overview",
    "href": "In-Class_Ex05/in-class_ex05.html#overview",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "Introducing sfdep:\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#getting-started",
    "href": "In-Class_Ex05/in-class_ex05.html#getting-started",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "Four R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#the-data",
    "href": "In-Class_Ex05/in-class_ex05.html#the-data",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "For the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nhunan &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#importing-attribute-table",
    "href": "In-Class_Ex05/in-class_ex05.html#importing-attribute-table",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "Import Hunan_2012.csv into R environment as an tibble data frame.\n\nhunan2012 &lt;- read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#combining-both-data-frame-by-using-left-join",
    "href": "In-Class_Ex05/in-class_ex05.html#combining-both-data-frame-by-using-left-join",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "hunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#plotting-a-choropleth-map",
    "href": "In-Class_Ex05/in-class_ex05.html#plotting-a-choropleth-map",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "Plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"Multiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#global-measures-of-spatial-association",
    "href": "In-Class_Ex05/in-class_ex05.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "wm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\n\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\n\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\n\n\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\n\n\nIt is always a good practice to use set.seed() before performing simulation. This is o ensure that the computation is reproducible.\n\nset.seed(1234)\n\n\n\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\nThe statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#lisa-map",
    "href": "In-Class_Ex05/in-class_ex05.html#lisa-map",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\n\n\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii     var_ii    z_ii    p_ii p_ii_sim p_folded_sim skewness\n      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.00147  0.00177   0.000418   -0.158  0.874       0.82         0.41   -0.812\n 2  0.0259   0.00641   0.0105      0.190  0.849       0.96         0.48   -1.09 \n 3 -0.0120  -0.0374    0.102       0.0796 0.937       0.76         0.38    0.824\n 4  0.00102 -0.0000349 0.00000437  0.506  0.613       0.64         0.32    1.04 \n 5  0.0148  -0.00340   0.00165     0.449  0.654       0.5          0.25    1.64 \n 6 -0.0388  -0.00339   0.00545    -0.480  0.631       0.82         0.41    0.614\n 7  3.37    -0.198     1.41        3.00   0.00266     0.08         0.04    1.46 \n 8  1.56    -0.265     0.804       2.04   0.0417      0.08         0.04    0.459\n 9  4.42     0.0450    1.79        3.27   0.00108     0.02         0.01    0.746\n10 -0.399   -0.0505    0.0859     -1.19   0.234       0.28         0.14   -0.685\n# ℹ 78 more rows\n# ℹ 13 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, NAME_2 &lt;chr&gt;, ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;,\n#   ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;, geometry &lt;POLYGON [°]&gt;\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of GDPPC\",\n    main.title.size = 2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\n\nFor effective comparison, it will be better for us to plot both maps next to each other.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\ntmap_arrange(map1, map2, ncol = 2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-Class_Ex05/in-class_ex05.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class Exercise 5a: Global and Local Measures of Spatial Autocorrelation using sfdep methods",
    "section": "",
    "text": "HCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.\n\n\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wts = st_inverse_distance(nb, \n                              geometry, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\nSince we are going to compute Gi* statistics, include_self() is used.\n\n\n\n\n\nNow, we will compute the local Gi* by using the code chunk below.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n    gi_star cluster     e_gi  var_gi std_dev p_value p_sim p_folded_sim skewness\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.261   Low     0.00126  1.07e-7  0.283  7.78e-1  0.66         0.33    0.783\n 2 -0.276   Low     0.000969 4.76e-8 -0.123  9.02e-1  0.98         0.49    0.713\n 3  0.00573 High    0.00156  2.53e-7 -0.0571 9.54e-1  0.78         0.39    0.972\n 4  0.528   High    0.00155  2.97e-7  0.321  7.48e-1  0.56         0.28    0.942\n 5  0.466   High    0.00137  2.76e-7  0.386  7.00e-1  0.52         0.26    1.32 \n 6 -0.445   High    0.000992 7.08e-8 -0.588  5.57e-1  0.68         0.34    0.692\n 7  2.99    High    0.000700 4.05e-8  3.13   1.74e-3  0.04         0.02    0.975\n 8  2.04    High    0.00152  1.58e-7  1.77   7.59e-2  0.16         0.08    1.26 \n 9  4.42    High    0.00130  1.18e-7  4.22   2.39e-5  0.02         0.01    1.20 \n10  1.21    Low     0.00175  1.25e-7  1.49   1.36e-1  0.18         0.09    0.408\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\n\n\nIn the code chunk below, tmap functions are used to plot the local Gi* (i.e. gi_star) at the province level.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, tmap functions are used to plot the p-values of local Gi* (i.e. p_sim) at the province level.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n\nFor effective comparison, you can plot both maps next to each other as shown below.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_view()`: use set_zoom_limits instead of set.zoom.limits[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\ntmap_arrange(map1, map2, ncol = 2)\n\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservation:\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "A spatio-temporal point process is a random collection of events identified by both time and location, such as disease cases, species sightings, or natural disasters. With the rise of geographically and temporally indexed data, analyzing these patterns has become increasingly important across many fields. In the past decade, several R methods and packages have been developed to support such analyses. This hands-on exercise demonstrates how these tools can be combined in a guided way, using forest fire events in Kepulauan Bangka Belitung, Indonesia (from 1 January to 31 December 2023) as a real-world case study to illustrate the procedures and interpretations.\n\n\n\n\n\nThe specific questions we would like to answer are:\n\nAre the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nIf the answer is NO, where and when the observed forest fire locations tend to cluster?\n\nA detailed discussion of the results, together with explicit insights and implications, is provided in Section 6.11 Discussion of Results (Answer to the Research Question).\n\n\n\n\nKnow what files you’ll use and where they come from.\n\nforestfires.csv: point events (each row = a fire). Has longitude, latitude, and date/time fields from MODIS.\nKepulauan_Bangka_Belitung shapefile: the study region polygon (administrative boundary). We’ll read only the Kepulauan Bangka Belitung subset for 2023 analyses.\n\n\n\n\nLoad all libraries used in this hands-on exercise.\n\npacman::p_load(sf,        # read, write, and transform spatial vector data\n               terra,      \n               spatstat,  # used for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc\n               sparr,     # spatio-temporal kernel density estimation (STKDE)\n               tmap,      # cartographic visualisation for quick maps\n               tidyverse  # readr (CSV), dplyr (mutate/select), ggplot, etc.\n               )\n\n\n\n\n\n\nFirst. read the Kepulauan Bangka Belitung boundary, clean geometry, and set projected CRS.\n\n# Read the shapefile from the data folder -----------------------------------------------------\nkbb_sf &lt;- st_read(dsn = \"data/rds/BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG\",    # folder containing the shapefile\n                  layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG\") %&gt;%       # shapefile base name (no .shp)\n  st_union() %&gt;%                       # dissolve internal boundaries into one polygon\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%  # drop Z/M dimensions if present (keeps 2D)\n  st_transform(crs = 32748)            # reproject to EPSG:32748 (UTM Zone 48S in meters)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG' from data source `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex03/Data/rds/BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG' \n  using driver `ESRI Shapefile'\nSimple feature collection with 391 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 105.1081 ymin: -3.416903 xmax: 108.848 ymax: -1.501757\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, convert the polygon (kb) to an observation window (owin) for point-pattern analysis.\n\n# Convert the sf polygon to an 'owin' (spatstat window) \nkbb_owin &lt;- as.owin(kbb_sf)      # turns the sf polygon into a spatstat window for ppp usage\nkbb_owin                         \n\nwindow: polygonal boundary\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n\n\n\n# Quick check that conversion succeeded\nclass(kbb_owin) \n\n[1] \"owin\"\n\n\n\n\n\n\nNext read the CSV, make it spatial, reproject to meters, and build helpful time fields.\n\nfire_sf &lt;- read_csv(\"data/rds/modis_2023_Indonesia.csv\")    %&gt;%  # load the event table\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%  # turn lon/lat cols into points (WGS84)\n  st_transform(crs = 32748)   # project to meters to match the study area\n\nRows: 52156 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (4): acq_time, satellite, instrument, daynight\ndbl  (10): latitude, longitude, brightness, scan, track, confidence, version...\ndate  (1): acq_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Add day-of-year and month fields used later\nfire_sf &lt;- fire_sf %&gt;% \n  mutate(DayofYear = yday(acq_date)) %&gt;%     # numeric day 1..365 from acquisition date\n  mutate(Month_num = month(acq_date)) %&gt;%    # numeric month 1..12\n  mutate(Month_fac = month(acq_date, label = TRUE, abbr = FALSE))  # factor month with full names (Jan..Dec)\n\n\n\n\n\n\nPlot one map with all 2023 fires over the study region\n\n# Bigger figure when rendering (Quarto/knitr)\n\n# Clip fire points so only those inside kbb_sf remain \nfire_sf &lt;- sf::st_intersection(fire_sf, kbb_sf)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Build a quick overall point map with tmap \ntmap_mode(\"plot\")                                         # use static plotting mode\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(kbb_sf) +                                        # add study area polygon layer\n  tm_polygons(col = \"grey85\", border.col = \"grey40\") +    # light fill, subtle border\n  tm_shape(fire_sf) +                                     # add fire points\n  tm_symbols(size = 0.5, col = \"red\") +                   # small red dots\n  tm_credits(\n    \"Forest Fire Points in 2023 (Kepulauan Bangka Belitung)\",\n    position = c(\"center\", \"top\"),    # center horizontally, top vertically\n    size = 1.2,                       # enlarge title text\n    fontface = 2                      # bold\n  ) +\n  tm_layout(\n    frame = TRUE,\n    # outer.margins = c(0.1, 0.05, 0.1, 0.05) # top, right, bottom, left (gives space for title)\n    inner.margins = 0.05                      # uncomment to maximize map area even more\n  )\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n\n\n\n\n\n\n\n\n\n\n\n\nNext, 12 small multiples (one map per month) like the practical guide.\n\n# Faceted monthly point maps \ntm_shape(kbb_sf) +\n  tm_polygons(col = \"grey85\", border.col = \"grey50\") +\n  tm_shape(fire_sf) +\n  tm_symbols(size = 0.1, col = \"red\") +\n  tm_facets(by = \"Month_fac\", ncol = 4) +                          # 4 columns x 3 rows facet layout\n  tm_layout(\n            frame = TRUE)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we will learn how to compute STKDE by using spattemp.density() of sparr package.\n\n\nWe need to keep only the mark (month) needed by ppp() and the geometry.\n\n# Keep only the monthly mark and geometry for ppp creation \nfire_month &lt;- fire_sf %&gt;% \n  dplyr::select(Month_num)     # ppp requires a numeric/character mark column and geometry\n\n\n\n\nNext, we need to convert to a spatstat ppp (planar point pattern) object.\n\n# Convert sf points to ppp (planar point pattern) \nfire_month_ppp &lt;- as.ppp(fire_month)   # create ppp with coordinates and 'Month_num' as the mark\nfire_month_ppp   # print basic info to confirm number of points/window box\n\nMarked planar point pattern: 899 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 862416.4] x [9642001, 9828767] units\n\n\n\n# Sanity checks on the ppp \nsummary(fire_month_ppp)   # detailed summary (intensity, mark stats, window box)\n\nMarked planar point pattern:  899 points\nAverage intensity 1.412198e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.644  10.000  12.000 \n\nWindow: rectangle = [521564.1, 862416.4] x [9642001, 9828767] units\n                    (340900 x 186800 units)\nWindow area = 63659700000 square units\n\nany(duplicated(fire_month_ppp)) # TRUE if any exact duplicate point events; expect FALSE per notes\n\n[1] FALSE\n\n\n\n# Check if there are duplicated point events by using the code chunk below.\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n\nNext, we need to clip/assign the point pattern to the study window.\n\n# Attach/clip the ppp to our study-area window \nfire_month_owin &lt;- fire_month_ppp[kbb_owin]  # ensure points are analysed within the polygon boundary\nsummary(fire_month_owin)   # verify marks, counts, window area\n\nMarked planar point pattern:  899 points\nAverage intensity 5.380183e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.644  10.000  12.000 \n\nWindow: polygonal boundary\n319 separate polygons (40 holes)\n                   vertices         area relative.area\npolygon 1              9026  1.15464e+10      6.91e-01\npolygon 2                19  1.26217e+04      7.55e-07\npolygon 3 (hole)          3 -1.80096e-01     -1.08e-11\npolygon 4 (hole)          3 -9.26944e-01     -5.55e-11\npolygon 5 (hole)          3 -1.07382e+03     -6.43e-08\npolygon 6 (hole)          3 -6.41537e+02     -3.84e-08\npolygon 7                23  9.26156e+04      5.54e-06\npolygon 8                30  3.37146e+04      2.02e-06\npolygon 9                54  3.25211e+05      1.95e-05\npolygon 10               33  1.30244e+06      7.79e-05\npolygon 11 (hole)         4 -9.57353e+02     -5.73e-08\npolygon 12 (hole)         4 -2.84738e+02     -1.70e-08\npolygon 13 (hole)         3 -1.38747e+02     -8.30e-09\npolygon 14 (hole)         4 -5.31445e+02     -3.18e-08\npolygon 15 (hole)         3 -6.42241e+02     -3.84e-08\npolygon 16 (hole)         6 -1.13809e+02     -6.81e-09\npolygon 17 (hole)         4 -2.10482e+02     -1.26e-08\npolygon 18 (hole)         4 -9.54892e+01     -5.71e-09\npolygon 19 (hole)         4 -6.47895e+01     -3.88e-09\npolygon 20 (hole)         4 -7.10903e+01     -4.25e-09\npolygon 21 (hole)         4 -2.18071e+02     -1.31e-08\npolygon 22 (hole)         4 -4.44993e+01     -2.66e-09\npolygon 23 (hole)         4 -4.90009e+01     -2.93e-09\npolygon 24 (hole)         4 -6.48299e+01     -3.88e-09\npolygon 25 (hole)         4 -2.36910e+02     -1.42e-08\npolygon 26 (hole)         4 -3.07205e+01     -1.84e-09\npolygon 27 (hole)         4 -5.24443e+01     -3.14e-09\npolygon 28 (hole)         5 -6.64298e+01     -3.98e-09\npolygon 29 (hole)         4 -6.08287e+01     -3.64e-09\npolygon 30 (hole)         4 -1.12681e+02     -6.74e-09\npolygon 31              132  1.16480e+06      6.97e-05\npolygon 32 (hole)         4 -9.56157e+01     -5.72e-09\npolygon 33               68  1.91291e+05      1.14e-05\npolygon 34 (hole)         4 -5.31171e+02     -3.18e-08\npolygon 35 (hole)         3 -1.76143e+00     -1.05e-10\npolygon 36              116  1.51984e+05      9.10e-06\npolygon 37               95  1.14322e+05      6.84e-06\npolygon 38               21  4.51914e+03      2.70e-07\npolygon 39               24  1.18511e+04      7.09e-07\npolygon 40              130  1.52296e+05      9.11e-06\npolygon 41               36  1.07878e+04      6.46e-07\npolygon 42               54  3.46932e+05      2.08e-05\npolygon 43               71  2.15798e+05      1.29e-05\npolygon 44               12  1.76977e+03      1.06e-07\npolygon 45               75  1.05557e+05      6.32e-06\npolygon 46               17  4.22421e+03      2.53e-07\npolygon 47              116  1.64725e+05      9.86e-06\npolygon 48               36  1.15582e+04      6.92e-07\npolygon 49               15  3.48567e+03      2.09e-07\npolygon 50              325  3.08585e+06      1.85e-04\npolygon 51 (hole)         4 -2.57635e+02     -1.54e-08\npolygon 52              191  2.36948e+06      1.42e-04\npolygon 53               75  8.81673e+04      5.28e-06\npolygon 54               49  6.13133e+04      3.67e-06\npolygon 55              117  7.65040e+04      4.58e-06\npolygon 56                5  1.73964e+04      1.04e-06\npolygon 57             6115  4.61702e+09      2.76e-01\npolygon 58                4  6.35998e+03      3.81e-07\npolygon 59               16  1.18306e+05      7.08e-06\npolygon 60                5  2.11525e+04      1.27e-06\npolygon 61               11  3.52792e+04      2.11e-06\npolygon 62               13  6.20254e+04      3.71e-06\npolygon 63                8  1.23192e+04      7.37e-07\npolygon 64                5  1.08320e+04      6.48e-07\npolygon 65                5  9.14438e+03      5.47e-07\npolygon 66                6  9.61120e+03      5.75e-07\npolygon 67               31  1.54535e+06      9.25e-05\npolygon 68                9  2.89834e+04      1.73e-06\npolygon 69               10  2.76357e+04      1.65e-06\npolygon 70               12  8.99515e+04      5.38e-06\npolygon 71               15  5.42889e+04      3.25e-06\npolygon 72                8  1.58645e+04      9.49e-07\npolygon 73                6  1.35398e+04      8.10e-07\npolygon 74                4  4.29102e+03      2.57e-07\npolygon 75               23  1.76942e+05      1.06e-05\npolygon 76               44  4.35606e+04      2.61e-06\npolygon 77                5  6.83234e+03      4.09e-07\npolygon 78                5  6.67728e+03      4.00e-07\npolygon 79                5  2.24964e+03      1.35e-07\npolygon 80                7  2.62416e+04      1.57e-06\npolygon 81                9  1.19857e+04      7.17e-07\npolygon 82                8  2.25169e+04      1.35e-06\npolygon 83                6  1.21525e+04      7.27e-07\npolygon 84                5  8.76352e+03      5.24e-07\npolygon 85                9  2.28019e+04      1.36e-06\npolygon 86                6  9.21428e+03      5.51e-07\npolygon 87               49  1.00151e+05      5.99e-06\npolygon 88                5  4.45695e+03      2.67e-07\npolygon 89                4  2.90460e+03      1.74e-07\npolygon 90                4  3.68683e+03      2.21e-07\npolygon 91               64  9.90797e+04      5.93e-06\npolygon 92                5  3.03789e+03      1.82e-07\npolygon 93                6  1.05164e+04      6.29e-07\npolygon 94               12  9.48365e+03      5.68e-07\npolygon 95               13  6.95068e+04      4.16e-06\npolygon 96               34  2.01269e+05      1.20e-05\npolygon 97              222  1.23410e+06      7.39e-05\npolygon 98                4  2.60388e+03      1.56e-07\npolygon 99                7  8.47934e+03      5.07e-07\npolygon 100               5  8.09538e+03      4.84e-07\npolygon 101              93  2.08256e+05      1.25e-05\npolygon 102              16  7.78971e+04      4.66e-06\npolygon 103              24  1.58864e+04      9.51e-07\npolygon 104               4  8.84658e+02      5.29e-08\npolygon 105              42  4.25525e+04      2.55e-06\npolygon 106               5  4.40655e+03      2.64e-07\npolygon 107               4  2.45366e+03      1.47e-07\npolygon 108               5  4.70713e+03      2.82e-07\npolygon 109             214  2.22664e+06      1.33e-04\npolygon 110              60  5.44949e+04      3.26e-06\npolygon 111               6  4.54843e+03      2.72e-07\npolygon 112               6  1.47384e+04      8.82e-07\npolygon 113               6  1.35534e+04      8.11e-07\npolygon 114              51  9.00778e+04      5.39e-06\npolygon 115               4  1.53560e+03      9.19e-08\npolygon 116 (hole)        3 -3.48195e+02     -2.08e-08\npolygon 117              32  4.34406e+04      2.60e-06\npolygon 118             144  5.46612e+05      3.27e-05\npolygon 119              36  7.39285e+04      4.42e-06\npolygon 120              42  9.17456e+04      5.49e-06\npolygon 121             141  1.10139e+06      6.59e-05\npolygon 122              94  2.55406e+05      1.53e-05\npolygon 123              71  3.74605e+04      2.24e-06\npolygon 124              72  9.11248e+05      5.45e-05\npolygon 125 (hole)        3 -2.91215e+01     -1.74e-09\npolygon 126               5  4.23910e+03      2.54e-07\npolygon 127              47  1.10186e+05      6.59e-06\npolygon 128               6  1.00636e+04      6.02e-07\npolygon 129 (hole)        3 -3.94431e+01     -2.36e-09\npolygon 130               4  1.41401e+02      8.46e-09\npolygon 131              32  2.34725e+04      1.40e-06\npolygon 132               5  3.61887e+02      2.17e-08\npolygon 133 (hole)        3 -2.90329e+03     -1.74e-07\npolygon 134             120  3.80494e+05      2.28e-05\npolygon 135              19  3.13628e+04      1.88e-06\npolygon 136              42  1.21238e+05      7.26e-06\npolygon 137              24  3.83277e+04      2.29e-06\npolygon 138              88  4.56947e+05      2.73e-05\npolygon 139              20  1.78310e+04      1.07e-06\npolygon 140              20  2.39227e+04      1.43e-06\npolygon 141               5  6.65825e+03      3.98e-07\npolygon 142               5  5.41759e+03      3.24e-07\npolygon 143              53  4.55018e+05      2.72e-05\npolygon 144             263  1.76197e+07      1.05e-03\npolygon 145 (hole)        4 -3.70130e+03     -2.22e-07\npolygon 146               4  6.79585e+03      4.07e-07\npolygon 147              15  5.74657e+03      3.44e-07\npolygon 148              25  2.16665e+05      1.30e-05\npolygon 149               9  7.99755e+03      4.79e-07\npolygon 150               8  3.28848e+03      1.97e-07\npolygon 151              10  1.13482e+04      6.79e-07\npolygon 152              45  1.05243e+05      6.30e-06\npolygon 153               7  4.67607e+03      2.80e-07\npolygon 154             533  4.88598e+07      2.92e-03\npolygon 155             641  1.19875e+08      7.17e-03\npolygon 156              68  3.72768e+06      2.23e-04\npolygon 157              10  1.01914e+05      6.10e-06\npolygon 158              19  8.26881e+04      4.95e-06\npolygon 159              17  4.00986e+04      2.40e-06\npolygon 160              11  4.05798e+04      2.43e-06\npolygon 161             111  1.46838e+06      8.79e-05\npolygon 162             112  3.73964e+06      2.24e-04\npolygon 163              19  1.91730e+04      1.15e-06\npolygon 164              73  3.61914e+06      2.17e-04\npolygon 165              10  3.05707e+04      1.83e-06\npolygon 166               6  1.62989e+04      9.75e-07\npolygon 167              81  2.65465e+05      1.59e-05\npolygon 168               8  2.09366e+04      1.25e-06\npolygon 169               9  2.23138e+04      1.34e-06\npolygon 170              11  1.96456e+04      1.18e-06\npolygon 171 (hole)        3 -4.90052e+02     -2.93e-08\npolygon 172             145  7.13727e+06      4.27e-04\npolygon 173              60  1.45353e+05      8.70e-06\npolygon 174 (hole)        4 -3.71089e+02     -2.22e-08\npolygon 175              74  2.06447e+06      1.24e-04\npolygon 176              36  2.65019e+04      1.59e-06\npolygon 177             931  2.12400e+08      1.27e-02\npolygon 178 (hole)        4 -1.54888e+02     -9.27e-09\npolygon 179              25  2.21142e+05      1.32e-05\npolygon 180               6  6.87439e+03      4.11e-07\npolygon 181              31  1.67778e+06      1.00e-04\npolygon 182               7  9.00046e+03      5.39e-07\npolygon 183              12  6.30886e+04      3.78e-06\npolygon 184             202  2.25185e+07      1.35e-03\npolygon 185              46  2.39825e+06      1.44e-04\npolygon 186              25  1.05566e+04      6.32e-07\npolygon 187               8  4.61437e+04      2.76e-06\npolygon 188               7  3.16305e+04      1.89e-06\npolygon 189               5  6.49689e+03      3.89e-07\npolygon 190              12  2.17421e+04      1.30e-06\npolygon 191               5  7.19819e+03      4.31e-07\npolygon 192              11  6.58157e+05      3.94e-05\npolygon 193             159  1.11857e+06      6.69e-05\npolygon 194             104  3.97323e+05      2.38e-05\npolygon 195              14  3.77571e+05      2.26e-05\npolygon 196              27  1.21314e+06      7.26e-05\npolygon 197              69  7.97787e+04      4.77e-06\npolygon 198              51  6.46735e+04      3.87e-06\npolygon 199              71  7.20054e+04      4.31e-06\npolygon 200              46  3.29862e+04      1.97e-06\npolygon 201              52  3.88477e+04      2.32e-06\npolygon 202              59  5.31603e+06      3.18e-04\npolygon 203 (hole)        3 -3.70978e+01     -2.22e-09\npolygon 204              48  1.76671e+06      1.06e-04\npolygon 205              34  5.03002e+04      3.01e-06\npolygon 206              46  1.83138e+04      1.10e-06\npolygon 207              18  5.28224e+03      3.16e-07\npolygon 208             244  3.09431e+06      1.85e-04\npolygon 209              28  1.99131e+05      1.19e-05\npolygon 210              21  4.52072e+05      2.71e-05\npolygon 211              14  5.66637e+04      3.39e-06\npolygon 212              12  1.09825e+05      6.57e-06\npolygon 213              15  1.37135e+06      8.21e-05\npolygon 214               6  1.22312e+05      7.32e-06\npolygon 215              43  2.41150e+05      1.44e-05\npolygon 216               7  2.79840e+04      1.67e-06\npolygon 217              46  1.72671e+06      1.03e-04\npolygon 218              13  6.36540e+04      3.81e-06\npolygon 219              11  7.02064e+05      4.20e-05\npolygon 220              47  9.13826e+05      5.47e-05\npolygon 221 (hole)        4 -3.82299e+03     -2.29e-07\npolygon 222              62  3.95149e+06      2.36e-04\npolygon 223               9  4.34659e+05      2.60e-05\npolygon 224              14  4.20832e+04      2.52e-06\npolygon 225              18  5.12063e+03      3.06e-07\npolygon 226              31  7.44741e+05      4.46e-05\npolygon 227              18  4.11233e+05      2.46e-05\npolygon 228 (hole)        3 -5.32643e+02     -3.19e-08\npolygon 229              23  7.71654e+03      4.62e-07\npolygon 230               8  3.86022e+04      2.31e-06\npolygon 231             168  7.46341e+06      4.47e-04\npolygon 232               4  9.19978e+03      5.51e-07\npolygon 233              14  2.75390e+05      1.65e-05\npolygon 234               9  9.63798e+03      5.77e-07\npolygon 235               9  6.16361e+04      3.69e-06\npolygon 236              10  1.04975e+05      6.28e-06\npolygon 237               8  8.75225e+04      5.24e-06\npolygon 238               5  1.84001e+04      1.10e-06\npolygon 239               5  3.30315e+03      1.98e-07\npolygon 240              13  3.15610e+04      1.89e-06\npolygon 241              67  3.93597e+05      2.36e-05\npolygon 242               4  2.83740e+03      1.70e-07\npolygon 243              15  4.86091e+03      2.91e-07\npolygon 244               4  1.26784e+03      7.59e-08\npolygon 245               8  1.67287e+04      1.00e-06\npolygon 246              12  2.45858e+04      1.47e-06\npolygon 247              12  5.95789e+04      3.57e-06\npolygon 248              12  2.14651e+03      1.28e-07\npolygon 249              14  4.30849e+03      2.58e-07\npolygon 250              16  7.75733e+03      4.64e-07\npolygon 251 (hole)        4 -2.56408e+03     -1.53e-07\npolygon 252              10  1.75648e+03      1.05e-07\npolygon 253              34  3.85386e+06      2.31e-04\npolygon 254               9  3.55249e+03      2.13e-07\npolygon 255              15  1.39816e+04      8.37e-07\npolygon 256              16  4.56059e+05      2.73e-05\npolygon 257              26  1.42631e+04      8.54e-07\npolygon 258              36  2.65697e+04      1.59e-06\npolygon 259              19  4.48214e+03      2.68e-07\npolygon 260              96  5.26087e+05      3.15e-05\npolygon 261              60  1.00635e+05      6.02e-06\npolygon 262              31  7.13046e+04      4.27e-06\npolygon 263              84  1.37245e+06      8.21e-05\npolygon 264             178  8.26947e+05      4.95e-05\npolygon 265              58  1.98953e+05      1.19e-05\npolygon 266             115  3.13297e+05      1.87e-05\npolygon 267             132  4.76047e+05      2.85e-05\npolygon 268             102  1.63410e+07      9.78e-04\npolygon 269              18  6.36647e+03      3.81e-07\npolygon 270              10  7.72550e+03      4.62e-07\npolygon 271             204  1.42141e+06      8.51e-05\npolygon 272             125  2.33639e+06      1.40e-04\npolygon 273              30  2.58819e+04      1.55e-06\npolygon 274              24  2.06403e+04      1.24e-06\npolygon 275              87  1.25178e+06      7.49e-05\npolygon 276              38  4.04777e+04      2.42e-06\npolygon 277              10  5.04036e+04      3.02e-06\npolygon 278               6  1.12482e+04      6.73e-07\npolygon 279              40  1.56825e+05      9.39e-06\npolygon 280              63  5.25132e+04      3.14e-06\npolygon 281               8  1.11128e+04      6.65e-07\npolygon 282               9  1.49650e+03      8.96e-08\npolygon 283              13  4.50724e+03      2.70e-07\npolygon 284              41  4.12892e+04      2.47e-06\npolygon 285               7  3.03545e+03      1.82e-07\npolygon 286              19  7.54584e+03      4.52e-07\npolygon 287              16  8.29528e+03      4.96e-07\npolygon 288              26  5.16293e+05      3.09e-05\npolygon 289              13  9.23979e+04      5.53e-06\npolygon 290              25  4.18391e+04      2.50e-06\npolygon 291               7  3.85120e+03      2.30e-07\npolygon 292              82  4.38489e+05      2.62e-05\npolygon 293             163  3.46322e+06      2.07e-04\npolygon 294              11  5.22182e+03      3.13e-07\npolygon 295              13  3.28208e+04      1.96e-06\npolygon 296               6  3.00920e+04      1.80e-06\npolygon 297               6  1.13309e+04      6.78e-07\npolygon 298              13  2.03134e+05      1.22e-05\npolygon 299              36  2.87112e+04      1.72e-06\npolygon 300               6  1.13102e+04      6.77e-07\npolygon 301              13  4.04370e+03      2.42e-07\npolygon 302              44  1.67056e+05      1.00e-05\npolygon 303              11  6.66250e+04      3.99e-06\npolygon 304             129  2.29313e+06      1.37e-04\npolygon 305               6  7.23372e+04      4.33e-06\npolygon 306              19  6.36948e+03      3.81e-07\npolygon 307             106  1.06881e+06      6.40e-05\npolygon 308               9  2.30443e+04      1.38e-06\npolygon 309              29  9.02725e+03      5.40e-07\npolygon 310              68  1.08514e+05      6.49e-06\npolygon 311              64  2.19772e+05      1.32e-05\npolygon 312              25  2.58054e+05      1.54e-05\npolygon 313              26  4.59967e+04      2.75e-06\npolygon 314              57  8.30317e+04      4.97e-06\npolygon 315              83  1.13991e+06      6.82e-05\npolygon 316              11  3.01692e+04      1.81e-06\npolygon 317              14  1.30445e+05      7.81e-06\npolygon 318              22  2.37053e+05      1.42e-05\npolygon 319              22  1.84800e+05      1.11e-05\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n                     (416100 x 212200 units)\nWindow area = 16709500000 square units\nFraction of frame area: 0.189\n\n\n\n# Visual check that points and window align\nplot(fire_month_owin)   # base plot; circles show mark values (here: months 1..12)\n\n\n\n\n\n\n\n\n\n\n\nSubsequently, we will estimate spatio-temporal kernel density over space (meters) and discrete time (months).\n\n# Compute STKDE using sparr::spattemp.density \nst_kde &lt;- sparr::spattemp.density(fire_month_owin)  # uses default bandwidths; time is Month_num (1..12)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(st_kde)                                     # see chosen bandwidths h (space) and lambda (time), bounds, range\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 19155.99 (spatial)\n  lambda = 0.0264 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.201154e-39, 6.369146e-10]\n\n\n\n\n\nThereafter, we will reproduce month-specific density maps for July–December example.\n\n# Plot a subset of months (e.g., 7..12) with a fixed legend range for comparability \ntims &lt;- c(7,8,9,10,11,12)      # months to display (Jul..Dec), matches the sample figure\npar(mfcol = c(2,3))            # 2 rows x 3 columns panel\nfor(i in tims){                # loop through the selected months\n  plot(st_kde, i,              # draw the STKDE slice at month 'i'\n       override.par = FALSE,   # keep mfcol settings\n       fix.range = TRUE,       # use common color scale across panels for fair comparison\n       main = paste(\"KDE at month\", i))  # panel title\n}\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we will repeat the workflow but using Day of Year (1..365) as the temporal mark.\n\n\n\n# Create a ppp whose mark is DayofYear (1..365) ----------------------------------------------\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  dplyr::select(DayofYear) %&gt;%     # keep only the DOY mark and geometry\n  as.ppp()                         # convert to ppp object\n\n\n\n\n\n# Attach the owin (study window) to the DayofYear ppp \nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin] # ensure points are constrained to study polygon\nsummary(fire_yday_owin)    # confirm counts, mark summary, window stats\n\nMarked planar point pattern:  899 points\nAverage intensity 5.380183e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   219.0   260.0   247.7   288.0   360.0 \n\nWindow: polygonal boundary\n319 separate polygons (40 holes)\n                   vertices         area relative.area\npolygon 1              9026  1.15464e+10      6.91e-01\npolygon 2                19  1.26217e+04      7.55e-07\npolygon 3 (hole)          3 -1.80096e-01     -1.08e-11\npolygon 4 (hole)          3 -9.26944e-01     -5.55e-11\npolygon 5 (hole)          3 -1.07382e+03     -6.43e-08\npolygon 6 (hole)          3 -6.41537e+02     -3.84e-08\npolygon 7                23  9.26156e+04      5.54e-06\npolygon 8                30  3.37146e+04      2.02e-06\npolygon 9                54  3.25211e+05      1.95e-05\npolygon 10               33  1.30244e+06      7.79e-05\npolygon 11 (hole)         4 -9.57353e+02     -5.73e-08\npolygon 12 (hole)         4 -2.84738e+02     -1.70e-08\npolygon 13 (hole)         3 -1.38747e+02     -8.30e-09\npolygon 14 (hole)         4 -5.31445e+02     -3.18e-08\npolygon 15 (hole)         3 -6.42241e+02     -3.84e-08\npolygon 16 (hole)         6 -1.13809e+02     -6.81e-09\npolygon 17 (hole)         4 -2.10482e+02     -1.26e-08\npolygon 18 (hole)         4 -9.54892e+01     -5.71e-09\npolygon 19 (hole)         4 -6.47895e+01     -3.88e-09\npolygon 20 (hole)         4 -7.10903e+01     -4.25e-09\npolygon 21 (hole)         4 -2.18071e+02     -1.31e-08\npolygon 22 (hole)         4 -4.44993e+01     -2.66e-09\npolygon 23 (hole)         4 -4.90009e+01     -2.93e-09\npolygon 24 (hole)         4 -6.48299e+01     -3.88e-09\npolygon 25 (hole)         4 -2.36910e+02     -1.42e-08\npolygon 26 (hole)         4 -3.07205e+01     -1.84e-09\npolygon 27 (hole)         4 -5.24443e+01     -3.14e-09\npolygon 28 (hole)         5 -6.64298e+01     -3.98e-09\npolygon 29 (hole)         4 -6.08287e+01     -3.64e-09\npolygon 30 (hole)         4 -1.12681e+02     -6.74e-09\npolygon 31              132  1.16480e+06      6.97e-05\npolygon 32 (hole)         4 -9.56157e+01     -5.72e-09\npolygon 33               68  1.91291e+05      1.14e-05\npolygon 34 (hole)         4 -5.31171e+02     -3.18e-08\npolygon 35 (hole)         3 -1.76143e+00     -1.05e-10\npolygon 36              116  1.51984e+05      9.10e-06\npolygon 37               95  1.14322e+05      6.84e-06\npolygon 38               21  4.51914e+03      2.70e-07\npolygon 39               24  1.18511e+04      7.09e-07\npolygon 40              130  1.52296e+05      9.11e-06\npolygon 41               36  1.07878e+04      6.46e-07\npolygon 42               54  3.46932e+05      2.08e-05\npolygon 43               71  2.15798e+05      1.29e-05\npolygon 44               12  1.76977e+03      1.06e-07\npolygon 45               75  1.05557e+05      6.32e-06\npolygon 46               17  4.22421e+03      2.53e-07\npolygon 47              116  1.64725e+05      9.86e-06\npolygon 48               36  1.15582e+04      6.92e-07\npolygon 49               15  3.48567e+03      2.09e-07\npolygon 50              325  3.08585e+06      1.85e-04\npolygon 51 (hole)         4 -2.57635e+02     -1.54e-08\npolygon 52              191  2.36948e+06      1.42e-04\npolygon 53               75  8.81673e+04      5.28e-06\npolygon 54               49  6.13133e+04      3.67e-06\npolygon 55              117  7.65040e+04      4.58e-06\npolygon 56                5  1.73964e+04      1.04e-06\npolygon 57             6115  4.61702e+09      2.76e-01\npolygon 58                4  6.35998e+03      3.81e-07\npolygon 59               16  1.18306e+05      7.08e-06\npolygon 60                5  2.11525e+04      1.27e-06\npolygon 61               11  3.52792e+04      2.11e-06\npolygon 62               13  6.20254e+04      3.71e-06\npolygon 63                8  1.23192e+04      7.37e-07\npolygon 64                5  1.08320e+04      6.48e-07\npolygon 65                5  9.14438e+03      5.47e-07\npolygon 66                6  9.61120e+03      5.75e-07\npolygon 67               31  1.54535e+06      9.25e-05\npolygon 68                9  2.89834e+04      1.73e-06\npolygon 69               10  2.76357e+04      1.65e-06\npolygon 70               12  8.99515e+04      5.38e-06\npolygon 71               15  5.42889e+04      3.25e-06\npolygon 72                8  1.58645e+04      9.49e-07\npolygon 73                6  1.35398e+04      8.10e-07\npolygon 74                4  4.29102e+03      2.57e-07\npolygon 75               23  1.76942e+05      1.06e-05\npolygon 76               44  4.35606e+04      2.61e-06\npolygon 77                5  6.83234e+03      4.09e-07\npolygon 78                5  6.67728e+03      4.00e-07\npolygon 79                5  2.24964e+03      1.35e-07\npolygon 80                7  2.62416e+04      1.57e-06\npolygon 81                9  1.19857e+04      7.17e-07\npolygon 82                8  2.25169e+04      1.35e-06\npolygon 83                6  1.21525e+04      7.27e-07\npolygon 84                5  8.76352e+03      5.24e-07\npolygon 85                9  2.28019e+04      1.36e-06\npolygon 86                6  9.21428e+03      5.51e-07\npolygon 87               49  1.00151e+05      5.99e-06\npolygon 88                5  4.45695e+03      2.67e-07\npolygon 89                4  2.90460e+03      1.74e-07\npolygon 90                4  3.68683e+03      2.21e-07\npolygon 91               64  9.90797e+04      5.93e-06\npolygon 92                5  3.03789e+03      1.82e-07\npolygon 93                6  1.05164e+04      6.29e-07\npolygon 94               12  9.48365e+03      5.68e-07\npolygon 95               13  6.95068e+04      4.16e-06\npolygon 96               34  2.01269e+05      1.20e-05\npolygon 97              222  1.23410e+06      7.39e-05\npolygon 98                4  2.60388e+03      1.56e-07\npolygon 99                7  8.47934e+03      5.07e-07\npolygon 100               5  8.09538e+03      4.84e-07\npolygon 101              93  2.08256e+05      1.25e-05\npolygon 102              16  7.78971e+04      4.66e-06\npolygon 103              24  1.58864e+04      9.51e-07\npolygon 104               4  8.84658e+02      5.29e-08\npolygon 105              42  4.25525e+04      2.55e-06\npolygon 106               5  4.40655e+03      2.64e-07\npolygon 107               4  2.45366e+03      1.47e-07\npolygon 108               5  4.70713e+03      2.82e-07\npolygon 109             214  2.22664e+06      1.33e-04\npolygon 110              60  5.44949e+04      3.26e-06\npolygon 111               6  4.54843e+03      2.72e-07\npolygon 112               6  1.47384e+04      8.82e-07\npolygon 113               6  1.35534e+04      8.11e-07\npolygon 114              51  9.00778e+04      5.39e-06\npolygon 115               4  1.53560e+03      9.19e-08\npolygon 116 (hole)        3 -3.48195e+02     -2.08e-08\npolygon 117              32  4.34406e+04      2.60e-06\npolygon 118             144  5.46612e+05      3.27e-05\npolygon 119              36  7.39285e+04      4.42e-06\npolygon 120              42  9.17456e+04      5.49e-06\npolygon 121             141  1.10139e+06      6.59e-05\npolygon 122              94  2.55406e+05      1.53e-05\npolygon 123              71  3.74605e+04      2.24e-06\npolygon 124              72  9.11248e+05      5.45e-05\npolygon 125 (hole)        3 -2.91215e+01     -1.74e-09\npolygon 126               5  4.23910e+03      2.54e-07\npolygon 127              47  1.10186e+05      6.59e-06\npolygon 128               6  1.00636e+04      6.02e-07\npolygon 129 (hole)        3 -3.94431e+01     -2.36e-09\npolygon 130               4  1.41401e+02      8.46e-09\npolygon 131              32  2.34725e+04      1.40e-06\npolygon 132               5  3.61887e+02      2.17e-08\npolygon 133 (hole)        3 -2.90329e+03     -1.74e-07\npolygon 134             120  3.80494e+05      2.28e-05\npolygon 135              19  3.13628e+04      1.88e-06\npolygon 136              42  1.21238e+05      7.26e-06\npolygon 137              24  3.83277e+04      2.29e-06\npolygon 138              88  4.56947e+05      2.73e-05\npolygon 139              20  1.78310e+04      1.07e-06\npolygon 140              20  2.39227e+04      1.43e-06\npolygon 141               5  6.65825e+03      3.98e-07\npolygon 142               5  5.41759e+03      3.24e-07\npolygon 143              53  4.55018e+05      2.72e-05\npolygon 144             263  1.76197e+07      1.05e-03\npolygon 145 (hole)        4 -3.70130e+03     -2.22e-07\npolygon 146               4  6.79585e+03      4.07e-07\npolygon 147              15  5.74657e+03      3.44e-07\npolygon 148              25  2.16665e+05      1.30e-05\npolygon 149               9  7.99755e+03      4.79e-07\npolygon 150               8  3.28848e+03      1.97e-07\npolygon 151              10  1.13482e+04      6.79e-07\npolygon 152              45  1.05243e+05      6.30e-06\npolygon 153               7  4.67607e+03      2.80e-07\npolygon 154             533  4.88598e+07      2.92e-03\npolygon 155             641  1.19875e+08      7.17e-03\npolygon 156              68  3.72768e+06      2.23e-04\npolygon 157              10  1.01914e+05      6.10e-06\npolygon 158              19  8.26881e+04      4.95e-06\npolygon 159              17  4.00986e+04      2.40e-06\npolygon 160              11  4.05798e+04      2.43e-06\npolygon 161             111  1.46838e+06      8.79e-05\npolygon 162             112  3.73964e+06      2.24e-04\npolygon 163              19  1.91730e+04      1.15e-06\npolygon 164              73  3.61914e+06      2.17e-04\npolygon 165              10  3.05707e+04      1.83e-06\npolygon 166               6  1.62989e+04      9.75e-07\npolygon 167              81  2.65465e+05      1.59e-05\npolygon 168               8  2.09366e+04      1.25e-06\npolygon 169               9  2.23138e+04      1.34e-06\npolygon 170              11  1.96456e+04      1.18e-06\npolygon 171 (hole)        3 -4.90052e+02     -2.93e-08\npolygon 172             145  7.13727e+06      4.27e-04\npolygon 173              60  1.45353e+05      8.70e-06\npolygon 174 (hole)        4 -3.71089e+02     -2.22e-08\npolygon 175              74  2.06447e+06      1.24e-04\npolygon 176              36  2.65019e+04      1.59e-06\npolygon 177             931  2.12400e+08      1.27e-02\npolygon 178 (hole)        4 -1.54888e+02     -9.27e-09\npolygon 179              25  2.21142e+05      1.32e-05\npolygon 180               6  6.87439e+03      4.11e-07\npolygon 181              31  1.67778e+06      1.00e-04\npolygon 182               7  9.00046e+03      5.39e-07\npolygon 183              12  6.30886e+04      3.78e-06\npolygon 184             202  2.25185e+07      1.35e-03\npolygon 185              46  2.39825e+06      1.44e-04\npolygon 186              25  1.05566e+04      6.32e-07\npolygon 187               8  4.61437e+04      2.76e-06\npolygon 188               7  3.16305e+04      1.89e-06\npolygon 189               5  6.49689e+03      3.89e-07\npolygon 190              12  2.17421e+04      1.30e-06\npolygon 191               5  7.19819e+03      4.31e-07\npolygon 192              11  6.58157e+05      3.94e-05\npolygon 193             159  1.11857e+06      6.69e-05\npolygon 194             104  3.97323e+05      2.38e-05\npolygon 195              14  3.77571e+05      2.26e-05\npolygon 196              27  1.21314e+06      7.26e-05\npolygon 197              69  7.97787e+04      4.77e-06\npolygon 198              51  6.46735e+04      3.87e-06\npolygon 199              71  7.20054e+04      4.31e-06\npolygon 200              46  3.29862e+04      1.97e-06\npolygon 201              52  3.88477e+04      2.32e-06\npolygon 202              59  5.31603e+06      3.18e-04\npolygon 203 (hole)        3 -3.70978e+01     -2.22e-09\npolygon 204              48  1.76671e+06      1.06e-04\npolygon 205              34  5.03002e+04      3.01e-06\npolygon 206              46  1.83138e+04      1.10e-06\npolygon 207              18  5.28224e+03      3.16e-07\npolygon 208             244  3.09431e+06      1.85e-04\npolygon 209              28  1.99131e+05      1.19e-05\npolygon 210              21  4.52072e+05      2.71e-05\npolygon 211              14  5.66637e+04      3.39e-06\npolygon 212              12  1.09825e+05      6.57e-06\npolygon 213              15  1.37135e+06      8.21e-05\npolygon 214               6  1.22312e+05      7.32e-06\npolygon 215              43  2.41150e+05      1.44e-05\npolygon 216               7  2.79840e+04      1.67e-06\npolygon 217              46  1.72671e+06      1.03e-04\npolygon 218              13  6.36540e+04      3.81e-06\npolygon 219              11  7.02064e+05      4.20e-05\npolygon 220              47  9.13826e+05      5.47e-05\npolygon 221 (hole)        4 -3.82299e+03     -2.29e-07\npolygon 222              62  3.95149e+06      2.36e-04\npolygon 223               9  4.34659e+05      2.60e-05\npolygon 224              14  4.20832e+04      2.52e-06\npolygon 225              18  5.12063e+03      3.06e-07\npolygon 226              31  7.44741e+05      4.46e-05\npolygon 227              18  4.11233e+05      2.46e-05\npolygon 228 (hole)        3 -5.32643e+02     -3.19e-08\npolygon 229              23  7.71654e+03      4.62e-07\npolygon 230               8  3.86022e+04      2.31e-06\npolygon 231             168  7.46341e+06      4.47e-04\npolygon 232               4  9.19978e+03      5.51e-07\npolygon 233              14  2.75390e+05      1.65e-05\npolygon 234               9  9.63798e+03      5.77e-07\npolygon 235               9  6.16361e+04      3.69e-06\npolygon 236              10  1.04975e+05      6.28e-06\npolygon 237               8  8.75225e+04      5.24e-06\npolygon 238               5  1.84001e+04      1.10e-06\npolygon 239               5  3.30315e+03      1.98e-07\npolygon 240              13  3.15610e+04      1.89e-06\npolygon 241              67  3.93597e+05      2.36e-05\npolygon 242               4  2.83740e+03      1.70e-07\npolygon 243              15  4.86091e+03      2.91e-07\npolygon 244               4  1.26784e+03      7.59e-08\npolygon 245               8  1.67287e+04      1.00e-06\npolygon 246              12  2.45858e+04      1.47e-06\npolygon 247              12  5.95789e+04      3.57e-06\npolygon 248              12  2.14651e+03      1.28e-07\npolygon 249              14  4.30849e+03      2.58e-07\npolygon 250              16  7.75733e+03      4.64e-07\npolygon 251 (hole)        4 -2.56408e+03     -1.53e-07\npolygon 252              10  1.75648e+03      1.05e-07\npolygon 253              34  3.85386e+06      2.31e-04\npolygon 254               9  3.55249e+03      2.13e-07\npolygon 255              15  1.39816e+04      8.37e-07\npolygon 256              16  4.56059e+05      2.73e-05\npolygon 257              26  1.42631e+04      8.54e-07\npolygon 258              36  2.65697e+04      1.59e-06\npolygon 259              19  4.48214e+03      2.68e-07\npolygon 260              96  5.26087e+05      3.15e-05\npolygon 261              60  1.00635e+05      6.02e-06\npolygon 262              31  7.13046e+04      4.27e-06\npolygon 263              84  1.37245e+06      8.21e-05\npolygon 264             178  8.26947e+05      4.95e-05\npolygon 265              58  1.98953e+05      1.19e-05\npolygon 266             115  3.13297e+05      1.87e-05\npolygon 267             132  4.76047e+05      2.85e-05\npolygon 268             102  1.63410e+07      9.78e-04\npolygon 269              18  6.36647e+03      3.81e-07\npolygon 270              10  7.72550e+03      4.62e-07\npolygon 271             204  1.42141e+06      8.51e-05\npolygon 272             125  2.33639e+06      1.40e-04\npolygon 273              30  2.58819e+04      1.55e-06\npolygon 274              24  2.06403e+04      1.24e-06\npolygon 275              87  1.25178e+06      7.49e-05\npolygon 276              38  4.04777e+04      2.42e-06\npolygon 277              10  5.04036e+04      3.02e-06\npolygon 278               6  1.12482e+04      6.73e-07\npolygon 279              40  1.56825e+05      9.39e-06\npolygon 280              63  5.25132e+04      3.14e-06\npolygon 281               8  1.11128e+04      6.65e-07\npolygon 282               9  1.49650e+03      8.96e-08\npolygon 283              13  4.50724e+03      2.70e-07\npolygon 284              41  4.12892e+04      2.47e-06\npolygon 285               7  3.03545e+03      1.82e-07\npolygon 286              19  7.54584e+03      4.52e-07\npolygon 287              16  8.29528e+03      4.96e-07\npolygon 288              26  5.16293e+05      3.09e-05\npolygon 289              13  9.23979e+04      5.53e-06\npolygon 290              25  4.18391e+04      2.50e-06\npolygon 291               7  3.85120e+03      2.30e-07\npolygon 292              82  4.38489e+05      2.62e-05\npolygon 293             163  3.46322e+06      2.07e-04\npolygon 294              11  5.22182e+03      3.13e-07\npolygon 295              13  3.28208e+04      1.96e-06\npolygon 296               6  3.00920e+04      1.80e-06\npolygon 297               6  1.13309e+04      6.78e-07\npolygon 298              13  2.03134e+05      1.22e-05\npolygon 299              36  2.87112e+04      1.72e-06\npolygon 300               6  1.13102e+04      6.77e-07\npolygon 301              13  4.04370e+03      2.42e-07\npolygon 302              44  1.67056e+05      1.00e-05\npolygon 303              11  6.66250e+04      3.99e-06\npolygon 304             129  2.29313e+06      1.37e-04\npolygon 305               6  7.23372e+04      4.33e-06\npolygon 306              19  6.36948e+03      3.81e-07\npolygon 307             106  1.06881e+06      6.40e-05\npolygon 308               9  2.30443e+04      1.38e-06\npolygon 309              29  9.02725e+03      5.40e-07\npolygon 310              68  1.08514e+05      6.49e-06\npolygon 311              64  2.19772e+05      1.32e-05\npolygon 312              25  2.58054e+05      1.54e-05\npolygon 313              26  4.59967e+04      2.75e-06\npolygon 314              57  8.30317e+04      4.97e-06\npolygon 315              83  1.13991e+06      6.82e-05\npolygon 316              11  3.01692e+04      1.81e-06\npolygon 317              14  1.30445e+05      7.81e-06\npolygon 318              22  2.37053e+05      1.42e-05\npolygon 319              22  1.84800e+05      1.11e-05\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n                     (416100 x 212200 units)\nWindow area = 16709500000 square units\nFraction of frame area: 0.189\n\n\n\n\n\nWe aim to compute STKDE with default bandwidths and plot.\n\n# Compute STKDE using default bandwidth selection (gives baseline) \nkde_yday &lt;- spattemp.density(\n  fire_yday_owin        # ppp with DayofYear marks and study window\n)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)       # examine space/time bandwidths (h, lambda) and density range\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 19155.99 (spatial)\n  lambda = 6.4456 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [10, 360]\n\nEvaluation\n  128 x 128 x 351 trivariate lattice\n  Density range: [1.594274e-28, 1.857482e-12]\n\n\n\n# Visualise the density surface aggregated over time (default plot) \nplot(kde_yday)     # continuous surface over the region with legend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy this step? Default bandwidths are generic. In this section, want to improve them by bootstrap MISE using BOOT.spattemp(), which estimates a scalar spatial bandwidth (h, meters) and a scalar temporal bandwidth (lambda, in DOY units) suited to our data.\n\n# For reproducibility of the bootstrap selection \nset.seed(1234)    # fixed seed so results are repeatable in class\n\n# Run bootstrap bandwidth selection (may take some time) \nBOOT.spattemp(fire_yday_owin)   # prints many trial (h, lambda) and the final pair at bottom\n\nInitialising...Done.\nOptimising...\nh = 19155.99 \b; lambda = 15.11399 \nh = 21071.59 \b; lambda = 15.11399 \nh = 19155.99 \b; lambda = 1930.713 \nh = 19634.89 \b; lambda = 972.9137 \nh = 19874.34 \b; lambda = 494.0138 \nh = 19994.07 \b; lambda = 254.5639 \nh = 20053.93 \b; lambda = 134.8389 \nh = 20083.86 \b; lambda = 74.97647 \nh = 20098.83 \b; lambda = 45.04523 \nh = 20106.31 \b; lambda = 30.07961 \nh = 20121.28 \b; lambda = 0.1483655 \nh = 20110.05 \b; lambda = 22.5968 \nh = 22025.65 \b; lambda = 22.5968 \nh = 23460.48 \b; lambda = 26.3382 \nh = 22498.94 \b; lambda = 33.82101 \nh = 21428.43 \b; lambda = 19.79074 \nh = 24778.86 \b; lambda = 23.53215 \nh = 27113.26 \b; lambda = 23.99982 \nh = 29145.31 \b; lambda = 30.54728 \nh = 27216.09 \b; lambda = 27.85815 \nh = 30868.87 \b; lambda = 25.51977 \nh = 34573.07 \b; lambda = 25.11055 \nh = 34470.24 \b; lambda = 21.25223 \nh = 38097.31 \b; lambda = 17.94927 \nh = 45557.12 \b; lambda = 19.06 \nh = 54779.04 \b; lambda = 16.59009 \nh = 49081.36 \b; lambda = 11.89872 \nh = 45454.29 \b; lambda = 15.20168 \nh = 38200.14 \b; lambda = 21.80759 \nh = 43640.75 \b; lambda = 16.85315 \nh = 51100.56 \b; lambda = 17.96388 \nh = 47849.75 \b; lambda = 17.96023 \nh = 49766.11 \b; lambda = 20.16708 \nh = 45172.09 \b; lambda = 17.68163 \nh = 42879.46 \b; lambda = 18.7814 \nh = 44122.03 \b; lambda = 18.57611 \nh = 44507.06 \b; lambda = 19.95447 \nh = 44673.32 \b; lambda = 19.38626 \nDone.\n\n\n          h      lambda \n44673.31732    19.38626 \n\n# Note the final recommended h and lambda reported by the function output.\n\n\n\nWe will re-run STKDE using the recommended bandwidths.\n\n# Refit STKDE using the bootstrap MISE recommended bandwidths \nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,     # the DayofYear ppp constrained by the study window\n  h      = 45000,     # spatial bandwidth in meters (per improved selection)\n  lambda = 19         # temporal bandwidth in days-of-year (per improved selection)\n)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)     # verify the bandwidths and evaluation grid\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 45000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [10, 360]\n\nEvaluation\n  128 x 128 x 351 trivariate lattice\n  Density range: [3.902314e-16, 9.815536e-13]\n\n\n\n\n\nFinally, we display the final improved surface.\n\n# Plot the improved STKDE surface\nplot(kde_yday)   # shows the density surface with color bar; higher values = higher intensity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe spatio-temporal kernel density estimation (STKDE) provides clear evidence that the locations of forest fires in Kepulauan Bangka Belitung are not spatially or spatio-temporally independent. If the events were independent, the density surfaces would be flat and homogeneous across space and time. Instead, the STKDE outputs reveal distinct peaks, rejecting the null hypothesis of independence.\nThe bootstrap-based bandwidth selection process identified an optimal smoothing window of h ≈ 44.7 km (spatial) and λ ≈ 19.4 days (temporal). These parameters indicate that fire events tend to cluster within a radius of ~45 km and persist across approximately three-week periods. The density range produced, from very low background values (~1.2e-39) to sharp peaks (~6.4e-10), confirms substantial clustering intensity.\nThe spatial clustering is most pronounced in southern and central Bangka and in eastern Belitung, where the STKDE consistently highlights hotspots of higher fire intensity. These locations repeatedly emerge in the density surfaces, showing that fire events are concentrated in specific sub-regions rather than being uniformly distributed.\nTemporally, clustering is most evident during the July–October period, corresponding to the dry season. The monthly KDE plots demonstrate that fire intensity builds steadily in July, peaks in September, and declines toward the year’s end. This seasonality shows that fire occurrence is not equally likely throughout the year but is instead conditioned by climatic and land-surface factors linked to the dry months.\nThe implications of these findings are important for both science and policy. Spatial dependence means that fire risk is localized, and mitigation resources should be concentrated in the fire-prone regions identified by the STKDE. Temporal dependence means that efforts should be time-targeted, with enhanced monitoring and preventive measures deployed during the critical dry-season window. This combination of spatial and temporal clustering suggests that forest fires in Bangka Belitung are shaped by systematic environmental drivers, not by random chance.\nIn summary, the STKDE results demonstrate that forest fire events are spatially and spatio-temporally dependent. They cluster within ~45 km regions and persist across ~3-week intervals, with the highest concentrations observed in southern/central Bangka and eastern Belitung during the July–October dry season."
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#overview",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#overview",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "A spatio-temporal point process is a random collection of events identified by both time and location, such as disease cases, species sightings, or natural disasters. With the rise of geographically and temporally indexed data, analyzing these patterns has become increasingly important across many fields. In the past decade, several R methods and packages have been developed to support such analyses. This hands-on exercise demonstrates how these tools can be combined in a guided way, using forest fire events in Kepulauan Bangka Belitung, Indonesia (from 1 January to 31 December 2023) as a real-world case study to illustrate the procedures and interpretations."
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#learning-outcome",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#learning-outcome",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "The specific questions we would like to answer are:\n\nAre the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nIf the answer is NO, where and when the observed forest fire locations tend to cluster?\n\nA detailed discussion of the results, together with explicit insights and implications, is provided in Section 6.11 Discussion of Results (Answer to the Research Question)."
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#the-data",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#the-data",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Know what files you’ll use and where they come from.\n\nforestfires.csv: point events (each row = a fire). Has longitude, latitude, and date/time fields from MODIS.\nKepulauan_Bangka_Belitung shapefile: the study region polygon (administrative boundary). We’ll read only the Kepulauan Bangka Belitung subset for 2023 analyses."
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#installing-and-loading-the-r-packages",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Load all libraries used in this hands-on exercise.\n\npacman::p_load(sf,        # read, write, and transform spatial vector data\n               terra,      \n               spatstat,  # used for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc\n               sparr,     # spatio-temporal kernel density estimation (STKDE)\n               tmap,      # cartographic visualisation for quick maps\n               tidyverse  # readr (CSV), dplyr (mutate/select), ggplot, etc.\n               )"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#importing-and-preparing-study-area",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#importing-and-preparing-study-area",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "First. read the Kepulauan Bangka Belitung boundary, clean geometry, and set projected CRS.\n\n# Read the shapefile from the data folder -----------------------------------------------------\nkbb_sf &lt;- st_read(dsn = \"data/rds/BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG\",    # folder containing the shapefile\n                  layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG\") %&gt;%       # shapefile base name (no .shp)\n  st_union() %&gt;%                       # dissolve internal boundaries into one polygon\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%  # drop Z/M dimensions if present (keeps 2D)\n  st_transform(crs = 32748)            # reproject to EPSG:32748 (UTM Zone 48S in meters)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG' from data source `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex03/Data/rds/BATAS_DESA_DESEMBER_2019_DUKCAPIL_BANGKA_BELITUNG' \n  using driver `ESRI Shapefile'\nSimple feature collection with 391 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 105.1081 ymin: -3.416903 xmax: 108.848 ymax: -1.501757\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, convert the polygon (kb) to an observation window (owin) for point-pattern analysis.\n\n# Convert the sf polygon to an 'owin' (spatstat window) \nkbb_owin &lt;- as.owin(kbb_sf)      # turns the sf polygon into a spatstat window for ppp usage\nkbb_owin                         \n\nwindow: polygonal boundary\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n\n\n\n# Quick check that conversion succeeded\nclass(kbb_owin) \n\n[1] \"owin\""
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#importing-and-preparing-forest-fire-data",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#importing-and-preparing-forest-fire-data",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Next read the CSV, make it spatial, reproject to meters, and build helpful time fields.\n\nfire_sf &lt;- read_csv(\"data/rds/modis_2023_Indonesia.csv\")    %&gt;%  # load the event table\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%  # turn lon/lat cols into points (WGS84)\n  st_transform(crs = 32748)   # project to meters to match the study area\n\nRows: 52156 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (4): acq_time, satellite, instrument, daynight\ndbl  (10): latitude, longitude, brightness, scan, track, confidence, version...\ndate  (1): acq_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Add day-of-year and month fields used later\nfire_sf &lt;- fire_sf %&gt;% \n  mutate(DayofYear = yday(acq_date)) %&gt;%     # numeric day 1..365 from acquisition date\n  mutate(Month_num = month(acq_date)) %&gt;%    # numeric month 1..12\n  mutate(Month_fac = month(acq_date, label = TRUE, abbr = FALSE))  # factor month with full names (Jan..Dec)"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#visualising-the-fire-points",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#visualising-the-fire-points",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Plot one map with all 2023 fires over the study region\n\n# Bigger figure when rendering (Quarto/knitr)\n\n# Clip fire points so only those inside kbb_sf remain \nfire_sf &lt;- sf::st_intersection(fire_sf, kbb_sf)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Build a quick overall point map with tmap \ntmap_mode(\"plot\")                                         # use static plotting mode\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(kbb_sf) +                                        # add study area polygon layer\n  tm_polygons(col = \"grey85\", border.col = \"grey40\") +    # light fill, subtle border\n  tm_shape(fire_sf) +                                     # add fire points\n  tm_symbols(size = 0.5, col = \"red\") +                   # small red dots\n  tm_credits(\n    \"Forest Fire Points in 2023 (Kepulauan Bangka Belitung)\",\n    position = c(\"center\", \"top\"),    # center horizontally, top vertically\n    size = 1.2,                       # enlarge title text\n    fontface = 2                      # bold\n  ) +\n  tm_layout(\n    frame = TRUE,\n    # outer.margins = c(0.1, 0.05, 0.1, 0.05) # top, right, bottom, left (gives space for title)\n    inner.margins = 0.05                      # uncomment to maximize map area even more\n  )\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n\n\n\n\n\n\n\n\n\n\n\n\nNext, 12 small multiples (one map per month) like the practical guide.\n\n# Faceted monthly point maps \ntm_shape(kbb_sf) +\n  tm_polygons(col = \"grey85\", border.col = \"grey50\") +\n  tm_shape(fire_sf) +\n  tm_symbols(size = 0.1, col = \"red\") +\n  tm_facets(by = \"Month_fac\", ncol = 4) +                          # 4 columns x 3 rows facet layout\n  tm_layout(\n            frame = TRUE)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-month",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-month",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "In this section, we will learn how to compute STKDE by using spattemp.density() of sparr package.\n\n\nWe need to keep only the mark (month) needed by ppp() and the geometry.\n\n# Keep only the monthly mark and geometry for ppp creation \nfire_month &lt;- fire_sf %&gt;% \n  dplyr::select(Month_num)     # ppp requires a numeric/character mark column and geometry\n\n\n\n\nNext, we need to convert to a spatstat ppp (planar point pattern) object.\n\n# Convert sf points to ppp (planar point pattern) \nfire_month_ppp &lt;- as.ppp(fire_month)   # create ppp with coordinates and 'Month_num' as the mark\nfire_month_ppp   # print basic info to confirm number of points/window box\n\nMarked planar point pattern: 899 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 862416.4] x [9642001, 9828767] units\n\n\n\n# Sanity checks on the ppp \nsummary(fire_month_ppp)   # detailed summary (intensity, mark stats, window box)\n\nMarked planar point pattern:  899 points\nAverage intensity 1.412198e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.644  10.000  12.000 \n\nWindow: rectangle = [521564.1, 862416.4] x [9642001, 9828767] units\n                    (340900 x 186800 units)\nWindow area = 63659700000 square units\n\nany(duplicated(fire_month_ppp)) # TRUE if any exact duplicate point events; expect FALSE per notes\n\n[1] FALSE\n\n\n\n# Check if there are duplicated point events by using the code chunk below.\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n\nNext, we need to clip/assign the point pattern to the study window.\n\n# Attach/clip the ppp to our study-area window \nfire_month_owin &lt;- fire_month_ppp[kbb_owin]  # ensure points are analysed within the polygon boundary\nsummary(fire_month_owin)   # verify marks, counts, window area\n\nMarked planar point pattern:  899 points\nAverage intensity 5.380183e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.644  10.000  12.000 \n\nWindow: polygonal boundary\n319 separate polygons (40 holes)\n                   vertices         area relative.area\npolygon 1              9026  1.15464e+10      6.91e-01\npolygon 2                19  1.26217e+04      7.55e-07\npolygon 3 (hole)          3 -1.80096e-01     -1.08e-11\npolygon 4 (hole)          3 -9.26944e-01     -5.55e-11\npolygon 5 (hole)          3 -1.07382e+03     -6.43e-08\npolygon 6 (hole)          3 -6.41537e+02     -3.84e-08\npolygon 7                23  9.26156e+04      5.54e-06\npolygon 8                30  3.37146e+04      2.02e-06\npolygon 9                54  3.25211e+05      1.95e-05\npolygon 10               33  1.30244e+06      7.79e-05\npolygon 11 (hole)         4 -9.57353e+02     -5.73e-08\npolygon 12 (hole)         4 -2.84738e+02     -1.70e-08\npolygon 13 (hole)         3 -1.38747e+02     -8.30e-09\npolygon 14 (hole)         4 -5.31445e+02     -3.18e-08\npolygon 15 (hole)         3 -6.42241e+02     -3.84e-08\npolygon 16 (hole)         6 -1.13809e+02     -6.81e-09\npolygon 17 (hole)         4 -2.10482e+02     -1.26e-08\npolygon 18 (hole)         4 -9.54892e+01     -5.71e-09\npolygon 19 (hole)         4 -6.47895e+01     -3.88e-09\npolygon 20 (hole)         4 -7.10903e+01     -4.25e-09\npolygon 21 (hole)         4 -2.18071e+02     -1.31e-08\npolygon 22 (hole)         4 -4.44993e+01     -2.66e-09\npolygon 23 (hole)         4 -4.90009e+01     -2.93e-09\npolygon 24 (hole)         4 -6.48299e+01     -3.88e-09\npolygon 25 (hole)         4 -2.36910e+02     -1.42e-08\npolygon 26 (hole)         4 -3.07205e+01     -1.84e-09\npolygon 27 (hole)         4 -5.24443e+01     -3.14e-09\npolygon 28 (hole)         5 -6.64298e+01     -3.98e-09\npolygon 29 (hole)         4 -6.08287e+01     -3.64e-09\npolygon 30 (hole)         4 -1.12681e+02     -6.74e-09\npolygon 31              132  1.16480e+06      6.97e-05\npolygon 32 (hole)         4 -9.56157e+01     -5.72e-09\npolygon 33               68  1.91291e+05      1.14e-05\npolygon 34 (hole)         4 -5.31171e+02     -3.18e-08\npolygon 35 (hole)         3 -1.76143e+00     -1.05e-10\npolygon 36              116  1.51984e+05      9.10e-06\npolygon 37               95  1.14322e+05      6.84e-06\npolygon 38               21  4.51914e+03      2.70e-07\npolygon 39               24  1.18511e+04      7.09e-07\npolygon 40              130  1.52296e+05      9.11e-06\npolygon 41               36  1.07878e+04      6.46e-07\npolygon 42               54  3.46932e+05      2.08e-05\npolygon 43               71  2.15798e+05      1.29e-05\npolygon 44               12  1.76977e+03      1.06e-07\npolygon 45               75  1.05557e+05      6.32e-06\npolygon 46               17  4.22421e+03      2.53e-07\npolygon 47              116  1.64725e+05      9.86e-06\npolygon 48               36  1.15582e+04      6.92e-07\npolygon 49               15  3.48567e+03      2.09e-07\npolygon 50              325  3.08585e+06      1.85e-04\npolygon 51 (hole)         4 -2.57635e+02     -1.54e-08\npolygon 52              191  2.36948e+06      1.42e-04\npolygon 53               75  8.81673e+04      5.28e-06\npolygon 54               49  6.13133e+04      3.67e-06\npolygon 55              117  7.65040e+04      4.58e-06\npolygon 56                5  1.73964e+04      1.04e-06\npolygon 57             6115  4.61702e+09      2.76e-01\npolygon 58                4  6.35998e+03      3.81e-07\npolygon 59               16  1.18306e+05      7.08e-06\npolygon 60                5  2.11525e+04      1.27e-06\npolygon 61               11  3.52792e+04      2.11e-06\npolygon 62               13  6.20254e+04      3.71e-06\npolygon 63                8  1.23192e+04      7.37e-07\npolygon 64                5  1.08320e+04      6.48e-07\npolygon 65                5  9.14438e+03      5.47e-07\npolygon 66                6  9.61120e+03      5.75e-07\npolygon 67               31  1.54535e+06      9.25e-05\npolygon 68                9  2.89834e+04      1.73e-06\npolygon 69               10  2.76357e+04      1.65e-06\npolygon 70               12  8.99515e+04      5.38e-06\npolygon 71               15  5.42889e+04      3.25e-06\npolygon 72                8  1.58645e+04      9.49e-07\npolygon 73                6  1.35398e+04      8.10e-07\npolygon 74                4  4.29102e+03      2.57e-07\npolygon 75               23  1.76942e+05      1.06e-05\npolygon 76               44  4.35606e+04      2.61e-06\npolygon 77                5  6.83234e+03      4.09e-07\npolygon 78                5  6.67728e+03      4.00e-07\npolygon 79                5  2.24964e+03      1.35e-07\npolygon 80                7  2.62416e+04      1.57e-06\npolygon 81                9  1.19857e+04      7.17e-07\npolygon 82                8  2.25169e+04      1.35e-06\npolygon 83                6  1.21525e+04      7.27e-07\npolygon 84                5  8.76352e+03      5.24e-07\npolygon 85                9  2.28019e+04      1.36e-06\npolygon 86                6  9.21428e+03      5.51e-07\npolygon 87               49  1.00151e+05      5.99e-06\npolygon 88                5  4.45695e+03      2.67e-07\npolygon 89                4  2.90460e+03      1.74e-07\npolygon 90                4  3.68683e+03      2.21e-07\npolygon 91               64  9.90797e+04      5.93e-06\npolygon 92                5  3.03789e+03      1.82e-07\npolygon 93                6  1.05164e+04      6.29e-07\npolygon 94               12  9.48365e+03      5.68e-07\npolygon 95               13  6.95068e+04      4.16e-06\npolygon 96               34  2.01269e+05      1.20e-05\npolygon 97              222  1.23410e+06      7.39e-05\npolygon 98                4  2.60388e+03      1.56e-07\npolygon 99                7  8.47934e+03      5.07e-07\npolygon 100               5  8.09538e+03      4.84e-07\npolygon 101              93  2.08256e+05      1.25e-05\npolygon 102              16  7.78971e+04      4.66e-06\npolygon 103              24  1.58864e+04      9.51e-07\npolygon 104               4  8.84658e+02      5.29e-08\npolygon 105              42  4.25525e+04      2.55e-06\npolygon 106               5  4.40655e+03      2.64e-07\npolygon 107               4  2.45366e+03      1.47e-07\npolygon 108               5  4.70713e+03      2.82e-07\npolygon 109             214  2.22664e+06      1.33e-04\npolygon 110              60  5.44949e+04      3.26e-06\npolygon 111               6  4.54843e+03      2.72e-07\npolygon 112               6  1.47384e+04      8.82e-07\npolygon 113               6  1.35534e+04      8.11e-07\npolygon 114              51  9.00778e+04      5.39e-06\npolygon 115               4  1.53560e+03      9.19e-08\npolygon 116 (hole)        3 -3.48195e+02     -2.08e-08\npolygon 117              32  4.34406e+04      2.60e-06\npolygon 118             144  5.46612e+05      3.27e-05\npolygon 119              36  7.39285e+04      4.42e-06\npolygon 120              42  9.17456e+04      5.49e-06\npolygon 121             141  1.10139e+06      6.59e-05\npolygon 122              94  2.55406e+05      1.53e-05\npolygon 123              71  3.74605e+04      2.24e-06\npolygon 124              72  9.11248e+05      5.45e-05\npolygon 125 (hole)        3 -2.91215e+01     -1.74e-09\npolygon 126               5  4.23910e+03      2.54e-07\npolygon 127              47  1.10186e+05      6.59e-06\npolygon 128               6  1.00636e+04      6.02e-07\npolygon 129 (hole)        3 -3.94431e+01     -2.36e-09\npolygon 130               4  1.41401e+02      8.46e-09\npolygon 131              32  2.34725e+04      1.40e-06\npolygon 132               5  3.61887e+02      2.17e-08\npolygon 133 (hole)        3 -2.90329e+03     -1.74e-07\npolygon 134             120  3.80494e+05      2.28e-05\npolygon 135              19  3.13628e+04      1.88e-06\npolygon 136              42  1.21238e+05      7.26e-06\npolygon 137              24  3.83277e+04      2.29e-06\npolygon 138              88  4.56947e+05      2.73e-05\npolygon 139              20  1.78310e+04      1.07e-06\npolygon 140              20  2.39227e+04      1.43e-06\npolygon 141               5  6.65825e+03      3.98e-07\npolygon 142               5  5.41759e+03      3.24e-07\npolygon 143              53  4.55018e+05      2.72e-05\npolygon 144             263  1.76197e+07      1.05e-03\npolygon 145 (hole)        4 -3.70130e+03     -2.22e-07\npolygon 146               4  6.79585e+03      4.07e-07\npolygon 147              15  5.74657e+03      3.44e-07\npolygon 148              25  2.16665e+05      1.30e-05\npolygon 149               9  7.99755e+03      4.79e-07\npolygon 150               8  3.28848e+03      1.97e-07\npolygon 151              10  1.13482e+04      6.79e-07\npolygon 152              45  1.05243e+05      6.30e-06\npolygon 153               7  4.67607e+03      2.80e-07\npolygon 154             533  4.88598e+07      2.92e-03\npolygon 155             641  1.19875e+08      7.17e-03\npolygon 156              68  3.72768e+06      2.23e-04\npolygon 157              10  1.01914e+05      6.10e-06\npolygon 158              19  8.26881e+04      4.95e-06\npolygon 159              17  4.00986e+04      2.40e-06\npolygon 160              11  4.05798e+04      2.43e-06\npolygon 161             111  1.46838e+06      8.79e-05\npolygon 162             112  3.73964e+06      2.24e-04\npolygon 163              19  1.91730e+04      1.15e-06\npolygon 164              73  3.61914e+06      2.17e-04\npolygon 165              10  3.05707e+04      1.83e-06\npolygon 166               6  1.62989e+04      9.75e-07\npolygon 167              81  2.65465e+05      1.59e-05\npolygon 168               8  2.09366e+04      1.25e-06\npolygon 169               9  2.23138e+04      1.34e-06\npolygon 170              11  1.96456e+04      1.18e-06\npolygon 171 (hole)        3 -4.90052e+02     -2.93e-08\npolygon 172             145  7.13727e+06      4.27e-04\npolygon 173              60  1.45353e+05      8.70e-06\npolygon 174 (hole)        4 -3.71089e+02     -2.22e-08\npolygon 175              74  2.06447e+06      1.24e-04\npolygon 176              36  2.65019e+04      1.59e-06\npolygon 177             931  2.12400e+08      1.27e-02\npolygon 178 (hole)        4 -1.54888e+02     -9.27e-09\npolygon 179              25  2.21142e+05      1.32e-05\npolygon 180               6  6.87439e+03      4.11e-07\npolygon 181              31  1.67778e+06      1.00e-04\npolygon 182               7  9.00046e+03      5.39e-07\npolygon 183              12  6.30886e+04      3.78e-06\npolygon 184             202  2.25185e+07      1.35e-03\npolygon 185              46  2.39825e+06      1.44e-04\npolygon 186              25  1.05566e+04      6.32e-07\npolygon 187               8  4.61437e+04      2.76e-06\npolygon 188               7  3.16305e+04      1.89e-06\npolygon 189               5  6.49689e+03      3.89e-07\npolygon 190              12  2.17421e+04      1.30e-06\npolygon 191               5  7.19819e+03      4.31e-07\npolygon 192              11  6.58157e+05      3.94e-05\npolygon 193             159  1.11857e+06      6.69e-05\npolygon 194             104  3.97323e+05      2.38e-05\npolygon 195              14  3.77571e+05      2.26e-05\npolygon 196              27  1.21314e+06      7.26e-05\npolygon 197              69  7.97787e+04      4.77e-06\npolygon 198              51  6.46735e+04      3.87e-06\npolygon 199              71  7.20054e+04      4.31e-06\npolygon 200              46  3.29862e+04      1.97e-06\npolygon 201              52  3.88477e+04      2.32e-06\npolygon 202              59  5.31603e+06      3.18e-04\npolygon 203 (hole)        3 -3.70978e+01     -2.22e-09\npolygon 204              48  1.76671e+06      1.06e-04\npolygon 205              34  5.03002e+04      3.01e-06\npolygon 206              46  1.83138e+04      1.10e-06\npolygon 207              18  5.28224e+03      3.16e-07\npolygon 208             244  3.09431e+06      1.85e-04\npolygon 209              28  1.99131e+05      1.19e-05\npolygon 210              21  4.52072e+05      2.71e-05\npolygon 211              14  5.66637e+04      3.39e-06\npolygon 212              12  1.09825e+05      6.57e-06\npolygon 213              15  1.37135e+06      8.21e-05\npolygon 214               6  1.22312e+05      7.32e-06\npolygon 215              43  2.41150e+05      1.44e-05\npolygon 216               7  2.79840e+04      1.67e-06\npolygon 217              46  1.72671e+06      1.03e-04\npolygon 218              13  6.36540e+04      3.81e-06\npolygon 219              11  7.02064e+05      4.20e-05\npolygon 220              47  9.13826e+05      5.47e-05\npolygon 221 (hole)        4 -3.82299e+03     -2.29e-07\npolygon 222              62  3.95149e+06      2.36e-04\npolygon 223               9  4.34659e+05      2.60e-05\npolygon 224              14  4.20832e+04      2.52e-06\npolygon 225              18  5.12063e+03      3.06e-07\npolygon 226              31  7.44741e+05      4.46e-05\npolygon 227              18  4.11233e+05      2.46e-05\npolygon 228 (hole)        3 -5.32643e+02     -3.19e-08\npolygon 229              23  7.71654e+03      4.62e-07\npolygon 230               8  3.86022e+04      2.31e-06\npolygon 231             168  7.46341e+06      4.47e-04\npolygon 232               4  9.19978e+03      5.51e-07\npolygon 233              14  2.75390e+05      1.65e-05\npolygon 234               9  9.63798e+03      5.77e-07\npolygon 235               9  6.16361e+04      3.69e-06\npolygon 236              10  1.04975e+05      6.28e-06\npolygon 237               8  8.75225e+04      5.24e-06\npolygon 238               5  1.84001e+04      1.10e-06\npolygon 239               5  3.30315e+03      1.98e-07\npolygon 240              13  3.15610e+04      1.89e-06\npolygon 241              67  3.93597e+05      2.36e-05\npolygon 242               4  2.83740e+03      1.70e-07\npolygon 243              15  4.86091e+03      2.91e-07\npolygon 244               4  1.26784e+03      7.59e-08\npolygon 245               8  1.67287e+04      1.00e-06\npolygon 246              12  2.45858e+04      1.47e-06\npolygon 247              12  5.95789e+04      3.57e-06\npolygon 248              12  2.14651e+03      1.28e-07\npolygon 249              14  4.30849e+03      2.58e-07\npolygon 250              16  7.75733e+03      4.64e-07\npolygon 251 (hole)        4 -2.56408e+03     -1.53e-07\npolygon 252              10  1.75648e+03      1.05e-07\npolygon 253              34  3.85386e+06      2.31e-04\npolygon 254               9  3.55249e+03      2.13e-07\npolygon 255              15  1.39816e+04      8.37e-07\npolygon 256              16  4.56059e+05      2.73e-05\npolygon 257              26  1.42631e+04      8.54e-07\npolygon 258              36  2.65697e+04      1.59e-06\npolygon 259              19  4.48214e+03      2.68e-07\npolygon 260              96  5.26087e+05      3.15e-05\npolygon 261              60  1.00635e+05      6.02e-06\npolygon 262              31  7.13046e+04      4.27e-06\npolygon 263              84  1.37245e+06      8.21e-05\npolygon 264             178  8.26947e+05      4.95e-05\npolygon 265              58  1.98953e+05      1.19e-05\npolygon 266             115  3.13297e+05      1.87e-05\npolygon 267             132  4.76047e+05      2.85e-05\npolygon 268             102  1.63410e+07      9.78e-04\npolygon 269              18  6.36647e+03      3.81e-07\npolygon 270              10  7.72550e+03      4.62e-07\npolygon 271             204  1.42141e+06      8.51e-05\npolygon 272             125  2.33639e+06      1.40e-04\npolygon 273              30  2.58819e+04      1.55e-06\npolygon 274              24  2.06403e+04      1.24e-06\npolygon 275              87  1.25178e+06      7.49e-05\npolygon 276              38  4.04777e+04      2.42e-06\npolygon 277              10  5.04036e+04      3.02e-06\npolygon 278               6  1.12482e+04      6.73e-07\npolygon 279              40  1.56825e+05      9.39e-06\npolygon 280              63  5.25132e+04      3.14e-06\npolygon 281               8  1.11128e+04      6.65e-07\npolygon 282               9  1.49650e+03      8.96e-08\npolygon 283              13  4.50724e+03      2.70e-07\npolygon 284              41  4.12892e+04      2.47e-06\npolygon 285               7  3.03545e+03      1.82e-07\npolygon 286              19  7.54584e+03      4.52e-07\npolygon 287              16  8.29528e+03      4.96e-07\npolygon 288              26  5.16293e+05      3.09e-05\npolygon 289              13  9.23979e+04      5.53e-06\npolygon 290              25  4.18391e+04      2.50e-06\npolygon 291               7  3.85120e+03      2.30e-07\npolygon 292              82  4.38489e+05      2.62e-05\npolygon 293             163  3.46322e+06      2.07e-04\npolygon 294              11  5.22182e+03      3.13e-07\npolygon 295              13  3.28208e+04      1.96e-06\npolygon 296               6  3.00920e+04      1.80e-06\npolygon 297               6  1.13309e+04      6.78e-07\npolygon 298              13  2.03134e+05      1.22e-05\npolygon 299              36  2.87112e+04      1.72e-06\npolygon 300               6  1.13102e+04      6.77e-07\npolygon 301              13  4.04370e+03      2.42e-07\npolygon 302              44  1.67056e+05      1.00e-05\npolygon 303              11  6.66250e+04      3.99e-06\npolygon 304             129  2.29313e+06      1.37e-04\npolygon 305               6  7.23372e+04      4.33e-06\npolygon 306              19  6.36948e+03      3.81e-07\npolygon 307             106  1.06881e+06      6.40e-05\npolygon 308               9  2.30443e+04      1.38e-06\npolygon 309              29  9.02725e+03      5.40e-07\npolygon 310              68  1.08514e+05      6.49e-06\npolygon 311              64  2.19772e+05      1.32e-05\npolygon 312              25  2.58054e+05      1.54e-05\npolygon 313              26  4.59967e+04      2.75e-06\npolygon 314              57  8.30317e+04      4.97e-06\npolygon 315              83  1.13991e+06      6.82e-05\npolygon 316              11  3.01692e+04      1.81e-06\npolygon 317              14  1.30445e+05      7.81e-06\npolygon 318              22  2.37053e+05      1.42e-05\npolygon 319              22  1.84800e+05      1.11e-05\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n                     (416100 x 212200 units)\nWindow area = 16709500000 square units\nFraction of frame area: 0.189\n\n\n\n# Visual check that points and window align\nplot(fire_month_owin)   # base plot; circles show mark values (here: months 1..12)\n\n\n\n\n\n\n\n\n\n\n\nSubsequently, we will estimate spatio-temporal kernel density over space (meters) and discrete time (months).\n\n# Compute STKDE using sparr::spattemp.density \nst_kde &lt;- sparr::spattemp.density(fire_month_owin)  # uses default bandwidths; time is Month_num (1..12)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(st_kde)                                     # see chosen bandwidths h (space) and lambda (time), bounds, range\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 19155.99 (spatial)\n  lambda = 0.0264 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.201154e-39, 6.369146e-10]\n\n\n\n\n\nThereafter, we will reproduce month-specific density maps for July–December example.\n\n# Plot a subset of months (e.g., 7..12) with a fixed legend range for comparability \ntims &lt;- c(7,8,9,10,11,12)      # months to display (Jul..Dec), matches the sample figure\npar(mfcol = c(2,3))            # 2 rows x 3 columns panel\nfor(i in tims){                # loop through the selected months\n  plot(st_kde, i,              # draw the STKDE slice at month 'i'\n       override.par = FALSE,   # keep mfcol settings\n       fix.range = TRUE,       # use common color scale across panels for fair comparison\n       main = paste(\"KDE at month\", i))  # panel title\n}"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-day-of-year",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-day-of-year",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "In this section, we will repeat the workflow but using Day of Year (1..365) as the temporal mark.\n\n\n\n# Create a ppp whose mark is DayofYear (1..365) ----------------------------------------------\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  dplyr::select(DayofYear) %&gt;%     # keep only the DOY mark and geometry\n  as.ppp()                         # convert to ppp object\n\n\n\n\n\n# Attach the owin (study window) to the DayofYear ppp \nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin] # ensure points are constrained to study polygon\nsummary(fire_yday_owin)    # confirm counts, mark summary, window stats\n\nMarked planar point pattern:  899 points\nAverage intensity 5.380183e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   219.0   260.0   247.7   288.0   360.0 \n\nWindow: polygonal boundary\n319 separate polygons (40 holes)\n                   vertices         area relative.area\npolygon 1              9026  1.15464e+10      6.91e-01\npolygon 2                19  1.26217e+04      7.55e-07\npolygon 3 (hole)          3 -1.80096e-01     -1.08e-11\npolygon 4 (hole)          3 -9.26944e-01     -5.55e-11\npolygon 5 (hole)          3 -1.07382e+03     -6.43e-08\npolygon 6 (hole)          3 -6.41537e+02     -3.84e-08\npolygon 7                23  9.26156e+04      5.54e-06\npolygon 8                30  3.37146e+04      2.02e-06\npolygon 9                54  3.25211e+05      1.95e-05\npolygon 10               33  1.30244e+06      7.79e-05\npolygon 11 (hole)         4 -9.57353e+02     -5.73e-08\npolygon 12 (hole)         4 -2.84738e+02     -1.70e-08\npolygon 13 (hole)         3 -1.38747e+02     -8.30e-09\npolygon 14 (hole)         4 -5.31445e+02     -3.18e-08\npolygon 15 (hole)         3 -6.42241e+02     -3.84e-08\npolygon 16 (hole)         6 -1.13809e+02     -6.81e-09\npolygon 17 (hole)         4 -2.10482e+02     -1.26e-08\npolygon 18 (hole)         4 -9.54892e+01     -5.71e-09\npolygon 19 (hole)         4 -6.47895e+01     -3.88e-09\npolygon 20 (hole)         4 -7.10903e+01     -4.25e-09\npolygon 21 (hole)         4 -2.18071e+02     -1.31e-08\npolygon 22 (hole)         4 -4.44993e+01     -2.66e-09\npolygon 23 (hole)         4 -4.90009e+01     -2.93e-09\npolygon 24 (hole)         4 -6.48299e+01     -3.88e-09\npolygon 25 (hole)         4 -2.36910e+02     -1.42e-08\npolygon 26 (hole)         4 -3.07205e+01     -1.84e-09\npolygon 27 (hole)         4 -5.24443e+01     -3.14e-09\npolygon 28 (hole)         5 -6.64298e+01     -3.98e-09\npolygon 29 (hole)         4 -6.08287e+01     -3.64e-09\npolygon 30 (hole)         4 -1.12681e+02     -6.74e-09\npolygon 31              132  1.16480e+06      6.97e-05\npolygon 32 (hole)         4 -9.56157e+01     -5.72e-09\npolygon 33               68  1.91291e+05      1.14e-05\npolygon 34 (hole)         4 -5.31171e+02     -3.18e-08\npolygon 35 (hole)         3 -1.76143e+00     -1.05e-10\npolygon 36              116  1.51984e+05      9.10e-06\npolygon 37               95  1.14322e+05      6.84e-06\npolygon 38               21  4.51914e+03      2.70e-07\npolygon 39               24  1.18511e+04      7.09e-07\npolygon 40              130  1.52296e+05      9.11e-06\npolygon 41               36  1.07878e+04      6.46e-07\npolygon 42               54  3.46932e+05      2.08e-05\npolygon 43               71  2.15798e+05      1.29e-05\npolygon 44               12  1.76977e+03      1.06e-07\npolygon 45               75  1.05557e+05      6.32e-06\npolygon 46               17  4.22421e+03      2.53e-07\npolygon 47              116  1.64725e+05      9.86e-06\npolygon 48               36  1.15582e+04      6.92e-07\npolygon 49               15  3.48567e+03      2.09e-07\npolygon 50              325  3.08585e+06      1.85e-04\npolygon 51 (hole)         4 -2.57635e+02     -1.54e-08\npolygon 52              191  2.36948e+06      1.42e-04\npolygon 53               75  8.81673e+04      5.28e-06\npolygon 54               49  6.13133e+04      3.67e-06\npolygon 55              117  7.65040e+04      4.58e-06\npolygon 56                5  1.73964e+04      1.04e-06\npolygon 57             6115  4.61702e+09      2.76e-01\npolygon 58                4  6.35998e+03      3.81e-07\npolygon 59               16  1.18306e+05      7.08e-06\npolygon 60                5  2.11525e+04      1.27e-06\npolygon 61               11  3.52792e+04      2.11e-06\npolygon 62               13  6.20254e+04      3.71e-06\npolygon 63                8  1.23192e+04      7.37e-07\npolygon 64                5  1.08320e+04      6.48e-07\npolygon 65                5  9.14438e+03      5.47e-07\npolygon 66                6  9.61120e+03      5.75e-07\npolygon 67               31  1.54535e+06      9.25e-05\npolygon 68                9  2.89834e+04      1.73e-06\npolygon 69               10  2.76357e+04      1.65e-06\npolygon 70               12  8.99515e+04      5.38e-06\npolygon 71               15  5.42889e+04      3.25e-06\npolygon 72                8  1.58645e+04      9.49e-07\npolygon 73                6  1.35398e+04      8.10e-07\npolygon 74                4  4.29102e+03      2.57e-07\npolygon 75               23  1.76942e+05      1.06e-05\npolygon 76               44  4.35606e+04      2.61e-06\npolygon 77                5  6.83234e+03      4.09e-07\npolygon 78                5  6.67728e+03      4.00e-07\npolygon 79                5  2.24964e+03      1.35e-07\npolygon 80                7  2.62416e+04      1.57e-06\npolygon 81                9  1.19857e+04      7.17e-07\npolygon 82                8  2.25169e+04      1.35e-06\npolygon 83                6  1.21525e+04      7.27e-07\npolygon 84                5  8.76352e+03      5.24e-07\npolygon 85                9  2.28019e+04      1.36e-06\npolygon 86                6  9.21428e+03      5.51e-07\npolygon 87               49  1.00151e+05      5.99e-06\npolygon 88                5  4.45695e+03      2.67e-07\npolygon 89                4  2.90460e+03      1.74e-07\npolygon 90                4  3.68683e+03      2.21e-07\npolygon 91               64  9.90797e+04      5.93e-06\npolygon 92                5  3.03789e+03      1.82e-07\npolygon 93                6  1.05164e+04      6.29e-07\npolygon 94               12  9.48365e+03      5.68e-07\npolygon 95               13  6.95068e+04      4.16e-06\npolygon 96               34  2.01269e+05      1.20e-05\npolygon 97              222  1.23410e+06      7.39e-05\npolygon 98                4  2.60388e+03      1.56e-07\npolygon 99                7  8.47934e+03      5.07e-07\npolygon 100               5  8.09538e+03      4.84e-07\npolygon 101              93  2.08256e+05      1.25e-05\npolygon 102              16  7.78971e+04      4.66e-06\npolygon 103              24  1.58864e+04      9.51e-07\npolygon 104               4  8.84658e+02      5.29e-08\npolygon 105              42  4.25525e+04      2.55e-06\npolygon 106               5  4.40655e+03      2.64e-07\npolygon 107               4  2.45366e+03      1.47e-07\npolygon 108               5  4.70713e+03      2.82e-07\npolygon 109             214  2.22664e+06      1.33e-04\npolygon 110              60  5.44949e+04      3.26e-06\npolygon 111               6  4.54843e+03      2.72e-07\npolygon 112               6  1.47384e+04      8.82e-07\npolygon 113               6  1.35534e+04      8.11e-07\npolygon 114              51  9.00778e+04      5.39e-06\npolygon 115               4  1.53560e+03      9.19e-08\npolygon 116 (hole)        3 -3.48195e+02     -2.08e-08\npolygon 117              32  4.34406e+04      2.60e-06\npolygon 118             144  5.46612e+05      3.27e-05\npolygon 119              36  7.39285e+04      4.42e-06\npolygon 120              42  9.17456e+04      5.49e-06\npolygon 121             141  1.10139e+06      6.59e-05\npolygon 122              94  2.55406e+05      1.53e-05\npolygon 123              71  3.74605e+04      2.24e-06\npolygon 124              72  9.11248e+05      5.45e-05\npolygon 125 (hole)        3 -2.91215e+01     -1.74e-09\npolygon 126               5  4.23910e+03      2.54e-07\npolygon 127              47  1.10186e+05      6.59e-06\npolygon 128               6  1.00636e+04      6.02e-07\npolygon 129 (hole)        3 -3.94431e+01     -2.36e-09\npolygon 130               4  1.41401e+02      8.46e-09\npolygon 131              32  2.34725e+04      1.40e-06\npolygon 132               5  3.61887e+02      2.17e-08\npolygon 133 (hole)        3 -2.90329e+03     -1.74e-07\npolygon 134             120  3.80494e+05      2.28e-05\npolygon 135              19  3.13628e+04      1.88e-06\npolygon 136              42  1.21238e+05      7.26e-06\npolygon 137              24  3.83277e+04      2.29e-06\npolygon 138              88  4.56947e+05      2.73e-05\npolygon 139              20  1.78310e+04      1.07e-06\npolygon 140              20  2.39227e+04      1.43e-06\npolygon 141               5  6.65825e+03      3.98e-07\npolygon 142               5  5.41759e+03      3.24e-07\npolygon 143              53  4.55018e+05      2.72e-05\npolygon 144             263  1.76197e+07      1.05e-03\npolygon 145 (hole)        4 -3.70130e+03     -2.22e-07\npolygon 146               4  6.79585e+03      4.07e-07\npolygon 147              15  5.74657e+03      3.44e-07\npolygon 148              25  2.16665e+05      1.30e-05\npolygon 149               9  7.99755e+03      4.79e-07\npolygon 150               8  3.28848e+03      1.97e-07\npolygon 151              10  1.13482e+04      6.79e-07\npolygon 152              45  1.05243e+05      6.30e-06\npolygon 153               7  4.67607e+03      2.80e-07\npolygon 154             533  4.88598e+07      2.92e-03\npolygon 155             641  1.19875e+08      7.17e-03\npolygon 156              68  3.72768e+06      2.23e-04\npolygon 157              10  1.01914e+05      6.10e-06\npolygon 158              19  8.26881e+04      4.95e-06\npolygon 159              17  4.00986e+04      2.40e-06\npolygon 160              11  4.05798e+04      2.43e-06\npolygon 161             111  1.46838e+06      8.79e-05\npolygon 162             112  3.73964e+06      2.24e-04\npolygon 163              19  1.91730e+04      1.15e-06\npolygon 164              73  3.61914e+06      2.17e-04\npolygon 165              10  3.05707e+04      1.83e-06\npolygon 166               6  1.62989e+04      9.75e-07\npolygon 167              81  2.65465e+05      1.59e-05\npolygon 168               8  2.09366e+04      1.25e-06\npolygon 169               9  2.23138e+04      1.34e-06\npolygon 170              11  1.96456e+04      1.18e-06\npolygon 171 (hole)        3 -4.90052e+02     -2.93e-08\npolygon 172             145  7.13727e+06      4.27e-04\npolygon 173              60  1.45353e+05      8.70e-06\npolygon 174 (hole)        4 -3.71089e+02     -2.22e-08\npolygon 175              74  2.06447e+06      1.24e-04\npolygon 176              36  2.65019e+04      1.59e-06\npolygon 177             931  2.12400e+08      1.27e-02\npolygon 178 (hole)        4 -1.54888e+02     -9.27e-09\npolygon 179              25  2.21142e+05      1.32e-05\npolygon 180               6  6.87439e+03      4.11e-07\npolygon 181              31  1.67778e+06      1.00e-04\npolygon 182               7  9.00046e+03      5.39e-07\npolygon 183              12  6.30886e+04      3.78e-06\npolygon 184             202  2.25185e+07      1.35e-03\npolygon 185              46  2.39825e+06      1.44e-04\npolygon 186              25  1.05566e+04      6.32e-07\npolygon 187               8  4.61437e+04      2.76e-06\npolygon 188               7  3.16305e+04      1.89e-06\npolygon 189               5  6.49689e+03      3.89e-07\npolygon 190              12  2.17421e+04      1.30e-06\npolygon 191               5  7.19819e+03      4.31e-07\npolygon 192              11  6.58157e+05      3.94e-05\npolygon 193             159  1.11857e+06      6.69e-05\npolygon 194             104  3.97323e+05      2.38e-05\npolygon 195              14  3.77571e+05      2.26e-05\npolygon 196              27  1.21314e+06      7.26e-05\npolygon 197              69  7.97787e+04      4.77e-06\npolygon 198              51  6.46735e+04      3.87e-06\npolygon 199              71  7.20054e+04      4.31e-06\npolygon 200              46  3.29862e+04      1.97e-06\npolygon 201              52  3.88477e+04      2.32e-06\npolygon 202              59  5.31603e+06      3.18e-04\npolygon 203 (hole)        3 -3.70978e+01     -2.22e-09\npolygon 204              48  1.76671e+06      1.06e-04\npolygon 205              34  5.03002e+04      3.01e-06\npolygon 206              46  1.83138e+04      1.10e-06\npolygon 207              18  5.28224e+03      3.16e-07\npolygon 208             244  3.09431e+06      1.85e-04\npolygon 209              28  1.99131e+05      1.19e-05\npolygon 210              21  4.52072e+05      2.71e-05\npolygon 211              14  5.66637e+04      3.39e-06\npolygon 212              12  1.09825e+05      6.57e-06\npolygon 213              15  1.37135e+06      8.21e-05\npolygon 214               6  1.22312e+05      7.32e-06\npolygon 215              43  2.41150e+05      1.44e-05\npolygon 216               7  2.79840e+04      1.67e-06\npolygon 217              46  1.72671e+06      1.03e-04\npolygon 218              13  6.36540e+04      3.81e-06\npolygon 219              11  7.02064e+05      4.20e-05\npolygon 220              47  9.13826e+05      5.47e-05\npolygon 221 (hole)        4 -3.82299e+03     -2.29e-07\npolygon 222              62  3.95149e+06      2.36e-04\npolygon 223               9  4.34659e+05      2.60e-05\npolygon 224              14  4.20832e+04      2.52e-06\npolygon 225              18  5.12063e+03      3.06e-07\npolygon 226              31  7.44741e+05      4.46e-05\npolygon 227              18  4.11233e+05      2.46e-05\npolygon 228 (hole)        3 -5.32643e+02     -3.19e-08\npolygon 229              23  7.71654e+03      4.62e-07\npolygon 230               8  3.86022e+04      2.31e-06\npolygon 231             168  7.46341e+06      4.47e-04\npolygon 232               4  9.19978e+03      5.51e-07\npolygon 233              14  2.75390e+05      1.65e-05\npolygon 234               9  9.63798e+03      5.77e-07\npolygon 235               9  6.16361e+04      3.69e-06\npolygon 236              10  1.04975e+05      6.28e-06\npolygon 237               8  8.75225e+04      5.24e-06\npolygon 238               5  1.84001e+04      1.10e-06\npolygon 239               5  3.30315e+03      1.98e-07\npolygon 240              13  3.15610e+04      1.89e-06\npolygon 241              67  3.93597e+05      2.36e-05\npolygon 242               4  2.83740e+03      1.70e-07\npolygon 243              15  4.86091e+03      2.91e-07\npolygon 244               4  1.26784e+03      7.59e-08\npolygon 245               8  1.67287e+04      1.00e-06\npolygon 246              12  2.45858e+04      1.47e-06\npolygon 247              12  5.95789e+04      3.57e-06\npolygon 248              12  2.14651e+03      1.28e-07\npolygon 249              14  4.30849e+03      2.58e-07\npolygon 250              16  7.75733e+03      4.64e-07\npolygon 251 (hole)        4 -2.56408e+03     -1.53e-07\npolygon 252              10  1.75648e+03      1.05e-07\npolygon 253              34  3.85386e+06      2.31e-04\npolygon 254               9  3.55249e+03      2.13e-07\npolygon 255              15  1.39816e+04      8.37e-07\npolygon 256              16  4.56059e+05      2.73e-05\npolygon 257              26  1.42631e+04      8.54e-07\npolygon 258              36  2.65697e+04      1.59e-06\npolygon 259              19  4.48214e+03      2.68e-07\npolygon 260              96  5.26087e+05      3.15e-05\npolygon 261              60  1.00635e+05      6.02e-06\npolygon 262              31  7.13046e+04      4.27e-06\npolygon 263              84  1.37245e+06      8.21e-05\npolygon 264             178  8.26947e+05      4.95e-05\npolygon 265              58  1.98953e+05      1.19e-05\npolygon 266             115  3.13297e+05      1.87e-05\npolygon 267             132  4.76047e+05      2.85e-05\npolygon 268             102  1.63410e+07      9.78e-04\npolygon 269              18  6.36647e+03      3.81e-07\npolygon 270              10  7.72550e+03      4.62e-07\npolygon 271             204  1.42141e+06      8.51e-05\npolygon 272             125  2.33639e+06      1.40e-04\npolygon 273              30  2.58819e+04      1.55e-06\npolygon 274              24  2.06403e+04      1.24e-06\npolygon 275              87  1.25178e+06      7.49e-05\npolygon 276              38  4.04777e+04      2.42e-06\npolygon 277              10  5.04036e+04      3.02e-06\npolygon 278               6  1.12482e+04      6.73e-07\npolygon 279              40  1.56825e+05      9.39e-06\npolygon 280              63  5.25132e+04      3.14e-06\npolygon 281               8  1.11128e+04      6.65e-07\npolygon 282               9  1.49650e+03      8.96e-08\npolygon 283              13  4.50724e+03      2.70e-07\npolygon 284              41  4.12892e+04      2.47e-06\npolygon 285               7  3.03545e+03      1.82e-07\npolygon 286              19  7.54584e+03      4.52e-07\npolygon 287              16  8.29528e+03      4.96e-07\npolygon 288              26  5.16293e+05      3.09e-05\npolygon 289              13  9.23979e+04      5.53e-06\npolygon 290              25  4.18391e+04      2.50e-06\npolygon 291               7  3.85120e+03      2.30e-07\npolygon 292              82  4.38489e+05      2.62e-05\npolygon 293             163  3.46322e+06      2.07e-04\npolygon 294              11  5.22182e+03      3.13e-07\npolygon 295              13  3.28208e+04      1.96e-06\npolygon 296               6  3.00920e+04      1.80e-06\npolygon 297               6  1.13309e+04      6.78e-07\npolygon 298              13  2.03134e+05      1.22e-05\npolygon 299              36  2.87112e+04      1.72e-06\npolygon 300               6  1.13102e+04      6.77e-07\npolygon 301              13  4.04370e+03      2.42e-07\npolygon 302              44  1.67056e+05      1.00e-05\npolygon 303              11  6.66250e+04      3.99e-06\npolygon 304             129  2.29313e+06      1.37e-04\npolygon 305               6  7.23372e+04      4.33e-06\npolygon 306              19  6.36948e+03      3.81e-07\npolygon 307             106  1.06881e+06      6.40e-05\npolygon 308               9  2.30443e+04      1.38e-06\npolygon 309              29  9.02725e+03      5.40e-07\npolygon 310              68  1.08514e+05      6.49e-06\npolygon 311              64  2.19772e+05      1.32e-05\npolygon 312              25  2.58054e+05      1.54e-05\npolygon 313              26  4.59967e+04      2.75e-06\npolygon 314              57  8.30317e+04      4.97e-06\npolygon 315              83  1.13991e+06      6.82e-05\npolygon 316              11  3.01692e+04      1.81e-06\npolygon 317              14  1.30445e+05      7.81e-06\npolygon 318              22  2.37053e+05      1.42e-05\npolygon 319              22  1.84800e+05      1.11e-05\nenclosing rectangle: [512017.1, 928107.3] x [9621818, 9833989] units\n                     (416100 x 212200 units)\nWindow area = 16709500000 square units\nFraction of frame area: 0.189\n\n\n\n\n\nWe aim to compute STKDE with default bandwidths and plot.\n\n# Compute STKDE using default bandwidth selection (gives baseline) \nkde_yday &lt;- spattemp.density(\n  fire_yday_owin        # ppp with DayofYear marks and study window\n)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)       # examine space/time bandwidths (h, lambda) and density range\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 19155.99 (spatial)\n  lambda = 6.4456 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [10, 360]\n\nEvaluation\n  128 x 128 x 351 trivariate lattice\n  Density range: [1.594274e-28, 1.857482e-12]\n\n\n\n# Visualise the density surface aggregated over time (default plot) \nplot(kde_yday)     # continuous surface over the region with legend"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-day-of-year-improved-method",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#computing-stkde-by-day-of-year-improved-method",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Why this step? Default bandwidths are generic. In this section, want to improve them by bootstrap MISE using BOOT.spattemp(), which estimates a scalar spatial bandwidth (h, meters) and a scalar temporal bandwidth (lambda, in DOY units) suited to our data.\n\n# For reproducibility of the bootstrap selection \nset.seed(1234)    # fixed seed so results are repeatable in class\n\n# Run bootstrap bandwidth selection (may take some time) \nBOOT.spattemp(fire_yday_owin)   # prints many trial (h, lambda) and the final pair at bottom\n\nInitialising...Done.\nOptimising...\nh = 19155.99 \b; lambda = 15.11399 \nh = 21071.59 \b; lambda = 15.11399 \nh = 19155.99 \b; lambda = 1930.713 \nh = 19634.89 \b; lambda = 972.9137 \nh = 19874.34 \b; lambda = 494.0138 \nh = 19994.07 \b; lambda = 254.5639 \nh = 20053.93 \b; lambda = 134.8389 \nh = 20083.86 \b; lambda = 74.97647 \nh = 20098.83 \b; lambda = 45.04523 \nh = 20106.31 \b; lambda = 30.07961 \nh = 20121.28 \b; lambda = 0.1483655 \nh = 20110.05 \b; lambda = 22.5968 \nh = 22025.65 \b; lambda = 22.5968 \nh = 23460.48 \b; lambda = 26.3382 \nh = 22498.94 \b; lambda = 33.82101 \nh = 21428.43 \b; lambda = 19.79074 \nh = 24778.86 \b; lambda = 23.53215 \nh = 27113.26 \b; lambda = 23.99982 \nh = 29145.31 \b; lambda = 30.54728 \nh = 27216.09 \b; lambda = 27.85815 \nh = 30868.87 \b; lambda = 25.51977 \nh = 34573.07 \b; lambda = 25.11055 \nh = 34470.24 \b; lambda = 21.25223 \nh = 38097.31 \b; lambda = 17.94927 \nh = 45557.12 \b; lambda = 19.06 \nh = 54779.04 \b; lambda = 16.59009 \nh = 49081.36 \b; lambda = 11.89872 \nh = 45454.29 \b; lambda = 15.20168 \nh = 38200.14 \b; lambda = 21.80759 \nh = 43640.75 \b; lambda = 16.85315 \nh = 51100.56 \b; lambda = 17.96388 \nh = 47849.75 \b; lambda = 17.96023 \nh = 49766.11 \b; lambda = 20.16708 \nh = 45172.09 \b; lambda = 17.68163 \nh = 42879.46 \b; lambda = 18.7814 \nh = 44122.03 \b; lambda = 18.57611 \nh = 44507.06 \b; lambda = 19.95447 \nh = 44673.32 \b; lambda = 19.38626 \nDone.\n\n\n          h      lambda \n44673.31732    19.38626 \n\n# Note the final recommended h and lambda reported by the function output.\n\n\n\nWe will re-run STKDE using the recommended bandwidths.\n\n# Refit STKDE using the bootstrap MISE recommended bandwidths \nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,     # the DayofYear ppp constrained by the study window\n  h      = 45000,     # spatial bandwidth in meters (per improved selection)\n  lambda = 19         # temporal bandwidth in days-of-year (per improved selection)\n)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)     # verify the bandwidths and evaluation grid\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 45000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  899 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512017.1, 928107.3] x [9621818, 9833989]\n\nTemporal bound\n  [10, 360]\n\nEvaluation\n  128 x 128 x 351 trivariate lattice\n  Density range: [3.902314e-16, 9.815536e-13]\n\n\n\n\n\nFinally, we display the final improved surface.\n\n# Plot the improved STKDE surface\nplot(kde_yday)   # shows the density surface with color bar; higher values = higher intensity"
  },
  {
    "objectID": "Hands-on_Ex03/Hand-on_Ex03.html#discussion-of-results-answer-to-the-research-question",
    "href": "Hands-on_Ex03/Hand-on_Ex03.html#discussion-of-results-answer-to-the-research-question",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "The spatio-temporal kernel density estimation (STKDE) provides clear evidence that the locations of forest fires in Kepulauan Bangka Belitung are not spatially or spatio-temporally independent. If the events were independent, the density surfaces would be flat and homogeneous across space and time. Instead, the STKDE outputs reveal distinct peaks, rejecting the null hypothesis of independence.\nThe bootstrap-based bandwidth selection process identified an optimal smoothing window of h ≈ 44.7 km (spatial) and λ ≈ 19.4 days (temporal). These parameters indicate that fire events tend to cluster within a radius of ~45 km and persist across approximately three-week periods. The density range produced, from very low background values (~1.2e-39) to sharp peaks (~6.4e-10), confirms substantial clustering intensity.\nThe spatial clustering is most pronounced in southern and central Bangka and in eastern Belitung, where the STKDE consistently highlights hotspots of higher fire intensity. These locations repeatedly emerge in the density surfaces, showing that fire events are concentrated in specific sub-regions rather than being uniformly distributed.\nTemporally, clustering is most evident during the July–October period, corresponding to the dry season. The monthly KDE plots demonstrate that fire intensity builds steadily in July, peaks in September, and declines toward the year’s end. This seasonality shows that fire occurrence is not equally likely throughout the year but is instead conditioned by climatic and land-surface factors linked to the dry months.\nThe implications of these findings are important for both science and policy. Spatial dependence means that fire risk is localized, and mitigation resources should be concentrated in the fire-prone regions identified by the STKDE. Temporal dependence means that efforts should be time-targeted, with enhanced monitoring and preventive measures deployed during the critical dry-season window. This combination of spatial and temporal clustering suggests that forest fires in Bangka Belitung are shaped by systematic environmental drivers, not by random chance.\nIn summary, the STKDE results demonstrate that forest fire events are spatially and spatio-temporally dependent. They cluster within ~45 km regions and persist across ~3-week intervals, with the highest concentrations observed in southern/central Bangka and eastern Belitung during the July–October dry season."
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html",
    "href": "Hands-on_Ex04/hand-on_ex04.html",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute spatial weights using R.\nBy the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\n\nimport csv file using appropriate function of readr package,\n\nperform relational join using appropriate join function of dplyr package,\n\ncompute spatial weights using appropriate functions of spdep package, and calculate spatially lagged variables using appropriate functions of spdep package.\n\n\n\n\nTwo data sets will be used in this hands-on exercise, they are: - Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. - Hunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n# \"pacman\" automatically installs and loads packages if not already installed.\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\n\n\n\n\n# Read the Hunan county polygons as an sf object (modern spatial class)\nhunan &lt;- sf::st_read(dsn = \"data/geospatial\", layer = \"Hunan\")  # dsn = folder, layer = shapefile basename\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n# Inspect the basic structure to confirm geometry + fields\nprint(hunan)    # shows feature count, geometry type, CRS, attributes\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3 Shape_Leng Shape_Area    County\n1   Changde 21098   Anxiang      County   1.869074 0.10056190   Anxiang\n2   Changde 21100   Hanshou      County   2.360691 0.19978745   Hanshou\n3   Changde 21101    Jinshi County City   1.425620 0.05302413    Jinshi\n4   Changde 21102        Li      County   3.474325 0.18908121        Li\n5   Changde 21103     Linli      County   2.289506 0.11450357     Linli\n6   Changde 21104    Shimen      County   4.171918 0.37194707    Shimen\n7  Changsha 21109   Liuyang County City   4.060579 0.46016789   Liuyang\n8  Changsha 21110 Ningxiang      County   3.323754 0.26614198 Ningxiang\n9  Changsha 21111 Wangcheng      County   2.292093 0.13049161 Wangcheng\n10 Chenzhou 21112     Anren      County   2.240739 0.13343936     Anren\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n# Read county-level attributes (e.g., GDPPC) for 2012\nhunan2012 &lt;- readr::read_csv(\"data/aspatial/Hunan_2012.csv\")   # creates a regular tibble/data.frame\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Peek at columns so we know what to join\ndplyr::glimpse(hunan2012)   # confirms column names such as County, GDPPC, etc.\n\nRows: 88\nColumns: 29\n$ County      &lt;chr&gt; \"Anhua\", \"Anren\", \"Anxiang\", \"Baojing\", \"Chaling\", \"Changn…\n$ City        &lt;chr&gt; \"Yiyang\", \"Chenzhou\", \"Changde\", \"Hunan West\", \"Zhuzhou\", …\n$ avg_wage    &lt;dbl&gt; 30544, 28058, 31935, 30843, 31251, 28518, 54540, 28597, 33…\n$ deposite    &lt;dbl&gt; 10967.0, 4598.9, 5517.2, 2250.0, 8241.4, 10860.0, 24332.0,…\n$ FAI         &lt;dbl&gt; 6831.7, 6386.1, 3541.0, 1005.4, 6508.4, 7920.0, 33624.0, 1…\n$ Gov_Rev     &lt;dbl&gt; 456.72, 220.57, 243.64, 192.59, 620.19, 769.86, 5350.00, 1…\n$ Gov_Exp     &lt;dbl&gt; 2703.0, 1454.7, 1779.5, 1379.1, 1947.0, 2631.6, 7885.5, 11…\n$ GDP         &lt;dbl&gt; 13225.0, 4941.2, 12482.0, 4087.9, 11585.0, 19886.0, 88009.…\n$ GDPPC       &lt;dbl&gt; 14567, 12761, 23667, 14563, 20078, 24418, 88656, 10132, 17…\n$ GIO         &lt;dbl&gt; 9276.90, 4189.20, 5108.90, 3623.50, 9157.70, 37392.00, 513…\n$ Loan        &lt;dbl&gt; 3954.90, 2555.30, 2806.90, 1253.70, 4287.40, 4242.80, 4053…\n$ NIPCR       &lt;dbl&gt; 3528.3, 3271.8, 7693.7, 4191.3, 3887.7, 9528.0, 17070.0, 3…\n$ Bed         &lt;dbl&gt; 2718, 970, 1931, 927, 1449, 3605, 3310, 582, 2170, 2179, 1…\n$ Emp         &lt;dbl&gt; 494.310, 290.820, 336.390, 195.170, 330.290, 548.610, 670.…\n$ EmpR        &lt;dbl&gt; 441.4, 255.4, 270.5, 145.6, 299.0, 415.1, 452.0, 127.6, 21…\n$ EmpRT       &lt;dbl&gt; 338.0, 99.4, 205.9, 116.4, 154.0, 273.7, 219.4, 94.4, 174.…\n$ Pri_Stu     &lt;dbl&gt; 54.175, 33.171, 19.584, 19.249, 33.906, 81.831, 59.151, 18…\n$ Sec_Stu     &lt;dbl&gt; 32.830, 17.505, 17.819, 11.831, 20.548, 44.485, 39.685, 7.…\n$ Household   &lt;dbl&gt; 290.4, 104.6, 148.1, 73.2, 148.7, 211.2, 300.3, 76.1, 139.…\n$ Household_R &lt;dbl&gt; 234.5, 121.9, 135.4, 69.9, 139.4, 211.7, 248.4, 59.6, 110.…\n$ NOIP        &lt;dbl&gt; 101, 34, 53, 18, 106, 115, 214, 17, 55, 70, 44, 84, 74, 17…\n$ Pop_R       &lt;dbl&gt; 670.3, 243.2, 346.0, 184.1, 301.6, 448.2, 475.1, 189.6, 31…\n$ RSCG        &lt;dbl&gt; 5760.60, 2386.40, 3957.90, 768.04, 4009.50, 5220.40, 22604…\n$ Pop_T       &lt;dbl&gt; 910.8, 388.7, 528.3, 281.3, 578.4, 816.3, 998.6, 256.7, 45…\n$ Agri        &lt;dbl&gt; 4942.253, 2357.764, 4524.410, 1118.561, 3793.550, 6430.782…\n$ Service     &lt;dbl&gt; 5414.5, 3814.1, 14100.0, 541.8, 5444.0, 13074.6, 17726.6, …\n$ Disp_Inc    &lt;dbl&gt; 12373, 16072, 16610, 13455, 20461, 20868, 183252, 12379, 1…\n$ RORP        &lt;dbl&gt; 0.7359464, 0.6256753, 0.6549309, 0.6544614, 0.5214385, 0.5…\n$ ROREmp      &lt;dbl&gt; 0.8929619, 0.8782065, 0.8041262, 0.7460163, 0.9052651, 0.7…\n\n\n\n\n\n\n# Left join: keep all polygons and bring matching columns from hunan2012 by common key(s)\nhunan &lt;- dplyr::left_join(hunan, hunan2012) %&gt;%   # merge attributes into polygons\n  dplyr::select(1:4, 7, 15)      # reproduce Prof’s column subset (indices from slides)\n\nJoining with `by = join_by(County)`\n\n# Inspect and verify that GDPPC is now present alongside geometry\nhead(hunan)   # should list fields incl. County, GDPPC, geometry\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\n\n\n\n\n\n# Prepare a simple basemap with names (quick tmap)\nbasemap &lt;- tmap::tm_shape(hunan) +                        # tell tmap what to draw (the polygons)\n  tmap::tm_polygons() +                                   # draw filled polygons\n  tmap::tm_text(\"NAME_3\", size = 0.5)                     # label polygons with county names (field matches slides)\n\n# Choropleth of GDPPC \ngdppc_map &lt;- tmap::qtm(hunan, \"GDPPC\") +                  # quick thematic map of GDPPC\n  tmap::tm_layout(legend.position = c(\"left\", \"bottom\"))  # put legend bottom-left to match the slides\n\n# Show basemap and GDPPC side-by-side\ntmap::tmap_arrange(basemap, gdppc_map, asp = 1, ncol = 2) # arrange 2 maps in one row, square aspect\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If we look at the documentation, we will see that we can pass a “queen” argument that takes TRUE or FALSE as options. If we do not specify this argument the default is set to TRUE, that is, if we don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\n\n# Build a neighbours list where polygons touching at edges OR corners are neighbours\nwm_q &lt;- spdep::poly2nb(hunan, queen = TRUE)    # queen = TRUE includes corner touches\nsummary(wm_q)   # prints number of regions, links, and link distribution\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n# Explore neighbours of polygon 1 (IDs are row positions of neighbours)\nwm_q[[1]]   # e.g., returns integer vector like c(2, 3, 4, 57, 85)\n\n[1]  2  3  4 57 85\n\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\n\nWe can retrieve the county name of Polygon ID=1 by using the code chunk below:\n\n# Retrieve county name for polygon ID = 1\nhunan$County[1]   # should print the county name for ID 1\n\n[1] \"Anxiang\"\n\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\n\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n# Retrieve the county names of polygon 1’s neighbours\nhunan$NAME_3[c(2, 3, 4, 57, 85)]    # show human-readable names for the neighbour IDs\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below:\n\n# Pull the neighbours’ GDPPC values for polygon 1\nnb1_ids  &lt;- wm_q[[1]]               # save neighbour indices for clarity\nnb1_gdppc &lt;- hunan$GDPPC[nb1_ids]   # vector of GDPPC values for those neighbours\nnb1_gdppc                           # print to compare with slide values\n\n[1] 20981 34592 24473 21311 22879\n\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\n\nWe can display the complete weight matrix by using str().\n\n# View the full list structure (long print, but matches slide)\nstr(wm_q)   # shows, for each region, the integer IDs of its neighbours\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"snap\")= num 9e-08\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nBe warned: The output might cut across several pages. Save the trees if we are going to print out the report.\n\n\n\n\n# Build neighbours list where polygons are neighbours only if they share an EDGE (no corner-only)\nwm_r &lt;- spdep::poly2nb(hunan, queen = FALSE)  # rook definition\nsummary(wm_r)                                 # compare to queen: usually slightly fewer links\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\n# Compute polygon centroids for plotting the graph over the map\n# We extract numeric (lon, lat) from the POINT geometry returned by st_centroid()\nlongitude &lt;- purrr::map_dbl(hunan$geometry, ~ sf::st_centroid(.x)[[1]])  # [[1]] = x (lon)\nlatitude  &lt;- purrr::map_dbl(hunan$geometry, ~ sf::st_centroid(.x)[[2]])  # [[2]] = y (lat)\ncoords    &lt;- cbind(longitude, latitude)                                  # combine into a two-column matrix as required by spdep\nhead(coords)                                                             # quick check\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n# Plot: Queen contiguity graph\nplot(hunan$geometry, border = \"lightgrey\")                       # draw county outlines\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\") # overlay neighbour links + nodes in red\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot: Rook contiguity graph\nplot(hunan$geometry, border = \"lightgrey\")                       # reset blank map\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\") # overlay rook links\n\n\n\n\n\n\n\n\n\n\n\n\n# Side-by-side comparison (Queen vs Rook)\npar(mfrow = c(1, 2))    # 1 row, 2 plots\nplot(hunan$geometry, border = \"lightgrey\", main = \"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\nplot(hunan$geometry, border = \"lightgrey\", main = \"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))    # reset layout\n\n\n\n\n\n\nIn this section, we will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\n\n# 1) Compute first-nearest neighbours (1-NN) from the centroid coordinates\nk1 &lt;- spdep::knn2nb(spdep::knearneigh(coords))  # returns an nb object where each region has 1 NN\n\nWarning in spdep::knn2nb(spdep::knearneigh(coords)): neighbour object has 25\nsub-graphs\n\n\n\n# 2) Get the distances of those NN relationships (in km if longlat = TRUE)\nk1dists &lt;- unlist(spdep::nbdists(k1, coords, longlat = TRUE)) # lengths of the 88 NN edges as a numeric vector\n\n\n# 3) Summarise to find a safe upper bound for a distance band\nsummary(k1dists)   # Max ≈ 61.79 km in the slide → choose 62 km as cutoff\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\n\n\n# Build neighbours where ANY pair of centroids within 62 km are linked\nwm_d62 &lt;- spdep::dnearneigh(coords, d1 = 0, d2 = 62, longlat = TRUE)  # 0 lower bound; 62 km upper bound from previous step\nwm_d62                                                                # print nb object summary like in slide\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNoteQuiz:\n\n\n\nWhat does “Average number of links: 3.681818” mean?\nOn average, each region has ~3.682 neighbours under the 62 km rule. Numerically, it equals total_links / number_of_regions = 324 / 88 ≈ 3.681818.\n\n\n\n# Show the internal list structure (optional)\nstr(wm_d62)   # each element lists neighbour indices for that region\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n# Cross-tab: how many regions have 1, 2, 3, ... neighbours under 62 km?\ntable(hunan$County, spdep::card(wm_d62))   # card() = neighbour count (degree) for each region\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n# Connected components check: is the graph fully connected?\nn_comp &lt;- spdep::n.comp.nb(wm_d62)  # counts connected subgraphs\nn_comp$nc                           # should be 1 = fully connected (as in slide)\n\n[1] 1\n\ntable(n_comp$comp.id)               # how many nodes in each component (expect 88 in id=1)\n\n\n 1 \n88 \n\n\n\n\nNow, we will plot the distance weight matrix by using the code chunk below.\n\n# Plot fixed-distance links (black) and 1-NN links (red) on top of the map\nplot(hunan$geometry, border = \"lightgrey\")                 # outline map\nplot(wm_d62, coords, add = TRUE)                           # distance-band neighbours (default black)\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)   # 1-NN links in red with short arrowheads\n\n\n\n\n\n\n\n\n\n# Side-by-side: 1-NN vs distance band\npar(mfrow = c(1, 2))\nplot(hunan$geometry, border = \"lightgrey\", main = \"1st nearest neighbours\")\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)\nplot(hunan$geometry, border = \"lightgrey\", main = \"Distance link (&lt;=62 km)\")\nplot(wm_d62, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n# Force exactly k neighbours per region (here k = 6 per the slide)\nknn6 &lt;- spdep::knn2nb(spdep::knearneigh(coords, k = 6))    # asymmetric possible (A near B not equal B near A)\nknn6                                                       # prints summary: \"Average number of links: 6\"\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nWe can plot the weight matrix using the code chunk below.\n\n# Visualise the kNN(6) graph\nplot(hunan$geometry, border = \"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we will learn how to derive a spatial weight matrix based on Inversed Distance method.\n\n# Compute distances ONLY for pairs that are QUEEN neighbours (wm_q) to avoid all-pairs blow-up\ndist_q &lt;- spdep::nbdists(wm_q, coords, longlat = TRUE)  # a list: element i contains distances to i's neighbours\nids    &lt;- lapply(dist_q, function(x) 1 / (x))           # convert distances to inverse-distance weights\nids    # print few entries to match slide’s example\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n# Build a row-standardised (style=\"W\") weights list from Queen neighbours\n# Each neighbour gets equal weight so that each row sums to 1 (average of neighbours).\nrswm_q &lt;- spdep::nb2listw(wm_q, style = \"W\", zero.policy = TRUE)   # zero.policy=TRUE avoids errors if any lonely nodes\nrswm_q    # shows \"Weights style: W\" + summary\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n# Inspect weights for a specific polygon (example in slide used index 10)\nrswm_q$weights[[10]]   # e.g., 8 neighbours → each weight = 1/8 = 0.125\n\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n# Row-standardised distance weights using the inverse-distance 'glist' (Professor used style=\"B\" list)\nrswm_ids &lt;- spdep::nb2listw(wm_q, glist = ids, style = \"B\", zero.policy = TRUE)  # weights stored as provided (not re-scaled to 1)\nrswm_ids    # print characteristics; style B = basic weights\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n# Summarise the distribution of all (unstandardised) IDW weights\nsummary(unlist(rswm_ids$weights))   # min/median/mean/... like the slide\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\nIn this section, we will learn how to create four different spatial lagged variables, they are: - spatial lag with row-standardized weights, - spatial lag as a sum of neighbouring values, - spatial window average, and - spatial window sum.\n\n\n\n# Compute spatial lag of GDPPC using row-standardised W (this is the neighbours' AVERAGE GDPPC)\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag    # matches slide numbers order\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below:\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nNoteQuestion:\n\n\n\nCan you see the meaning of Spatial lag with row-standardized weights now?\n\nWith row-standardised W, each neighbour gets weight 1 / (#neighbours).\n\nlag GDPPC is therefore the average GDPPC of each county’s neighbours, not its own GDPPC.\n\nHigh values in a county surrounded by high-GDPPC neighbours turn the lag map dark even if the county’s own GDPPC is modest, and vice versa.\n\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below:\n\n# Append lag column back to sf for mapping and tabulation\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below:\n\n# Map: GDPPC vs lag(GDPPC) side-by-side for visual comparison\ngdppc_map   &lt;- tmap::qtm(hunan, \"GDPPC\")       # original values\nlag_gdppc_m &lt;- tmap::qtm(hunan, \"lag GDPPC\")   # neighbour average\ntmap::tmap_arrange(gdppc_map, lag_gdppc_m, asp = 1, ncol = 2) # compare patterns\n\n\n\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n# Build a \"binary\" weights list (each neighbour = 1), still using the Queen neighbours\nb_weights  &lt;- lapply(wm_q, function(x) 0 * x + 1)                    # for each neighbour ID vector, make a same-length vector of 1s\nb_weights2 &lt;- spdep::nb2listw(wm_q, glist = b_weights, style = \"B\")  # store those 1s as raw weights (no row standardisation)\nb_weights2                                                           # check characteristics (Weights style: B)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\n\n# Compute the spatial lag as a SUM of neighbour GDPPC (not an average)\nlag_sum &lt;- list(hunan$NAME_3, spdep::lag.listw(b_weights2, hunan$GDPPC))  # returns the summed values, paired with names\nlag.res &lt;- as.data.frame(lag_sum)                                         # convert list → data.frame\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")                         # name columns as in slide\n\nLet’s investigate the result by using the code chunck below:\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\n\n\n\n\n\n\nNoteQuestion:\n\n\n\nCan you understand the meaning of Spatial lag as a sum of neighboring values now?\nlag_sum GDPPC is the total GDPPC of neighbours, so counties with many neighbours (or neighbours with large GDPPC) stand out more than in the averaged lag.\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- dplyr::left_join(hunan, lag.res, by = \"NAME_3\")   \n\nNow, we can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- spdep::include.self(wm_q)    # neighbour list with self included\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below:\n\nwm_qs[[1]]  # first county now lists itself plus prior neighbours\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\n# Convert to weights (row-standardised) so we compute an average including self\nwm_qs_w &lt;- spdep::nb2listw(wm_qs)                                               # default style = \"W\" (row-standardised) on the expanded list\nwm_qs_w   \n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n# Compute window-average lag of GDPPC (includes county itself in the mean)\nlag_w_avg_gdppc &lt;- spdep::lag.listw(wm_qs_w, hunan$GDPPC)   # average of (self + neighbours)\nlag_w_avg_gdppc     \n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data frame by using as.data.frame().\n\n# Convert and join back for tables/maps\nlag.list.wm_qs   &lt;- list(hunan$NAME_3, lag_w_avg_gdppc)        # pair names with values\nlag_wm_qs.res    &lt;- as.data.frame(lag.list.wm_qs)              # to data.frame\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\") # final column names\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- dplyr::left_join(hunan, lag_wm_qs.res, by = \"NAME_3\")  # attach to sf\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below:\n\n# Quick table comparing lag (neighbour average) vs window average (self + neighbours average)\nhunan %&gt;%\n  dplyr::select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;%\n  knitr::kable()  \n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n# Map comparison: lag (avg of neighbours) vs window avg (self+neighbours)\nw_avg_gdppc &lt;- tmap::qtm(hunan, \"lag_window_avg GDPPC\")\ntmap::tmap_arrange(lag_gdppc_m, w_avg_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWindow average dampens extremes because each county’s own value is blended with its neighbours’. Border counties with few neighbours change more when we include “self”.\n\n\nNote: For more effective comparison, it is advisable to use the core tmap mapping functions.\n\n\n\n\n# Start from neighbour list that includes self\nwm_qs &lt;- spdep::include.self(wm_q)                                          # ensure self is included\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\n\n# Create binary weights for the self-included list (all ones)\nb_weights_qs  &lt;- lapply(wm_qs, function(x) 0 * x + 1)                       # vector of 1s for each neighbour including self\nb_weights2_qs &lt;- spdep::nb2listw(wm_qs, glist = b_weights_qs, style = \"B\")  # store as raw (binary) weights\nb_weights2_qs                                                               # confirm characteristics\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\n\n# Compute window SUM lag (self + neighbours summed)\nw_sum_gdppc &lt;- list(hunan$NAME_3, spdep::lag.listw(b_weights2_qs, hunan$GDPPC))  # list of names + sums\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)                                    # to data.frame\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")                          # rename to match slide\n\n\nhunan &lt;- dplyr::left_join(hunan, w_sum_gdppc.res, by = \"NAME_3\")                 # attach to sf\n\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#overview",
    "href": "Hands-on_Ex04/hand-on_ex04.html#overview",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute spatial weights using R.\nBy the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\n\nimport csv file using appropriate function of readr package,\n\nperform relational join using appropriate join function of dplyr package,\n\ncompute spatial weights using appropriate functions of spdep package, and calculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#the-data",
    "href": "Hands-on_Ex04/hand-on_ex04.html#the-data",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise, they are: - Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. - Hunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n# \"pacman\" automatically installs and loads packages if not already installed.\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex04/hand-on_ex04.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "# Read the Hunan county polygons as an sf object (modern spatial class)\nhunan &lt;- sf::st_read(dsn = \"data/geospatial\", layer = \"Hunan\")  # dsn = folder, layer = shapefile basename\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n# Inspect the basic structure to confirm geometry + fields\nprint(hunan)    # shows feature count, geometry type, CRS, attributes\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3 Shape_Leng Shape_Area    County\n1   Changde 21098   Anxiang      County   1.869074 0.10056190   Anxiang\n2   Changde 21100   Hanshou      County   2.360691 0.19978745   Hanshou\n3   Changde 21101    Jinshi County City   1.425620 0.05302413    Jinshi\n4   Changde 21102        Li      County   3.474325 0.18908121        Li\n5   Changde 21103     Linli      County   2.289506 0.11450357     Linli\n6   Changde 21104    Shimen      County   4.171918 0.37194707    Shimen\n7  Changsha 21109   Liuyang County City   4.060579 0.46016789   Liuyang\n8  Changsha 21110 Ningxiang      County   3.323754 0.26614198 Ningxiang\n9  Changsha 21111 Wangcheng      County   2.292093 0.13049161 Wangcheng\n10 Chenzhou 21112     Anren      County   2.240739 0.13343936     Anren\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n# Read county-level attributes (e.g., GDPPC) for 2012\nhunan2012 &lt;- readr::read_csv(\"data/aspatial/Hunan_2012.csv\")   # creates a regular tibble/data.frame\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Peek at columns so we know what to join\ndplyr::glimpse(hunan2012)   # confirms column names such as County, GDPPC, etc.\n\nRows: 88\nColumns: 29\n$ County      &lt;chr&gt; \"Anhua\", \"Anren\", \"Anxiang\", \"Baojing\", \"Chaling\", \"Changn…\n$ City        &lt;chr&gt; \"Yiyang\", \"Chenzhou\", \"Changde\", \"Hunan West\", \"Zhuzhou\", …\n$ avg_wage    &lt;dbl&gt; 30544, 28058, 31935, 30843, 31251, 28518, 54540, 28597, 33…\n$ deposite    &lt;dbl&gt; 10967.0, 4598.9, 5517.2, 2250.0, 8241.4, 10860.0, 24332.0,…\n$ FAI         &lt;dbl&gt; 6831.7, 6386.1, 3541.0, 1005.4, 6508.4, 7920.0, 33624.0, 1…\n$ Gov_Rev     &lt;dbl&gt; 456.72, 220.57, 243.64, 192.59, 620.19, 769.86, 5350.00, 1…\n$ Gov_Exp     &lt;dbl&gt; 2703.0, 1454.7, 1779.5, 1379.1, 1947.0, 2631.6, 7885.5, 11…\n$ GDP         &lt;dbl&gt; 13225.0, 4941.2, 12482.0, 4087.9, 11585.0, 19886.0, 88009.…\n$ GDPPC       &lt;dbl&gt; 14567, 12761, 23667, 14563, 20078, 24418, 88656, 10132, 17…\n$ GIO         &lt;dbl&gt; 9276.90, 4189.20, 5108.90, 3623.50, 9157.70, 37392.00, 513…\n$ Loan        &lt;dbl&gt; 3954.90, 2555.30, 2806.90, 1253.70, 4287.40, 4242.80, 4053…\n$ NIPCR       &lt;dbl&gt; 3528.3, 3271.8, 7693.7, 4191.3, 3887.7, 9528.0, 17070.0, 3…\n$ Bed         &lt;dbl&gt; 2718, 970, 1931, 927, 1449, 3605, 3310, 582, 2170, 2179, 1…\n$ Emp         &lt;dbl&gt; 494.310, 290.820, 336.390, 195.170, 330.290, 548.610, 670.…\n$ EmpR        &lt;dbl&gt; 441.4, 255.4, 270.5, 145.6, 299.0, 415.1, 452.0, 127.6, 21…\n$ EmpRT       &lt;dbl&gt; 338.0, 99.4, 205.9, 116.4, 154.0, 273.7, 219.4, 94.4, 174.…\n$ Pri_Stu     &lt;dbl&gt; 54.175, 33.171, 19.584, 19.249, 33.906, 81.831, 59.151, 18…\n$ Sec_Stu     &lt;dbl&gt; 32.830, 17.505, 17.819, 11.831, 20.548, 44.485, 39.685, 7.…\n$ Household   &lt;dbl&gt; 290.4, 104.6, 148.1, 73.2, 148.7, 211.2, 300.3, 76.1, 139.…\n$ Household_R &lt;dbl&gt; 234.5, 121.9, 135.4, 69.9, 139.4, 211.7, 248.4, 59.6, 110.…\n$ NOIP        &lt;dbl&gt; 101, 34, 53, 18, 106, 115, 214, 17, 55, 70, 44, 84, 74, 17…\n$ Pop_R       &lt;dbl&gt; 670.3, 243.2, 346.0, 184.1, 301.6, 448.2, 475.1, 189.6, 31…\n$ RSCG        &lt;dbl&gt; 5760.60, 2386.40, 3957.90, 768.04, 4009.50, 5220.40, 22604…\n$ Pop_T       &lt;dbl&gt; 910.8, 388.7, 528.3, 281.3, 578.4, 816.3, 998.6, 256.7, 45…\n$ Agri        &lt;dbl&gt; 4942.253, 2357.764, 4524.410, 1118.561, 3793.550, 6430.782…\n$ Service     &lt;dbl&gt; 5414.5, 3814.1, 14100.0, 541.8, 5444.0, 13074.6, 17726.6, …\n$ Disp_Inc    &lt;dbl&gt; 12373, 16072, 16610, 13455, 20461, 20868, 183252, 12379, 1…\n$ RORP        &lt;dbl&gt; 0.7359464, 0.6256753, 0.6549309, 0.6544614, 0.5214385, 0.5…\n$ ROREmp      &lt;dbl&gt; 0.8929619, 0.8782065, 0.8041262, 0.7460163, 0.9052651, 0.7…\n\n\n\n\n\n\n# Left join: keep all polygons and bring matching columns from hunan2012 by common key(s)\nhunan &lt;- dplyr::left_join(hunan, hunan2012) %&gt;%   # merge attributes into polygons\n  dplyr::select(1:4, 7, 15)      # reproduce Prof’s column subset (indices from slides)\n\nJoining with `by = join_by(County)`\n\n# Inspect and verify that GDPPC is now present alongside geometry\nhead(hunan)   # should list fields incl. County, GDPPC, geometry\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex04/hand-on_ex04.html#visualising-regional-development-indicator",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "# Prepare a simple basemap with names (quick tmap)\nbasemap &lt;- tmap::tm_shape(hunan) +                        # tell tmap what to draw (the polygons)\n  tmap::tm_polygons() +                                   # draw filled polygons\n  tmap::tm_text(\"NAME_3\", size = 0.5)                     # label polygons with county names (field matches slides)\n\n# Choropleth of GDPPC \ngdppc_map &lt;- tmap::qtm(hunan, \"GDPPC\") +                  # quick thematic map of GDPPC\n  tmap::tm_layout(legend.position = c(\"left\", \"bottom\"))  # put legend bottom-left to match the slides\n\n# Show basemap and GDPPC side-by-side\ntmap::tmap_arrange(basemap, gdppc_map, asp = 1, ncol = 2) # arrange 2 maps in one row, square aspect"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#contiguity-spatial-weights-queen-and-rook",
    "href": "Hands-on_Ex04/hand-on_ex04.html#contiguity-spatial-weights-queen-and-rook",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this section, we will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If we look at the documentation, we will see that we can pass a “queen” argument that takes TRUE or FALSE as options. If we do not specify this argument the default is set to TRUE, that is, if we don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\n\n# Build a neighbours list where polygons touching at edges OR corners are neighbours\nwm_q &lt;- spdep::poly2nb(hunan, queen = TRUE)    # queen = TRUE includes corner touches\nsummary(wm_q)   # prints number of regions, links, and link distribution\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n# Explore neighbours of polygon 1 (IDs are row positions of neighbours)\nwm_q[[1]]   # e.g., returns integer vector like c(2, 3, 4, 57, 85)\n\n[1]  2  3  4 57 85\n\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\n\nWe can retrieve the county name of Polygon ID=1 by using the code chunk below:\n\n# Retrieve county name for polygon ID = 1\nhunan$County[1]   # should print the county name for ID 1\n\n[1] \"Anxiang\"\n\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\n\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n# Retrieve the county names of polygon 1’s neighbours\nhunan$NAME_3[c(2, 3, 4, 57, 85)]    # show human-readable names for the neighbour IDs\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below:\n\n# Pull the neighbours’ GDPPC values for polygon 1\nnb1_ids  &lt;- wm_q[[1]]               # save neighbour indices for clarity\nnb1_gdppc &lt;- hunan$GDPPC[nb1_ids]   # vector of GDPPC values for those neighbours\nnb1_gdppc                           # print to compare with slide values\n\n[1] 20981 34592 24473 21311 22879\n\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\n\nWe can display the complete weight matrix by using str().\n\n# View the full list structure (long print, but matches slide)\nstr(wm_q)   # shows, for each region, the integer IDs of its neighbours\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"snap\")= num 9e-08\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nBe warned: The output might cut across several pages. Save the trees if we are going to print out the report.\n\n\n\n\n# Build neighbours list where polygons are neighbours only if they share an EDGE (no corner-only)\nwm_r &lt;- spdep::poly2nb(hunan, queen = FALSE)  # rook definition\nsummary(wm_r)                                 # compare to queen: usually slightly fewer links\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\n# Compute polygon centroids for plotting the graph over the map\n# We extract numeric (lon, lat) from the POINT geometry returned by st_centroid()\nlongitude &lt;- purrr::map_dbl(hunan$geometry, ~ sf::st_centroid(.x)[[1]])  # [[1]] = x (lon)\nlatitude  &lt;- purrr::map_dbl(hunan$geometry, ~ sf::st_centroid(.x)[[2]])  # [[2]] = y (lat)\ncoords    &lt;- cbind(longitude, latitude)                                  # combine into a two-column matrix as required by spdep\nhead(coords)                                                             # quick check\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n# Plot: Queen contiguity graph\nplot(hunan$geometry, border = \"lightgrey\")                       # draw county outlines\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\") # overlay neighbour links + nodes in red\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot: Rook contiguity graph\nplot(hunan$geometry, border = \"lightgrey\")                       # reset blank map\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\") # overlay rook links\n\n\n\n\n\n\n\n\n\n\n\n\n# Side-by-side comparison (Queen vs Rook)\npar(mfrow = c(1, 2))    # 1 row, 2 plots\nplot(hunan$geometry, border = \"lightgrey\", main = \"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\nplot(hunan$geometry, border = \"lightgrey\", main = \"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))    # reset layout"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#distance-based-neighbours",
    "href": "Hands-on_Ex04/hand-on_ex04.html#distance-based-neighbours",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this section, we will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\n\n# 1) Compute first-nearest neighbours (1-NN) from the centroid coordinates\nk1 &lt;- spdep::knn2nb(spdep::knearneigh(coords))  # returns an nb object where each region has 1 NN\n\nWarning in spdep::knn2nb(spdep::knearneigh(coords)): neighbour object has 25\nsub-graphs\n\n\n\n# 2) Get the distances of those NN relationships (in km if longlat = TRUE)\nk1dists &lt;- unlist(spdep::nbdists(k1, coords, longlat = TRUE)) # lengths of the 88 NN edges as a numeric vector\n\n\n# 3) Summarise to find a safe upper bound for a distance band\nsummary(k1dists)   # Max ≈ 61.79 km in the slide → choose 62 km as cutoff\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\n\n\n# Build neighbours where ANY pair of centroids within 62 km are linked\nwm_d62 &lt;- spdep::dnearneigh(coords, d1 = 0, d2 = 62, longlat = TRUE)  # 0 lower bound; 62 km upper bound from previous step\nwm_d62                                                                # print nb object summary like in slide\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNoteQuiz:\n\n\n\nWhat does “Average number of links: 3.681818” mean?\nOn average, each region has ~3.682 neighbours under the 62 km rule. Numerically, it equals total_links / number_of_regions = 324 / 88 ≈ 3.681818.\n\n\n\n# Show the internal list structure (optional)\nstr(wm_d62)   # each element lists neighbour indices for that region\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n# Cross-tab: how many regions have 1, 2, 3, ... neighbours under 62 km?\ntable(hunan$County, spdep::card(wm_d62))   # card() = neighbour count (degree) for each region\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n# Connected components check: is the graph fully connected?\nn_comp &lt;- spdep::n.comp.nb(wm_d62)  # counts connected subgraphs\nn_comp$nc                           # should be 1 = fully connected (as in slide)\n\n[1] 1\n\ntable(n_comp$comp.id)               # how many nodes in each component (expect 88 in id=1)\n\n\n 1 \n88 \n\n\n\n\nNow, we will plot the distance weight matrix by using the code chunk below.\n\n# Plot fixed-distance links (black) and 1-NN links (red) on top of the map\nplot(hunan$geometry, border = \"lightgrey\")                 # outline map\nplot(wm_d62, coords, add = TRUE)                           # distance-band neighbours (default black)\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)   # 1-NN links in red with short arrowheads\n\n\n\n\n\n\n\n\n\n# Side-by-side: 1-NN vs distance band\npar(mfrow = c(1, 2))\nplot(hunan$geometry, border = \"lightgrey\", main = \"1st nearest neighbours\")\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)\nplot(hunan$geometry, border = \"lightgrey\", main = \"Distance link (&lt;=62 km)\")\nplot(wm_d62, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n# Force exactly k neighbours per region (here k = 6 per the slide)\nknn6 &lt;- spdep::knn2nb(spdep::knearneigh(coords, k = 6))    # asymmetric possible (A near B not equal B near A)\nknn6                                                       # prints summary: \"Average number of links: 6\"\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language spdep::knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : num 1\n  ..$ comp.id: num [1:88] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nWe can plot the weight matrix using the code chunk below.\n\n# Visualise the kNN(6) graph\nplot(hunan$geometry, border = \"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#weights-based-on-inverse-distance-idw-on-queen-neighbours",
    "href": "Hands-on_Ex04/hand-on_ex04.html#weights-based-on-inverse-distance-idw-on-queen-neighbours",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this section, we will learn how to derive a spatial weight matrix based on Inversed Distance method.\n\n# Compute distances ONLY for pairs that are QUEEN neighbours (wm_q) to avoid all-pairs blow-up\ndist_q &lt;- spdep::nbdists(wm_q, coords, longlat = TRUE)  # a list: element i contains distances to i's neighbours\nids    &lt;- lapply(dist_q, function(x) 1 / (x))           # convert distances to inverse-distance weights\nids    # print few entries to match slide’s example\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex04/hand-on_ex04.html#row-standardised-weights-matrix",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n# Build a row-standardised (style=\"W\") weights list from Queen neighbours\n# Each neighbour gets equal weight so that each row sums to 1 (average of neighbours).\nrswm_q &lt;- spdep::nb2listw(wm_q, style = \"W\", zero.policy = TRUE)   # zero.policy=TRUE avoids errors if any lonely nodes\nrswm_q    # shows \"Weights style: W\" + summary\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n# Inspect weights for a specific polygon (example in slide used index 10)\nrswm_q$weights[[10]]   # e.g., 8 neighbours → each weight = 1/8 = 0.125\n\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n# Row-standardised distance weights using the inverse-distance 'glist' (Professor used style=\"B\" list)\nrswm_ids &lt;- spdep::nb2listw(wm_q, glist = ids, style = \"B\", zero.policy = TRUE)  # weights stored as provided (not re-scaled to 1)\nrswm_ids    # print characteristics; style B = basic weights\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n# Summarise the distribution of all (unstandardised) IDW weights\nsummary(unlist(rswm_ids$weights))   # min/median/mean/... like the slide\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex04/hand-on_ex04.html#applications-of-the-spatial-weight-matrix",
    "href": "Hands-on_Ex04/hand-on_ex04.html#applications-of-the-spatial-weight-matrix",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this section, we will learn how to create four different spatial lagged variables, they are: - spatial lag with row-standardized weights, - spatial lag as a sum of neighbouring values, - spatial window average, and - spatial window sum.\n\n\n\n# Compute spatial lag of GDPPC using row-standardised W (this is the neighbours' AVERAGE GDPPC)\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag    # matches slide numbers order\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below:\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nNoteQuestion:\n\n\n\nCan you see the meaning of Spatial lag with row-standardized weights now?\n\nWith row-standardised W, each neighbour gets weight 1 / (#neighbours).\n\nlag GDPPC is therefore the average GDPPC of each county’s neighbours, not its own GDPPC.\n\nHigh values in a county surrounded by high-GDPPC neighbours turn the lag map dark even if the county’s own GDPPC is modest, and vice versa.\n\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below:\n\n# Append lag column back to sf for mapping and tabulation\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below:\n\n# Map: GDPPC vs lag(GDPPC) side-by-side for visual comparison\ngdppc_map   &lt;- tmap::qtm(hunan, \"GDPPC\")       # original values\nlag_gdppc_m &lt;- tmap::qtm(hunan, \"lag GDPPC\")   # neighbour average\ntmap::tmap_arrange(gdppc_map, lag_gdppc_m, asp = 1, ncol = 2) # compare patterns\n\n\n\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n# Build a \"binary\" weights list (each neighbour = 1), still using the Queen neighbours\nb_weights  &lt;- lapply(wm_q, function(x) 0 * x + 1)                    # for each neighbour ID vector, make a same-length vector of 1s\nb_weights2 &lt;- spdep::nb2listw(wm_q, glist = b_weights, style = \"B\")  # store those 1s as raw weights (no row standardisation)\nb_weights2                                                           # check characteristics (Weights style: B)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\n\n# Compute the spatial lag as a SUM of neighbour GDPPC (not an average)\nlag_sum &lt;- list(hunan$NAME_3, spdep::lag.listw(b_weights2, hunan$GDPPC))  # returns the summed values, paired with names\nlag.res &lt;- as.data.frame(lag_sum)                                         # convert list → data.frame\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")                         # name columns as in slide\n\nLet’s investigate the result by using the code chunck below:\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\n\n\n\n\n\n\nNoteQuestion:\n\n\n\nCan you understand the meaning of Spatial lag as a sum of neighboring values now?\nlag_sum GDPPC is the total GDPPC of neighbours, so counties with many neighbours (or neighbours with large GDPPC) stand out more than in the averaged lag.\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- dplyr::left_join(hunan, lag.res, by = \"NAME_3\")   \n\nNow, we can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- spdep::include.self(wm_q)    # neighbour list with self included\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below:\n\nwm_qs[[1]]  # first county now lists itself plus prior neighbours\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\n# Convert to weights (row-standardised) so we compute an average including self\nwm_qs_w &lt;- spdep::nb2listw(wm_qs)                                               # default style = \"W\" (row-standardised) on the expanded list\nwm_qs_w   \n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n# Compute window-average lag of GDPPC (includes county itself in the mean)\nlag_w_avg_gdppc &lt;- spdep::lag.listw(wm_qs_w, hunan$GDPPC)   # average of (self + neighbours)\nlag_w_avg_gdppc     \n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data frame by using as.data.frame().\n\n# Convert and join back for tables/maps\nlag.list.wm_qs   &lt;- list(hunan$NAME_3, lag_w_avg_gdppc)        # pair names with values\nlag_wm_qs.res    &lt;- as.data.frame(lag.list.wm_qs)              # to data.frame\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\") # final column names\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- dplyr::left_join(hunan, lag_wm_qs.res, by = \"NAME_3\")  # attach to sf\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below:\n\n# Quick table comparing lag (neighbour average) vs window average (self + neighbours average)\nhunan %&gt;%\n  dplyr::select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;%\n  knitr::kable()  \n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n# Map comparison: lag (avg of neighbours) vs window avg (self+neighbours)\nw_avg_gdppc &lt;- tmap::qtm(hunan, \"lag_window_avg GDPPC\")\ntmap::tmap_arrange(lag_gdppc_m, w_avg_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWindow average dampens extremes because each county’s own value is blended with its neighbours’. Border counties with few neighbours change more when we include “self”.\n\n\nNote: For more effective comparison, it is advisable to use the core tmap mapping functions.\n\n\n\n\n# Start from neighbour list that includes self\nwm_qs &lt;- spdep::include.self(wm_q)                                          # ensure self is included\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\n\n# Create binary weights for the self-included list (all ones)\nb_weights_qs  &lt;- lapply(wm_qs, function(x) 0 * x + 1)                       # vector of 1s for each neighbour including self\nb_weights2_qs &lt;- spdep::nb2listw(wm_qs, glist = b_weights_qs, style = \"B\")  # store as raw (binary) weights\nb_weights2_qs                                                               # confirm characteristics\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\n\n# Compute window SUM lag (self + neighbours summed)\nw_sum_gdppc &lt;- list(hunan$NAME_3, spdep::lag.listw(b_weights2_qs, hunan$GDPPC))  # list of names + sums\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)                                    # to data.frame\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")                          # rename to match slide\n\n\nhunan &lt;- dplyr::left_join(hunan, w_sum_gdppc.res, by = \"NAME_3\")                 # attach to sf\n\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html",
    "href": "In-Class_Ex04/in-class_ex04.html",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "",
    "text": "Using the step we leanred from previous hands-in, install and load the necessary R packages in R environment.\n\npacman::p_load(sf, ggstatsplot, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#loading-the-package",
    "href": "In-Class_Ex04/in-class_ex04.html#loading-the-package",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "",
    "text": "Using the step we leanred from previous hands-in, install and load the necessary R packages in R environment.\n\npacman::p_load(sf, ggstatsplot, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#preparing-the-data",
    "href": "In-Class_Ex04/in-class_ex04.html#preparing-the-data",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nUsing the steps we learned from previous hands-on, complete the following tasks:\n\nimport Hunan shapefile and parse it into a sf polygon feature object.\n\nimport Hunan_2012.csv file parse it into a tibble data.frame.\n\njoin Hunan and Hunan_2012 data.frames.\n\n\nImporting Hunan shapefile\n\nhunan_sf &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting Hunan 2012 table\n\n# hunan2012 &lt;- read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/aspatial/Hunan_2012.csv\")\n\nhunan2012 &lt;- read.csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex04/data/aspatial/Hunan_2012.csv\",\n                      stringsAsFactors = FALSE, check.names = FALSE)\n\n\n\nJoining Hunan and Hunan_2012\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#mapping-gdppc",
    "href": "In-Class_Ex04/in-class_ex04.html#mapping-gdppc",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Mapping GDPPC",
    "text": "Mapping GDPPC\nUsing the steps we learned from Hands-on Exercise 5, prepare a choropleth map showing the geographic distribution of GDPPC of Hunan Province.\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#converting-to-spatialpolygondataframe",
    "href": "In-Class_Ex04/in-class_ex04.html#converting-to-spatialpolygondataframe",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Converting to SpatialPolygonDataFrame",
    "text": "Converting to SpatialPolygonDataFrame\nNote: GWmodel presently is built around the older sp and not sf formats for handling spatial data in R.\n\nhunan_sp &lt;- hunan_sf %&gt;%\n  as_Spatial()"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "href": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\n\nDetermine adaptive bandwidth\n\nCross-validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\nbw_CV\n\n[1] 22\n\n\n\n\nAIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\nbw_AIC\n\n[1] 22\n\n\n\n\n\nComputing geographically wieghted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\n\n\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)\n\n\n\nVisualising geographically weighted summary statistic\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-summary-statistics-with-fixed",
    "href": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-summary-statistics-with-fixed",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\n\nDetermine fixed bandwodth\n\nCross-validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\nbw_CV \n\n[1] 76.29126\n\n\n\n\nAIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\nbw_AIC\n\n[1] 160.5517\n\n\n\n\n\nComputing adaptive bandwidth\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)\n\n\n\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)\n\n\n\nVisualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "href": "In-Class_Ex04/in-class_ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel method",
    "section": "Geographically Weighted Correlation with Adaptive Bandwidth",
    "text": "Geographically Weighted Correlation with Adaptive Bandwidth\nBusiness question: Is there any relationship between GDP per capita and Gross Industry Output?\n\nConventional statistical solution\n\nggscatterstats(\n  data = hunan2012, \n  x = Agri, \n  y = GDPPC,\n  xlab = \"Gross Agriculture Output\", ## label for the x-axis\n  ylab = \"GDP per capita\", \n  label.var = County, \n  label.expression = Agri &gt; 10000 & GDPPC &gt; 50000, \n  point.label.args = list(alpha = 0.7, size = 4, color = \"grey50\"),\n  xfill = \"#CC79A7\", \n  yfill = \"#009E73\", \n  title = \"Relationship between GDP PC and Gross Agriculture Output\")\n\n\n\n\n\n\n\n\n\n\nGeospatial analytics solution\n\nDetermine the bandwidth\n\nbw &lt;- bw.gwr(GDPPC ~ GIO, \n             data = hunan_sp, \n             approach = \"AICc\", \n             adaptive = TRUE)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1870.235 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1870.852 \nAdaptive bandwidth (number of nearest neighbours): 72 AICc value: 1869.744 \nAdaptive bandwidth (number of nearest neighbours): 78 AICc value: 1869.713 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 1869.604 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: 1869.537 \nAdaptive bandwidth (number of nearest neighbours): 86 AICc value: 1869.647 \nAdaptive bandwidth (number of nearest neighbours): 83 AICc value: 1869.567 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: 1869.537 \n\n\n\n\nComputing gwCorrelation\n\ngwstats &lt;- gwss(hunan_sp, \n                vars = c(\"GDPPC\", \"GIO\"), \n                bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, \n                longlat = T)\n\n\n\nExtracting the result\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstats$SDF) %&gt;%\n  select(c(12,13)) %&gt;%\n  rename(gwCorr = Corr_GDPPC.GIO,\n         gwSpearman = Spearman_rho_GDPPC.GIO)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_Corr &lt;- cbind(hunan_sf, gwstat_df)\n\n\n\nVisualising Local Correlation\n\ntm_shape(hunan_Corr) +\n  tm_polygons(fill = \"gwSpearman\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"local Spearman Rho\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Local Spearman Rho\", \n  size = 2.0) +\n  tm_layout(frame = TRUE)\n\n\n\n\n\n\n\n\nReferences\n\nBrunsdon, C. et. al. (2002) “Geographically weighted summary statistics - a framework for localised exploratory data analysis”, Computer, Environment and Urban Systems, Vol 26, pp. 501-525. Available as e-journal, SMU library.\nHarris, P. & Brunsdon, C. (2010) “Exploring spatial variation and spatial relationships in freshwater acidification critical load data set for Great Britain using geographically weighted summary statistics”, Computers & Geosciences, Vol. 36, pp. 54-70. Available as e-journal, SMU library."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "The specific questions we would like to answer are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nIf the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?\n\n\n\n\n\n# Install 'pacman' once if you do not have it (uncomment next line and run only once)\n# install.packages(\"pacman\")\n\n# Use pacman::p_load() to install (if needed) and then load the listed packages automatically\npacman::p_load(\n  sf,            # modern vector geospatial data handling (Simple Features)\n  terra,         # modern raster + vector geospatial data handling\n  spatstat,      # meta‑package: tools for spatial point pattern analysis (SPPA)\n  tmap,          # cartographic and interactive mapping\n  rvest,         # web‑scraping helper used here to parse HTML inside KML attributes\n  tidyverse      # data wrangling helpers (dplyr, purrr, stringr, readr, etc.)\n)\n\n\n\n\nImport the Master Plan 2019 Subzone (No Sea) polygons as sf and set projection to EPSG:3414 (SVY21 / Singapore TM)\n\n# Read the subzone boundary KML into an sf object named 'mpsz_sf'\nmpsz_sf &lt;- sf::st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\") %&gt;%\n  sf::st_zm(drop = TRUE, what = \"ZM\") %&gt;%  # remove Z (elevation) and M (measure) dimensions to keep 2D only\n  sf::st_transform(crs = 3414)   \n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex02/Data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nWhy remove Z/M? Our analysis is purely 2D on a planar map. Extra dimensions are unnecessary and can break some operations.\nWhy EPSG:3414? This is the standard projected system for Singapore. Distances/areas become meaningful (meters) instead of degrees.\n\nExtract REGION_N, PLN_AREA_N, SUBZONE_N, SUBZONE_C from the Description field (HTML inside KML) (as in the guidance helper function)\n\n# Define a small helper function to pull one named item from the HTML table stored in the 'Description' field\nextract_kml_field &lt;- function(html_text, field_name) {\n  if (is.na(html_text) || html_text == \"\") return(NA_character_) # if Description is empty, return NA immediately\n\n  page &lt;- rvest::read_html(html_text)       # parse the HTML string\n  rows &lt;- rvest::html_elements(page, \"tr\")  # get all table rows (&lt;tr&gt;)\n\n  # Find the row whose header cell (&lt;th&gt;) equals the field_name, then take the value in its &lt;td&gt;\n  value &lt;- rows %&gt;%\n    purrr::keep(~ rvest::html_text2(rvest::html_element(.x, \"th\")) == field_name) %&gt;%\n    rvest::html_element(\"td\") %&gt;%\n    rvest::html_text2()\n\n  if (length(value) == 0) NA_character_ else value    # if not found, return NA\n}\n\n\n# Apply the function to build clean attributes from 'Description'\nmpsz_sf &lt;- mpsz_sf %&gt;%\n  dplyr::mutate(\n    REGION_N   = purrr::map_chr(Description, extract_kml_field, \"REGION_N\"),   # region name\n    PLN_AREA_N = purrr::map_chr(Description, extract_kml_field, \"PLN_AREA_N\"), # planning area name\n    SUBZONE_N  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_N\"),  # subzone name\n    SUBZONE_C  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_C\")   # subzone code\n  ) %&gt;%\n  dplyr::select(-Name, -Description) %&gt;%     # drop original noisy fields\n  dplyr::relocate(geometry, .after = dplyr::last_col())  # move geometry column to the end for readability\n\n\nCheckpoint: You now have clean polygon attributes and the proper CRS.\n\n(Optional) Filter out offshore/irrelevant areas exactly like in the slides\n\n# Create a cleaned version that excludes southern group, western islands, and north‑eastern islands (as per slides)\nmpsz_cl &lt;- mpsz_sf %&gt;%\n  dplyr::filter(\n    SUBZONE_N != \"SOUTHERN GROUP\",\n    PLN_AREA_N != \"WESTERN ISLANDS\",\n    PLN_AREA_N != \"NORTH-EASTERN ISLANDS\"\n  )\n\n# Save a small RDS for repeatability (optional)\n# readr::write_rds(mpsz_cl, \"chap04/data/mpsz_cl.rds\")\n\nImport Child Care Services points as sf, drop Z/M, and project to EPSG:3414\n\n# Read childcare locations KML into an sf object named 'childcare_sf'\nchildcare_sf &lt;- sf::st_read(\"data/geospatial/ChildCareServices.kml\") %&gt;%\n  sf::st_zm(drop = TRUE, what = \"ZM\") %&gt;%     # keep 2D only\n  sf::st_transform(crs = 3414)         \n\nReading layer `CHILDCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex02/Data/geospatial/ChildCareServices.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n# Quick mapping of both layers to verify alignment (childcare points inside planning subzones)\ntmap::tmap_mode(\"plot\")  # ensure static plotting mode\n\nℹ tmap mode set to \"plot\".\n\n# Plot subzones with childcare points overlaid\ntmap::tm_shape(mpsz_cl) +\n  tmap::tm_polygons(col = \"grey85\", border.col = \"black\") +\n  tmap::tm_shape(childcare_sf) +\n  tmap::tm_dots(size = 0.2, col = \"red\")\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n\n\n\n\n\n\n\n\n\n\nWhat to check: All childcare centres (black dots) fall neatly inside Singapore’s subzones (grey polygons). If not, check CRS and transformations.\n\nInteractive mapping with tmap + leaflet\n\n# Switch tmap into interactive mode (internally uses the leaflet library)\ntmap::tmap_mode(\"view\")                # enable pan/zoom and feature popups\n\nℹ tmap mode set to \"view\".\n\n# Draw an interactive point map on top of a web basemap\n# - Click a point to see its attributes\n# - Use the layer control to change basemaps (e.g., ESRI.WorldGrayCanvas, OpenStreetMap)\ntmap::tm_shape(childcare_sf) +         # select the childcare sf layer\n  tmap::tm_dots()                      # render as clickable points\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n# IMPORTANT: After exploring, switch back to static plotting for the rest of the workflow\n# This avoids keeping too many active web-map connections during knitting/deployment\ntmap::tmap_mode(\"plot\")               # restore static plotting mode\n\nℹ tmap mode set to \"plot\".\n\n\n\n# Switch to interactive mode for web-map style exploration\n# tmap::tmap_mode(\"view\")\n# \n# # Create an interactive map where you can zoom and click points\n# tmap::tm_shape(mpsz_cl) +\n#   tmap::tm_polygons(col = \"lightgrey\", border.col = \"white\") +\n#   tmap::tm_shape(childcare_sf) +\n#   tmap::tm_dots(col = \"blue\", size = 0.5)\n# \n# # Always switch back to static plotting when done\n# tmap::tmap_mode(\"plot\")\n\n\nNotes: In interactive mode you can zoom, pan, and click childcare points to view their attributes. Remember to switch back to plot mode before continuing with static maps.\n\n\n\n\n\n\n\n\n# Convert the sf point layer to spatstat's planar point pattern (ppp) object\nchildcare_ppp &lt;- spatstat.geom::as.ppp(childcare_sf)  # now suitable for SPPA functions\n\n\n# Verify the class\nclass(childcare_ppp)  # expect: \"ppp\"\n\n[1] \"ppp\"\n\n\n\n# Peek at summary to understand the point pattern and its window\nsummary(childcare_ppp)  # quick glance (number of points, intensity, window size)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nMark variables: Name, Description\nSummary:\n     Name           Description       \n Length:1925        Length:1925       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n# Convert the cleaned subzones into a single observation window (owin) for clipping/analysis\nsg_owin &lt;- spatstat.geom::as.owin(mpsz_cl)\n\n\n# Verify the class\nclass(sg_owin)  # expect: \"owin\"\n\n[1] \"owin\"\n\n\n\n# Visualize the boundary to confirm it looks right\nplot(sg_owin, main = \"sg_owin — Singapore Observation Window\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Clip the point pattern to the Singapore window to exclude any points outside the boundary\nchildcareSG_ppp &lt;- childcare_ppp[sg_owin]\n\n\n# Confirm the combined object (ppp with polygon window information)\nchildcareSG_ppp  # should report a polygonal window and the point count\n\nMarked planar point pattern: 1925 points\nMark variables: Name, Description \nwindow: polygonal boundary\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n\n\n\n\n\nHypotheses:\n\n\\(H_0\\): Childcare centres are randomly distributed (Complete Spatial Randomness, CSR).\n\\(H_1\\): Childcare centres are not randomly distributed (clustered or regular).\n\nThe 95% confident interval will be used.\nThe Clark–Evans test returns an index R:\n\n\\(R &lt; 1 → clustered\\).\n\\(R = 1 → random\\)\n\\(R &gt; 1 → regular\\)\n\n\n\n\n# Run Clark–Evans using observed pattern only (Z-test)\nce_noCSR &lt;- spatstat.explore::clarkevans.test(\nX = childcareSG_ppp, # ppp object of childcare centres\ncorrection = \"none\", # no edge correction (per slides)\nalternative = \"clustered\" # one-sided: test for clustering (R &lt; 1)\n)\n\n# Show results\nce_noCSR\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNoteClark–Evans Test without CSR (Z-test)\n\n\n\n\nR = 0.53532 (&lt; 1) → indicates clustering.\np-value &lt; 2.2e-16 (&lt; 0.05): → reject H₀.\nStatistical conclusion: Distribution is not random; strong clustering exists.\nBusiness communication: Childcare centres are concentrated in specific neighbourhoods. This reflects demand-driven placement but may leave some areas underserved. Authorities should consider equity in future allocations.\n\n\n\n\n\n\n\n# Monte‑Carlo Clark–Evans: compare observed R to R from CSR simulations (nsim = 99)\nce_MC &lt;- spatstat.explore::clarkevans.test( # compute simulated p‑value under CSR\nX = childcareSG_ppp, # same ppp as above (metres)\ncorrection = \"none\", # keep consistent with slides\nalternative = \"clustered\", # one‑sided for clustering (R &lt; 1)\nmethod = \"MonteCarlo\", # enable Monte‑Carlo simulation mode\nnsim = 99 # number of CSR replicates as per slides\n) # end clarkevans.test call\n\nce_MC\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value = 0.01\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNoteClark–Evans Test with CSR (Monte Carlo, nsim=99)\n\n\n\n\nR = 0.53532 (&lt; 1): → clustering again.\np-value = 0.01 (&lt; 0.05) → reject H₀ at 95% CI.\nStatistical conclusion: Distribution is not random; clustering is statistically significant even under CSR simulations.\nBusiness communication: Confirms earlier finding with stronger robustness. Clustering is not due to chance — it is systematic. Policymakers should expand childcare access in low-density areas to reduce inequality in service availability.\n\n\n\n\n\n\n\nGoal: Turn points into a smooth surface to reveal hotspots. Bandwidth controls smoothness; kernel controls the spread shape.\n\n\n\nkde_SG_diggle &lt;- density(          # Create KDE surface and save to object 'kde_SG_diggle'\n  childcareSG_ppp,                 # Input: childcare centres dataset in 'ppp' format\n  sigma = bw.diggle,               # Bandwidth: automatic smoothing radius (Diggle’s selector)\n  edge  = TRUE,                    # Apply edge correction to fix boundary underestimation\n  kernel = \"gaussian\"              # Kernel type: Gaussian (bell-shaped smoothing function)\n)                                  # End of density() call\n\n\n# Plot the kernel density estimation surface for childcare centres\nplot(kde_SG_diggle)        \n\n\n\n\n\n\n\n\n\nsummary(kde_SG_diggle)\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2667.538, 55941.94] x [21448.47, 50256.33] units\ndimensions of each pixel: 416 x 225.0614 units\nImage is defined on a subset of the rectangular grid\nSubset area = 669941961.12249 square units\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-5.824417e-21, 3.063698e-05]\n    integral = 1927.788\n    mean = 2.877545e-06\n\n\n\n# Calculate optimal bandwidth (smoothing parameter) using Diggle’s method\nbw &lt;- bw.diggle(childcareSG_ppp)             \nbw   # Display the selected bandwidth value\n\n   sigma \n295.9712 \n\n\n\n\n\n\nchildcareSG_ppp_km &lt;- rescale.ppp(  # Rescale the point pattern dataset from metres to kilometres\n  childcareSG_ppp, 1000, \"km\")      # Display the selected bandwidth value \n\n\nkde_childcareSG_km &lt;- density(  # Compute kernel density estimation on rescaled dataset\n  childcareSG_ppp_km,           # Input point pattern in kilometres\n  sigma = bw.diggle,            # Use Diggle’s optimal bandwidth (sigma)\n  edge  = TRUE,                 # Apply edge correction to fix boundary bias\n  kernel = \"gaussian\"           # Use Gaussian kernel for smoothing\n)\n\n\n# Plot KDE surface for childcare centres (units in km)\nplot(kde_childcareSG_km)\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute bandwidth using Cronie & van Lieshout method\nbw.CvL(childcareSG_ppp_km)\n\n   sigma \n4.357209 \n\n\n\n# Compute bandwidth using Scott’s rule-of-thumb method\nbw.scott(childcareSG_ppp_km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\n\n\n# Compute bandwidth using likelihood cross-validation (PPL)\nbw.ppl(childcareSG_ppp_km)\n\n   sigma \n0.378997 \n\n\n\n# Re-compute bandwidth using Diggle’s method (on km scale)\nbw.diggle(childcareSG_ppp_km)\n\n    sigma \n0.2959712 \n\n\n\nkde_childcareSG_ppl &lt;- density(   # KDE using PPL bandwidth for smoothing\n  childcareSG_ppp_km,             # Input dataset in kilometres\n  sigma = bw.ppl,                 # Use bandwidth chosen by PPL\n  edge  = TRUE,                   # Apply edge correction\n  kernel = \"gaussian\"             # Gaussian smoothing kernel\n)\n\npar(mfrow=c(1,2))                            # Set plot area into 1 row, 2 columns for comparison\nplot(kde_childcareSG_km, main = \"bw.diggle\") # Plot KDE using Diggle bandwidth\nplot(kde_childcareSG_ppl, main = \"bw.ppl\")   # Plot KDE using PPL bandwidth\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))                        # Divide plotting window into 2 rows × 2 columns\n\nplot(density(childcareSG_ppp_km,         # KDE with Gaussian kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")                    # Title for Gaussian plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Epanechnikov kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")                # Title for Epanechnikov plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Quartic kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")                     # Title for Quartic plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Disc kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")                        # Title for Disc plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkde_childcareSG_fb &lt;- density(   # KDE using fixed bandwidth method\n  childcareSG_ppp_km,            # Input childcare centres dataset (in km)\n  sigma = 0.6,                   # Fixed bandwidth = 0.6 km (600 metres)\n  edge  = TRUE,                  # Apply edge correction at boundaries\n  kernel = \"gaussian\"            # Gaussian smoothing kernel\n)\n\nplot(kde_childcareSG_fb)         # Plot the fixed bandwidth KDE result\n\n\n\n\n\n\n\n\n\n\n\n\nkde_childcareSG_ab &lt;- adaptive.density(    # KDE using adaptive bandwidth method\n  childcareSG_ppp_km,                      # Input childcare centres dataset (in km)\n  method=\"kernel\"                          # Kernel smoothing method\n)\n\nplot(kde_childcareSG_ab)  # Plot the adaptive bandwidth KDE result\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))         # Set plotting window to 1 row × 2 columns\nplot(kde_childcareSG_fb, main = \"Fixed bandwidth\")     # Show fixed bandwidth KDE\nplot(kde_childcareSG_ab, main = \"Adaptive bandwidth\")  # Show adaptive bandwidth KDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Convert KDE (im class) into SpatRaster object\nkde_childcareSG_bw_terra &lt;- rast(kde_childcareSG_km)\n\n\n# Check the class of the raster object (should be \"SpatRaster\")\nclass(kde_childcareSG_bw_terra)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\n\n# Print raster properties: resolution, extent, units, etc.\nkde_childcareSG_bw_terra\n\nclass       : SpatRaster \nsize        : 128, 128, 1  (nrow, ncol, nlyr)\nresolution  : 0.4162063, 0.2250614  (x, y)\nextent      : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        :         lyr.1 \nmin value   : -4.314789e-15 \nmax value   :  3.063698e+01 \nunit        :            km \n\n\n\n\n\n\n# Assign projection system SVY21 / Singapore TM (EPSG:3414)\ncrs(kde_childcareSG_bw_terra) &lt;- \"EPSG:3414\"\n\n\n# Re-check raster details, now with CRS applied\nkde_childcareSG_bw_terra\n\nclass       : SpatRaster \nsize        : 128, 128, 1  (nrow, ncol, nlyr)\nresolution  : 0.4162063, 0.2250614  (x, y)\nextent      : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :         lyr.1 \nmin value   : -4.314789e-15 \nmax value   :  3.063698e+01 \nunit        :            km \n\n\n\n\n\n\ntm_shape(kde_childcareSG_bw_terra) +      # Define raster object to be plotted\n  tm_raster(col.scale =                        # Set raster colour scheme\n              tm_scale_continuous(values=\"viridis\"), \n            col.legend = tm_legend(       # Add legend for density values\n              title = \"Density values\",   # Legend title\n              title.size = 0.7,           # Legend title text size\n              text.size = 0.7),           # Legend labels text size\n            bg.color = \"white\",           # Background colour of map\n            bg.alpha = 0.7,               # Transparency of background\n            position = tm_pos_in(\"right\",\"bottom\"), # Place legend bottom-right\n            frame = TRUE) +               # Draw frame around raster\n  tm_graticules(labels.size = 0.7) +      # Add graticule grid with label size 0.7\n  tm_compass() +                          # Add compass to map\n  tm_layout(scale = 1.0)                  # Set layout scale\n\n[tm_raster()] Arguments `bg.color`, `bg.alpha`, `position`, and `frame`\nunknown.\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npg &lt;- mpsz_cl %&gt;%                         # Create dataset 'pg' by filtering master plan polygons\n  filter(PLN_AREA_N == \"PUNGGOL\")         # Keep only polygons where planning area = Punggol\n\ntm &lt;- mpsz_cl %&gt;%                         # Create dataset 'tm'\n  filter(PLN_AREA_N == \"TAMPINES\")        # Keep only polygons where planning area = Tampines\n\nck &lt;- mpsz_cl %&gt;%                         # Create dataset 'ck'\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")   # Keep only polygons where planning area = Choa Chu Kang\n\njw &lt;- mpsz_cl %&gt;%                         # Create dataset 'jw'\n  filter(PLN_AREA_N == \"JURONG WEST\")     # Keep only polygons where planning area = Jurong West\n\n\npar(mfrow=c(2,2))                              # Arrange plotting area into 2 rows × 2 columns\n\nplot(st_geometry(pg), main=\"Punggol\")          # Plot polygon boundary of Punggol planning area\nplot(st_geometry(tm), main=\"Tampines\")         # Plot polygon boundary of Tampines planning area\nplot(st_geometry(ck), main=\"Choa Chu Kang\")    # Plot polygon boundary of Choa Chu Kang planning area\nplot(st_geometry(jw), main=\"Jurong West\")      # Plot polygon boundary of Jurong West planning area\n\n\n\n\n\n\n\n\n\n\n\n\npg_owin &lt;- as.owin(pg)     # Convert Punggol polygon(s) to an 'owin' window (study region)\ntm_owin &lt;- as.owin(tm)     # Convert Tampines polygon(s) to 'owin'\nck_owin &lt;- as.owin(ck)     # Convert Choa Chu Kang polygon(s) to 'owin'\njw_owin &lt;- as.owin(jw)     # Convert Jurong West polygon(s) to 'owin'\n\n\n\n\n\nchildcare_pg_ppp &lt;- childcare_ppp[pg_owin]  # Clip national points to Punggol window (points inside only)\nchildcare_tm_ppp &lt;- childcare_ppp[tm_owin]  # Clip points to Tampines window\nchildcare_ck_ppp &lt;- childcare_ppp[ck_owin]  # Clip points to Choa Chu Kang window\nchildcare_jw_ppp &lt;- childcare_ppp[jw_owin]  # Clip points to Jurong West window\n\n\nchildcare_pg_ppp_km &lt;- rescale.ppp(  # Rescale Punggol points from metres → kilometres\n  childcare_pg_ppp, 1000, \"km\"       # divide coords by 1000; label new unit as \"km\"\n)\nchildcare_tm_ppp_km &lt;- rescale.ppp(  # Rescale Tampines points to kilometres\n  childcare_tm_ppp, 1000, \"km\"\n)\nchildcare_ck_ppp_km &lt;- rescale.ppp(  # Rescale CCK points to kilometres\n  childcare_ck_ppp, 1000, \"km\"\n)\nchildcare_jw_ppp_km &lt;- rescale.ppp(  # Rescale Jurong West points to kilometres\n  childcare_jw_ppp, 1000, \"km\"\n)\n\n\npar(mfrow = c(2,2))               # Arrange plotting area into a 2×2 grid\n\nplot(unmark(childcare_pg_ppp_km), # Plot Punggol points (unmark = hide text marks)\n     main = \"Punggol\")            # Panel title\n\nplot(unmark(childcare_tm_ppp_km), # Plot Tampines points\n     main = \"Tampines\")\n\nplot(unmark(childcare_ck_ppp_km), # Plot Choa Chu Kang points\n     main = \"Choa Chu Kang\")\n\nplot(unmark(childcare_jw_ppp_km), # Plot Jurong West points\n     main = \"Jurong West\")\n\n\n\n\n\n\n\npar(mfrow = c(1,1))               # Reset plotting layout back to single panel\n\n\n\n\n\n\nclarkevans.test(childcare_ck_ppp, # Clark–Evans test for the Choa Chu Kang point pattern\n  correction  = \"none\",           # No edge correction (consistent with slides)\n  clipregion  = NULL,             # Use the pattern's own observation window\n  alternative = c(\"two.sided\"),   # Two-sided hypothesis (clustered or regular)\n  nsim        = 999               # 999 CSR simulations for p-value\n)                                 # End of clarkevans.test call\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.84097, p-value = 0.008866\nalternative hypothesis: two-sided\n\n\n\n\n\nclarkevans.test(childcare_tm_ppp,   # Clark–Evans test for the Tampines point pattern\n  correction  = \"none\",             # No edge correction (to match slides)\n  clipregion  = NULL,               # Do not clip by an additional region (use pattern's window)\n  alternative = c(\"two.sided\"),     # Two-sided test: allow clustering (R&lt;1) or regularity (R&gt;1)\n  nsim        = 999                 # Monte-Carlo CSR simulations (999 replicates)\n)                                   # End of clarkevans.test call\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.66817, p-value = 6.58e-12\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\npar(mfrow = c(2,2))                # Arrange plotting window into 2 rows × 2 columns\n\nplot(density(childcare_pg_ppp_km,  # KDE for Punggol (data already rescaled to km)\n             sigma = bw.diggle,    # Use Diggle’s automatic bandwidth selector\n             edge  = TRUE,         # Apply edge correction (reduces boundary bias)\n             kernel = \"gaussian\"), # Gaussian kernel (smooth bell-shaped influence)\n     main = \"Punggol\")             # Panel title\n\nplot(density(childcare_tm_ppp_km,  # KDE for Tampines\n             sigma = bw.diggle,    # Same bandwidth rule for comparability\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Tampines\")            # Panel title\n\nplot(density(childcare_ck_ppp_km,  # KDE for Choa Chu Kang\n             sigma = bw.diggle,    # Diggle bandwidth (km)\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Choa Chu Kang\")       # Panel title\n\nplot(density(childcare_jw_ppp_km,  # KDE for Jurong West\n             sigma = bw.diggle,    # Diggle bandwidth (km)\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Jurong West\")         # Panel title"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#overview",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#overview",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "The specific questions we would like to answer are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nIf the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#installing-and-loading-the-r-packagesn",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#installing-and-loading-the-r-packagesn",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "# Install 'pacman' once if you do not have it (uncomment next line and run only once)\n# install.packages(\"pacman\")\n\n# Use pacman::p_load() to install (if needed) and then load the listed packages automatically\npacman::p_load(\n  sf,            # modern vector geospatial data handling (Simple Features)\n  terra,         # modern raster + vector geospatial data handling\n  spatstat,      # meta‑package: tools for spatial point pattern analysis (SPPA)\n  tmap,          # cartographic and interactive mapping\n  rvest,         # web‑scraping helper used here to parse HTML inside KML attributes\n  tidyverse      # data wrangling helpers (dplyr, purrr, stringr, readr, etc.)\n)"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#importing-and-wrangling-geospatial-data-sets",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#importing-and-wrangling-geospatial-data-sets",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "Import the Master Plan 2019 Subzone (No Sea) polygons as sf and set projection to EPSG:3414 (SVY21 / Singapore TM)\n\n# Read the subzone boundary KML into an sf object named 'mpsz_sf'\nmpsz_sf &lt;- sf::st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\") %&gt;%\n  sf::st_zm(drop = TRUE, what = \"ZM\") %&gt;%  # remove Z (elevation) and M (measure) dimensions to keep 2D only\n  sf::st_transform(crs = 3414)   \n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex02/Data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nWhy remove Z/M? Our analysis is purely 2D on a planar map. Extra dimensions are unnecessary and can break some operations.\nWhy EPSG:3414? This is the standard projected system for Singapore. Distances/areas become meaningful (meters) instead of degrees.\n\nExtract REGION_N, PLN_AREA_N, SUBZONE_N, SUBZONE_C from the Description field (HTML inside KML) (as in the guidance helper function)\n\n# Define a small helper function to pull one named item from the HTML table stored in the 'Description' field\nextract_kml_field &lt;- function(html_text, field_name) {\n  if (is.na(html_text) || html_text == \"\") return(NA_character_) # if Description is empty, return NA immediately\n\n  page &lt;- rvest::read_html(html_text)       # parse the HTML string\n  rows &lt;- rvest::html_elements(page, \"tr\")  # get all table rows (&lt;tr&gt;)\n\n  # Find the row whose header cell (&lt;th&gt;) equals the field_name, then take the value in its &lt;td&gt;\n  value &lt;- rows %&gt;%\n    purrr::keep(~ rvest::html_text2(rvest::html_element(.x, \"th\")) == field_name) %&gt;%\n    rvest::html_element(\"td\") %&gt;%\n    rvest::html_text2()\n\n  if (length(value) == 0) NA_character_ else value    # if not found, return NA\n}\n\n\n# Apply the function to build clean attributes from 'Description'\nmpsz_sf &lt;- mpsz_sf %&gt;%\n  dplyr::mutate(\n    REGION_N   = purrr::map_chr(Description, extract_kml_field, \"REGION_N\"),   # region name\n    PLN_AREA_N = purrr::map_chr(Description, extract_kml_field, \"PLN_AREA_N\"), # planning area name\n    SUBZONE_N  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_N\"),  # subzone name\n    SUBZONE_C  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_C\")   # subzone code\n  ) %&gt;%\n  dplyr::select(-Name, -Description) %&gt;%     # drop original noisy fields\n  dplyr::relocate(geometry, .after = dplyr::last_col())  # move geometry column to the end for readability\n\n\nCheckpoint: You now have clean polygon attributes and the proper CRS.\n\n(Optional) Filter out offshore/irrelevant areas exactly like in the slides\n\n# Create a cleaned version that excludes southern group, western islands, and north‑eastern islands (as per slides)\nmpsz_cl &lt;- mpsz_sf %&gt;%\n  dplyr::filter(\n    SUBZONE_N != \"SOUTHERN GROUP\",\n    PLN_AREA_N != \"WESTERN ISLANDS\",\n    PLN_AREA_N != \"NORTH-EASTERN ISLANDS\"\n  )\n\n# Save a small RDS for repeatability (optional)\n# readr::write_rds(mpsz_cl, \"chap04/data/mpsz_cl.rds\")\n\nImport Child Care Services points as sf, drop Z/M, and project to EPSG:3414\n\n# Read childcare locations KML into an sf object named 'childcare_sf'\nchildcare_sf &lt;- sf::st_read(\"data/geospatial/ChildCareServices.kml\") %&gt;%\n  sf::st_zm(drop = TRUE, what = \"ZM\") %&gt;%     # keep 2D only\n  sf::st_transform(crs = 3414)         \n\nReading layer `CHILDCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex02/Data/geospatial/ChildCareServices.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n# Quick mapping of both layers to verify alignment (childcare points inside planning subzones)\ntmap::tmap_mode(\"plot\")  # ensure static plotting mode\n\nℹ tmap mode set to \"plot\".\n\n# Plot subzones with childcare points overlaid\ntmap::tm_shape(mpsz_cl) +\n  tmap::tm_polygons(col = \"grey85\", border.col = \"black\") +\n  tmap::tm_shape(childcare_sf) +\n  tmap::tm_dots(size = 0.2, col = \"red\")\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n\n\n\n\n\n\n\n\n\n\nWhat to check: All childcare centres (black dots) fall neatly inside Singapore’s subzones (grey polygons). If not, check CRS and transformations.\n\nInteractive mapping with tmap + leaflet\n\n# Switch tmap into interactive mode (internally uses the leaflet library)\ntmap::tmap_mode(\"view\")                # enable pan/zoom and feature popups\n\nℹ tmap mode set to \"view\".\n\n# Draw an interactive point map on top of a web basemap\n# - Click a point to see its attributes\n# - Use the layer control to change basemaps (e.g., ESRI.WorldGrayCanvas, OpenStreetMap)\ntmap::tm_shape(childcare_sf) +         # select the childcare sf layer\n  tmap::tm_dots()                      # render as clickable points\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n# IMPORTANT: After exploring, switch back to static plotting for the rest of the workflow\n# This avoids keeping too many active web-map connections during knitting/deployment\ntmap::tmap_mode(\"plot\")               # restore static plotting mode\n\nℹ tmap mode set to \"plot\".\n\n\n\n# Switch to interactive mode for web-map style exploration\n# tmap::tmap_mode(\"view\")\n# \n# # Create an interactive map where you can zoom and click points\n# tmap::tm_shape(mpsz_cl) +\n#   tmap::tm_polygons(col = \"lightgrey\", border.col = \"white\") +\n#   tmap::tm_shape(childcare_sf) +\n#   tmap::tm_dots(col = \"blue\", size = 0.5)\n# \n# # Always switch back to static plotting when done\n# tmap::tmap_mode(\"plot\")\n\n\nNotes: In interactive mode you can zoom, pan, and click childcare points to view their attributes. Remember to switch back to plot mode before continuing with static maps."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "# Convert the sf point layer to spatstat's planar point pattern (ppp) object\nchildcare_ppp &lt;- spatstat.geom::as.ppp(childcare_sf)  # now suitable for SPPA functions\n\n\n# Verify the class\nclass(childcare_ppp)  # expect: \"ppp\"\n\n[1] \"ppp\"\n\n\n\n# Peek at summary to understand the point pattern and its window\nsummary(childcare_ppp)  # quick glance (number of points, intensity, window size)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nMark variables: Name, Description\nSummary:\n     Name           Description       \n Length:1925        Length:1925       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n# Convert the cleaned subzones into a single observation window (owin) for clipping/analysis\nsg_owin &lt;- spatstat.geom::as.owin(mpsz_cl)\n\n\n# Verify the class\nclass(sg_owin)  # expect: \"owin\"\n\n[1] \"owin\"\n\n\n\n# Visualize the boundary to confirm it looks right\nplot(sg_owin, main = \"sg_owin — Singapore Observation Window\")"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#combine-the-ppp-with-the-owin-keep-only-points-inside-singapore",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#combine-the-ppp-with-the-owin-keep-only-points-inside-singapore",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "# Clip the point pattern to the Singapore window to exclude any points outside the boundary\nchildcareSG_ppp &lt;- childcare_ppp[sg_owin]\n\n\n# Confirm the combined object (ppp with polygon window information)\nchildcareSG_ppp  # should report a polygonal window and the point count\n\nMarked planar point pattern: 1925 points\nMark variables: Name, Description \nwindow: polygonal boundary\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#clark-evan-test-for-nearest-neighbour-analysis",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#clark-evan-test-for-nearest-neighbour-analysis",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "Hypotheses:\n\n\\(H_0\\): Childcare centres are randomly distributed (Complete Spatial Randomness, CSR).\n\\(H_1\\): Childcare centres are not randomly distributed (clustered or regular).\n\nThe 95% confident interval will be used.\nThe Clark–Evans test returns an index R:\n\n\\(R &lt; 1 → clustered\\).\n\\(R = 1 → random\\)\n\\(R &gt; 1 → regular\\)\n\n\n\n\n# Run Clark–Evans using observed pattern only (Z-test)\nce_noCSR &lt;- spatstat.explore::clarkevans.test(\nX = childcareSG_ppp, # ppp object of childcare centres\ncorrection = \"none\", # no edge correction (per slides)\nalternative = \"clustered\" # one-sided: test for clustering (R &lt; 1)\n)\n\n# Show results\nce_noCSR\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNoteClark–Evans Test without CSR (Z-test)\n\n\n\n\nR = 0.53532 (&lt; 1) → indicates clustering.\np-value &lt; 2.2e-16 (&lt; 0.05): → reject H₀.\nStatistical conclusion: Distribution is not random; strong clustering exists.\nBusiness communication: Childcare centres are concentrated in specific neighbourhoods. This reflects demand-driven placement but may leave some areas underserved. Authorities should consider equity in future allocations.\n\n\n\n\n\n\n\n# Monte‑Carlo Clark–Evans: compare observed R to R from CSR simulations (nsim = 99)\nce_MC &lt;- spatstat.explore::clarkevans.test( # compute simulated p‑value under CSR\nX = childcareSG_ppp, # same ppp as above (metres)\ncorrection = \"none\", # keep consistent with slides\nalternative = \"clustered\", # one‑sided for clustering (R &lt; 1)\nmethod = \"MonteCarlo\", # enable Monte‑Carlo simulation mode\nnsim = 99 # number of CSR replicates as per slides\n) # end clarkevans.test call\n\nce_MC\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value = 0.01\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNoteClark–Evans Test with CSR (Monte Carlo, nsim=99)\n\n\n\n\nR = 0.53532 (&lt; 1): → clustering again.\np-value = 0.01 (&lt; 0.05) → reject H₀ at 95% CI.\nStatistical conclusion: Distribution is not random; clustering is statistically significant even under CSR simulations.\nBusiness communication: Confirms earlier finding with stronger robustness. Clustering is not due to chance — it is systematic. Policymakers should expand childcare access in low-density areas to reduce inequality in service availability."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#kernel-density-estimation-kde-method",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#kernel-density-estimation-kde-method",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "Goal: Turn points into a smooth surface to reveal hotspots. Bandwidth controls smoothness; kernel controls the spread shape.\n\n\n\nkde_SG_diggle &lt;- density(          # Create KDE surface and save to object 'kde_SG_diggle'\n  childcareSG_ppp,                 # Input: childcare centres dataset in 'ppp' format\n  sigma = bw.diggle,               # Bandwidth: automatic smoothing radius (Diggle’s selector)\n  edge  = TRUE,                    # Apply edge correction to fix boundary underestimation\n  kernel = \"gaussian\"              # Kernel type: Gaussian (bell-shaped smoothing function)\n)                                  # End of density() call\n\n\n# Plot the kernel density estimation surface for childcare centres\nplot(kde_SG_diggle)        \n\n\n\n\n\n\n\n\n\nsummary(kde_SG_diggle)\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2667.538, 55941.94] x [21448.47, 50256.33] units\ndimensions of each pixel: 416 x 225.0614 units\nImage is defined on a subset of the rectangular grid\nSubset area = 669941961.12249 square units\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-5.824417e-21, 3.063698e-05]\n    integral = 1927.788\n    mean = 2.877545e-06\n\n\n\n# Calculate optimal bandwidth (smoothing parameter) using Diggle’s method\nbw &lt;- bw.diggle(childcareSG_ppp)             \nbw   # Display the selected bandwidth value\n\n   sigma \n295.9712 \n\n\n\n\n\n\nchildcareSG_ppp_km &lt;- rescale.ppp(  # Rescale the point pattern dataset from metres to kilometres\n  childcareSG_ppp, 1000, \"km\")      # Display the selected bandwidth value \n\n\nkde_childcareSG_km &lt;- density(  # Compute kernel density estimation on rescaled dataset\n  childcareSG_ppp_km,           # Input point pattern in kilometres\n  sigma = bw.diggle,            # Use Diggle’s optimal bandwidth (sigma)\n  edge  = TRUE,                 # Apply edge correction to fix boundary bias\n  kernel = \"gaussian\"           # Use Gaussian kernel for smoothing\n)\n\n\n# Plot KDE surface for childcare centres (units in km)\nplot(kde_childcareSG_km)\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute bandwidth using Cronie & van Lieshout method\nbw.CvL(childcareSG_ppp_km)\n\n   sigma \n4.357209 \n\n\n\n# Compute bandwidth using Scott’s rule-of-thumb method\nbw.scott(childcareSG_ppp_km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\n\n\n# Compute bandwidth using likelihood cross-validation (PPL)\nbw.ppl(childcareSG_ppp_km)\n\n   sigma \n0.378997 \n\n\n\n# Re-compute bandwidth using Diggle’s method (on km scale)\nbw.diggle(childcareSG_ppp_km)\n\n    sigma \n0.2959712 \n\n\n\nkde_childcareSG_ppl &lt;- density(   # KDE using PPL bandwidth for smoothing\n  childcareSG_ppp_km,             # Input dataset in kilometres\n  sigma = bw.ppl,                 # Use bandwidth chosen by PPL\n  edge  = TRUE,                   # Apply edge correction\n  kernel = \"gaussian\"             # Gaussian smoothing kernel\n)\n\npar(mfrow=c(1,2))                            # Set plot area into 1 row, 2 columns for comparison\nplot(kde_childcareSG_km, main = \"bw.diggle\") # Plot KDE using Diggle bandwidth\nplot(kde_childcareSG_ppl, main = \"bw.ppl\")   # Plot KDE using PPL bandwidth\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))                        # Divide plotting window into 2 rows × 2 columns\n\nplot(density(childcareSG_ppp_km,         # KDE with Gaussian kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")                    # Title for Gaussian plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Epanechnikov kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")                # Title for Epanechnikov plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Quartic kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")                     # Title for Quartic plot\n\nplot(density(childcareSG_ppp_km,         # KDE with Disc kernel\n             sigma=0.2959712, edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")                        # Title for Disc plot"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#fixed-and-adaptive-kde",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "kde_childcareSG_fb &lt;- density(   # KDE using fixed bandwidth method\n  childcareSG_ppp_km,            # Input childcare centres dataset (in km)\n  sigma = 0.6,                   # Fixed bandwidth = 0.6 km (600 metres)\n  edge  = TRUE,                  # Apply edge correction at boundaries\n  kernel = \"gaussian\"            # Gaussian smoothing kernel\n)\n\nplot(kde_childcareSG_fb)         # Plot the fixed bandwidth KDE result\n\n\n\n\n\n\n\n\n\n\n\n\nkde_childcareSG_ab &lt;- adaptive.density(    # KDE using adaptive bandwidth method\n  childcareSG_ppp_km,                      # Input childcare centres dataset (in km)\n  method=\"kernel\"                          # Kernel smoothing method\n)\n\nplot(kde_childcareSG_ab)  # Plot the adaptive bandwidth KDE result\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))         # Set plotting window to 1 row × 2 columns\nplot(kde_childcareSG_fb, main = \"Fixed bandwidth\")     # Show fixed bandwidth KDE\nplot(kde_childcareSG_ab, main = \"Adaptive bandwidth\")  # Show adaptive bandwidth KDE"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#plotting-cartographic-quality-kde-map",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#plotting-cartographic-quality-kde-map",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "# Convert KDE (im class) into SpatRaster object\nkde_childcareSG_bw_terra &lt;- rast(kde_childcareSG_km)\n\n\n# Check the class of the raster object (should be \"SpatRaster\")\nclass(kde_childcareSG_bw_terra)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\n\n# Print raster properties: resolution, extent, units, etc.\nkde_childcareSG_bw_terra\n\nclass       : SpatRaster \nsize        : 128, 128, 1  (nrow, ncol, nlyr)\nresolution  : 0.4162063, 0.2250614  (x, y)\nextent      : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        :         lyr.1 \nmin value   : -4.314789e-15 \nmax value   :  3.063698e+01 \nunit        :            km \n\n\n\n\n\n\n# Assign projection system SVY21 / Singapore TM (EPSG:3414)\ncrs(kde_childcareSG_bw_terra) &lt;- \"EPSG:3414\"\n\n\n# Re-check raster details, now with CRS applied\nkde_childcareSG_bw_terra\n\nclass       : SpatRaster \nsize        : 128, 128, 1  (nrow, ncol, nlyr)\nresolution  : 0.4162063, 0.2250614  (x, y)\nextent      : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :         lyr.1 \nmin value   : -4.314789e-15 \nmax value   :  3.063698e+01 \nunit        :            km \n\n\n\n\n\n\ntm_shape(kde_childcareSG_bw_terra) +      # Define raster object to be plotted\n  tm_raster(col.scale =                        # Set raster colour scheme\n              tm_scale_continuous(values=\"viridis\"), \n            col.legend = tm_legend(       # Add legend for density values\n              title = \"Density values\",   # Legend title\n              title.size = 0.7,           # Legend title text size\n              text.size = 0.7),           # Legend labels text size\n            bg.color = \"white\",           # Background colour of map\n            bg.alpha = 0.7,               # Transparency of background\n            position = tm_pos_in(\"right\",\"bottom\"), # Place legend bottom-right\n            frame = TRUE) +               # Draw frame around raster\n  tm_graticules(labels.size = 0.7) +      # Add graticule grid with label size 0.7\n  tm_compass() +                          # Add compass to map\n  tm_layout(scale = 1.0)                  # Set layout scale\n\n[tm_raster()] Arguments `bg.color`, `bg.alpha`, `position`, and `frame`\nunknown.\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#first-order-sppa-at-the-planning-subzone-level",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#first-order-sppa-at-the-planning-subzone-level",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "pg &lt;- mpsz_cl %&gt;%                         # Create dataset 'pg' by filtering master plan polygons\n  filter(PLN_AREA_N == \"PUNGGOL\")         # Keep only polygons where planning area = Punggol\n\ntm &lt;- mpsz_cl %&gt;%                         # Create dataset 'tm'\n  filter(PLN_AREA_N == \"TAMPINES\")        # Keep only polygons where planning area = Tampines\n\nck &lt;- mpsz_cl %&gt;%                         # Create dataset 'ck'\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")   # Keep only polygons where planning area = Choa Chu Kang\n\njw &lt;- mpsz_cl %&gt;%                         # Create dataset 'jw'\n  filter(PLN_AREA_N == \"JURONG WEST\")     # Keep only polygons where planning area = Jurong West\n\n\npar(mfrow=c(2,2))                              # Arrange plotting area into 2 rows × 2 columns\n\nplot(st_geometry(pg), main=\"Punggol\")          # Plot polygon boundary of Punggol planning area\nplot(st_geometry(tm), main=\"Tampines\")         # Plot polygon boundary of Tampines planning area\nplot(st_geometry(ck), main=\"Choa Chu Kang\")    # Plot polygon boundary of Choa Chu Kang planning area\nplot(st_geometry(jw), main=\"Jurong West\")      # Plot polygon boundary of Jurong West planning area\n\n\n\n\n\n\n\n\n\n\n\n\npg_owin &lt;- as.owin(pg)     # Convert Punggol polygon(s) to an 'owin' window (study region)\ntm_owin &lt;- as.owin(tm)     # Convert Tampines polygon(s) to 'owin'\nck_owin &lt;- as.owin(ck)     # Convert Choa Chu Kang polygon(s) to 'owin'\njw_owin &lt;- as.owin(jw)     # Convert Jurong West polygon(s) to 'owin'\n\n\n\n\n\nchildcare_pg_ppp &lt;- childcare_ppp[pg_owin]  # Clip national points to Punggol window (points inside only)\nchildcare_tm_ppp &lt;- childcare_ppp[tm_owin]  # Clip points to Tampines window\nchildcare_ck_ppp &lt;- childcare_ppp[ck_owin]  # Clip points to Choa Chu Kang window\nchildcare_jw_ppp &lt;- childcare_ppp[jw_owin]  # Clip points to Jurong West window\n\n\nchildcare_pg_ppp_km &lt;- rescale.ppp(  # Rescale Punggol points from metres → kilometres\n  childcare_pg_ppp, 1000, \"km\"       # divide coords by 1000; label new unit as \"km\"\n)\nchildcare_tm_ppp_km &lt;- rescale.ppp(  # Rescale Tampines points to kilometres\n  childcare_tm_ppp, 1000, \"km\"\n)\nchildcare_ck_ppp_km &lt;- rescale.ppp(  # Rescale CCK points to kilometres\n  childcare_ck_ppp, 1000, \"km\"\n)\nchildcare_jw_ppp_km &lt;- rescale.ppp(  # Rescale Jurong West points to kilometres\n  childcare_jw_ppp, 1000, \"km\"\n)\n\n\npar(mfrow = c(2,2))               # Arrange plotting area into a 2×2 grid\n\nplot(unmark(childcare_pg_ppp_km), # Plot Punggol points (unmark = hide text marks)\n     main = \"Punggol\")            # Panel title\n\nplot(unmark(childcare_tm_ppp_km), # Plot Tampines points\n     main = \"Tampines\")\n\nplot(unmark(childcare_ck_ppp_km), # Plot Choa Chu Kang points\n     main = \"Choa Chu Kang\")\n\nplot(unmark(childcare_jw_ppp_km), # Plot Jurong West points\n     main = \"Jurong West\")\n\n\n\n\n\n\n\npar(mfrow = c(1,1))               # Reset plotting layout back to single panel\n\n\n\n\n\n\nclarkevans.test(childcare_ck_ppp, # Clark–Evans test for the Choa Chu Kang point pattern\n  correction  = \"none\",           # No edge correction (consistent with slides)\n  clipregion  = NULL,             # Use the pattern's own observation window\n  alternative = c(\"two.sided\"),   # Two-sided hypothesis (clustered or regular)\n  nsim        = 999               # 999 CSR simulations for p-value\n)                                 # End of clarkevans.test call\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.84097, p-value = 0.008866\nalternative hypothesis: two-sided\n\n\n\n\n\nclarkevans.test(childcare_tm_ppp,   # Clark–Evans test for the Tampines point pattern\n  correction  = \"none\",             # No edge correction (to match slides)\n  clipregion  = NULL,               # Do not clip by an additional region (use pattern's window)\n  alternative = c(\"two.sided\"),     # Two-sided test: allow clustering (R&lt;1) or regularity (R&gt;1)\n  nsim        = 999                 # Monte-Carlo CSR simulations (999 replicates)\n)                                   # End of clarkevans.test call\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.66817, p-value = 6.58e-12\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\npar(mfrow = c(2,2))                # Arrange plotting window into 2 rows × 2 columns\n\nplot(density(childcare_pg_ppp_km,  # KDE for Punggol (data already rescaled to km)\n             sigma = bw.diggle,    # Use Diggle’s automatic bandwidth selector\n             edge  = TRUE,         # Apply edge correction (reduces boundary bias)\n             kernel = \"gaussian\"), # Gaussian kernel (smooth bell-shaped influence)\n     main = \"Punggol\")             # Panel title\n\nplot(density(childcare_tm_ppp_km,  # KDE for Tampines\n             sigma = bw.diggle,    # Same bandwidth rule for comparability\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Tampines\")            # Panel title\n\nplot(density(childcare_ck_ppp_km,  # KDE for Choa Chu Kang\n             sigma = bw.diggle,    # Diggle bandwidth (km)\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Choa Chu Kang\")       # Panel title\n\nplot(density(childcare_jw_ppp_km,  # KDE for Jurong West\n             sigma = bw.diggle,    # Diggle bandwidth (km)\n             edge  = TRUE,         # Edge correction on\n             kernel = \"gaussian\"), # Gaussian kernel\n     main = \"Jurong West\")         # Panel title"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#second-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#second-order-spatial-point-patterns-analysis",
    "title": "Hands-on_Ex02",
    "section": "5.5 Second-order Spatial Point Patterns Analysis",
    "text": "5.5 Second-order Spatial Point Patterns Analysis\nFirst-order asks “where are points denser?”; second-order asks “how do points interact with each other across distance?”\nWe’ll use four classical functions: - G(r) — nearest-neighbour distribution (from each event to its nearest event). - F(r) — empty-space distribution (from random locations to nearest event). - K(r) — accumulates neighbours within radius r; higher than CSR suggests clustering. - L(r) = √(K(r)/π) — variance-stabilised K; plot L(r) − r to read deviations easily.\nFor each function your Prof shows estimation and a Monte Carlo CSR test with envelopes."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on_Ex02",
    "section": "5.6 Analysing Spatial Point Process Using G-Function",
    "text": "5.6 Analysing Spatial Point Process Using G-Function\n\n5.6.1 Choa Chu Kang planning area\n\n5.6.1.1 Computing G-function estimation\n\nset.seed(1234)                                             # fix the random seed so plots are reproducible for students\nG_CK &lt;- Gest(childcare_ck_ppp, correction = \"border\")      # estimate nearest-neighbour CDF G(r) with border edge correction\nplot(G_CK, xlim = c(0, 500))                               # plot G(r) up to 500 m; dashed line shows CSR (Poisson) reference\n\n\n\n\n\n\n\n\n\n\n5.6.1.2 Performing Complete Spatial Randomness (CSR) Test — Monte Carlo\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n# run 999 CSR simulations; compute simulation envelopes for G(r)\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)   \n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n# plot observed G(r), CSR expectation, and 999-sim envelopes\nplot(G_CK.csr)    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReading the plot:\n\n\n\nIf the black observed curve is mostly above the grey envelope → points are clustered at those distances. If it is below → regular/repulsive (inhibition). Inside the envelope → not significantly different from CSR at that scale.\n\n\n\n\n\n5.6.2 Tampines planning area\n\n5.6.2.1 Computing G-function estimation\n\nG_tm &lt;- Gest(childcare_tm_ppp, correction = \"best\")        # estimate G(r) using 'best' automatic edge correction\nplot(G_tm)                                                 # plot observed G(r) vs theoretical CSR G(r)\n\n\n\n\n\n\n\n\n\n\n5.6.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_o\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest,   # simulate CSR envelopes for G(r) at Tampines\n                     correction = \"all\",       # request all supported edge corrections inside envelope calc\n                     nsim = 999)               # use 999 simulations as in Prof’s slide\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)                                 # display observed curve, CSR, and envelopes\n\n\n\n\n\n\n\n\n\nDecision rule (as per slides): Reject \\(H_0\\): CSR if the observed curve exits the envelope and the associated p-value &lt; 0.001 (α = 0.001)."
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on_Ex02",
    "section": "5.7 Analysing Spatial Point Process Using F-Function",
    "text": "5.7 Analysing Spatial Point Process Using F-Function\n\n5.7.1 Choa Chu Kang planning area\n\n5.7.1.1 Computing F-function estimation\n\nF_CK &lt;- Fest(childcare_ck_ppp)   # estimate empty-space CDF F(r): distance from random locations to nearest facility\nplot(F_CK)                       # plot multiple estimators and the CSR reference curve F_pois(r)\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)   # simulate 999 CSR patterns; compute F(r) envelopes\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReading the plot:\n\n\n\n\nF(r) above CSR → space tends to be closer to facilities than CSR (suggests clustering).\nF(r) below CSR → larger gaps than expected (suggests inhibition).\n\n\n\n5.7.3 Tampines planning area\n****5.7.3.1 Computing F-function estimation****\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n5.7.3.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on_Ex02",
    "section": "5.8 Analysing Spatial Point Process Using K-Function",
    "text": "5.8 Analysing Spatial Point Process Using K-Function\n\n5.8.1 Choa Chu Kang planning area\n\n5.8.1.1 Computing K-fucntion estimate\n\n# estimate K(r) using Ripley (isotropic) edge correction\nK_ck &lt;- Kest(childcare_ck_ppp, correction = \"Ripley\")      \n\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n5.8.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest,   # build CSR envelopes for K(r) in Tampines\n                     nsim = 99,                # 99 simulations exactly as shown\n                     rank = 1,                 # use rank-1 global envelope\n                     glocal = TRUE)            # enable global+local envelope calculation\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(K_tm.csr, . - r ~ r,                      # plot (K_hat - r) vs r using the formula used by Prof\n     xlab = \"d\", ylab = \"K(d)-r\",              # match labels from the screenshot\n     xlim = c(0, 500))                         # limit the x-axis to 0–500 m as in the slide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReading the plot:\n\n\n\n\nUnder CSR, K(r) = πr² and L(r) − r = 0.\nCurve above CSR → clustering; below → inhibition; inside envelopes → not significantly different from CSR.\n\n\n\n\n\n\n5.8.2 Tampines planning area\n\n5.8.2.1 Computing K-fucntion estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n5.8.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex02/Hand-on_Ex02.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on_Ex02",
    "section": "5.9 Analysing Spatial Point Process Using L-Function",
    "text": "5.9 Analysing Spatial Point Process Using L-Function\n\n5.9.1 Choa Chu Kang planning area\n\n5.9.1.1 Computing L Fucntion estimation\n\nL_ck &lt;- Lest(childcare_ck_ppp, correction = \"Ripley\")      # estimate L(r) using Ripley edge correction\nplot(L_ck, . - r ~ r,                                      # plot (L_hat - r) vs r to centre CSR at zero\n     ylab = \"L(d)-r\", xlab = \"d(m)\")                       # match axis labels exactly as shown\n\n\n\n\n\n\n\n\n\n\n5.9.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest,  # generate L-function CSR envelopes for CK\n                     nsim = 99,               # 99 simulations (as per screenshot)\n                     rank = 1,                # rank-1 global envelope\n                     glocal = TRUE)           # global+local envelope option\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(L_ck.csr, . - r ~ r,                     # plot (L_hat - r) vs r using Prof’s plotting formula\n     xlab = \"d\", ylab = \"L(d)-r\")             # axis labels exactly as in the slide\n\n\n\n\n\n\n\n\n\n\n\n5.9.2 Tampines planning area\n\n5.9.2.1 Computing L-function estimation\n\nL_tm &lt;- Lest(childcare_tm_ppp, correction = \"Ripley\")  # estimate L(r) for Tampines\nplot(L_tm, . - r ~ r,                                  # plot (L_hat - r) vs r as in Prof’s figure\n     ylab = \"L(d)-r\", xlab = \"d(m)\",                   # axis labels to match the slide\n     xlim = c(0, 1000))                                # distance window 0–1000 m as shown\n\n\n\n\n\n\n\n\n\n\n5.9.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest,  # CSR envelopes for L(r) in Tampines\n                     nsim = 99,               # 99 simulations\n                     rank = 1,                # rank-1 global envelope\n                     glocal = TRUE)           # global+local envelope handling\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(L_tm.csr, . - r ~ r,                     # plot (L_hat - r) vs r per Prof’s code\n     xlab = \"d\", ylab = \"L(d)-r\",             # keep labels consistent with the slide\n     xlim = c(0, 500))                        # show 0–500 m window exactly as given"
  },
  {
    "objectID": "In-Class_Ex03b/in-class_ex03b.html",
    "href": "In-Class_Ex03b/in-class_ex03b.html",
    "title": "In-class Exercise 3b: Working with Open Government Data",
    "section": "",
    "text": "1 Learning Outcome\nBy the end of this hands-on exercise, you will be able to: - Preparing ACRA (Accounting and Corporate Regulatory Authority) Information on Corporate Entities datasets downloaded from data.gov.sg portal for geocoding, - Geocoding the tidydata by using SLA OneMap API, - Converting the geocoded transaction data into sf point feature data.frame, and - Wrangling the sf point features to avoid overlapping point features.\n\n\n2 Loading the R package\n\npacman::p_load(tidyverse, sf, tmap, httr)\n\n\n\n3 Importing ACRA data\n\nfolder_path &lt;- \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/aspatial\"\nfile_list &lt;- list.files(path = folder_path, \n                        pattern = \"^ACRA*.*\\\\.csv$\", \n                        full.names = TRUE)\n\nacra_data &lt;- file_list %&gt;%\n  map_dfr(read_csv)\n\n\n\n4 Saving ACRA data\n\nwrite_rds(acra_data,\n          \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/rds/acra_data.rds\")\n\n\n\n5 Tidying ACRA data\n\nbiz_56111 &lt;- acra_data %&gt;%\n  select(1:24) %&gt;%\n  filter(primary_ssic_code == 56111) %&gt;%\n  rename(date = registration_incorporation_date) %&gt;%\n  mutate(date = as.Date(date),\n         YEAR = year(date),\n         MONTH_NUM = month(date),\n         MONTH_ABBR = month(date, \n                            label = TRUE, \n                            abbr = TRUE)) %&gt;% \n  mutate(\n    postal_code = str_pad(postal_code, \n    width = 6, side = \"left\", pad = \"0\")) %&gt;%\n           filter(YEAR == 2025)    \n\n\n\n6 Geocoding (It took 32 seconds to process the chunk of codes)\n\npostcodes &lt;- unique(biz_56111$postal_code)\n\nurl &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"\n\nfound &lt;- data.frame()\nnot_found &lt;- data.frame(postcode = character())\n\nfor (pc in postcodes) {\n  query &lt;- list(\n    searchVal = pc,\n    returnGeom = \"Y\",\n    getAddrDetails = \"Y\",\n    pageNum = \"1\"\n  )\n  \n  res &lt;- GET(url, query = query)\n  json &lt;- content(res)\n  \n  if (json$found != 0) {\n    df &lt;- as.data.frame(json$results, stringsAsFactors = FALSE)\n    df$input_postcode &lt;- pc\n    found &lt;- bind_rows(found, df)\n  } else {\n    not_found &lt;- bind_rows(not_found, data.frame(postcode = pc))\n  }\n}\n\n\n\n7 Tidying the geocoded data\n\nfound &lt;- found %&gt;%\n  select(1:10)\n\n\n\n8 Appending the location information\n\nbiz_56111 = biz_56111 %&gt;%\n  left_join(found, \n            by = c('postal_code' = 'POSTAL'))\n\n\n\n9 Saving the data\n\nwrite_rds(biz_56111, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/rds/biz_56111.rds\")\n\n\n\n10 Converting into SF data frame\n\nbiz_56111_sf &lt;- st_as_sf(biz_56111, \n                         coords = c(\"X\",\"Y\"),\n                         crs=3414) \n\n\n\n11 Visualising the distribution\n\nggplot(data = biz_56111,\n       aes(x = MONTH_ABBR)) +\n  geom_bar()"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html",
    "href": "In-Class_Ex01/in-class_ex01.html",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used: - tidyverse - sf - tmap - ggstatsplot\nWrite a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\npacman::p_load(tidyverse, sf, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "For the purpose of this in-class exercise, the following R packages will be used: - tidyverse - sf - tmap - ggstatsplot\nWrite a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\npacman::p_load(tidyverse, sf, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-1",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "2.1 The task",
    "text": "2.1 The task\n\nCreate a sub-folder called data in In-class_Ex01 folder.\nIf necessary visit data.gov.sg and download Master Plan 2014 Subzone Boundary (Web) from the portal. You are required to download both the ESRI shapefile and kml file.\nWrite a code chunk to import Master Plan 2014 Subzone Boundary (Web) in shapefile and kml save them in sf simple features data frame.\nWrite a code chunk to export mpsz14_shp sf data.frame into kml file save the output in data sub-folder. Name the output file MP14_SUBZONE_WEB_PL.\n\n\n# This code chunk imports shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data/MasterPlan2014SubzoneBoundaryWebSHP\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex01/data/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# This code chunk imports kml file\n# mpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryKML.kml\")\n\n\n# To export mpsz14_shp sf data.frame into kml file save the output in data sub-folder\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon."
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-2",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-2",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "3.1 The task",
    "text": "3.1 The task\n\nIf necessary visit data.gov.sg and download Pre-Schools Location from the portal. You are required to download both the kml and geojson files.\nWrite a code chunk to import Pre-Schools Location in kml geojson save them in sf simple features data frame.\n\n\n## This code chunk imports kml file.\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex01/data/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n# This code chunk imports geojson file\npreschool_geojson &lt;- st_read(\"data/PreSchoolsLocation.geojson\") \n\nReading layer `PreSchoolsLocation' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex01/data/PreSchoolsLocation.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-3",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-3",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "4.1 The task",
    "text": "4.1 The task\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both the kml file.\nMove MPSZ-2019 shapefile provided for In-class Exercise 1 folder on elearn to data sub-folder of In-class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No SEA) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n# To import shapefile\n# mpsz19_shp &lt;- st_read(dsn = \"data/\",\n#                 layer = \"MPSZ-2019\")\n\n\n# To import kml file\nmpsz19_kml &lt;- st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex01/data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-4",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-4",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "5.1 The task",
    "text": "5.1 The task\nWrite a code chunk to check the project of the imported sf objects\n\n# st_crs(mpsz19_shp)"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-5",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-5",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "6.1 The task",
    "text": "6.1 The task\nRe-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation\n\n# To import MPSZ-2019\n# mpsz19_shp &lt;- st_read(dsn = \"data/\",\n#                 layer = \"MPSZ-2019\") %&gt;%\n#   st_transform(crs = 3414)\n\n\n# To import PreSchoolsLocation.kml\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex01/data/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-6",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-6",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "7.1 The task",
    "text": "7.1 The task\nWrite a code chunk to count the number of pre-schools in each planning sub-zone.\n\n# mpsz19_shp &lt;- mpsz19_shp %&gt;%\n#   mutate(`PreSch Count` = lengths(\n#     st_intersects(mpsz19_shp, preschool)))"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-7",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-7",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "9.1 The task",
    "text": "9.1 The task\nUsing appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\n\n\n\n\n\n\nTip\n\n\n\nRefer to ggscatterstats() of ggstatsplot package.\n\n\n\n# mpsz$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\n# mpsz$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`)) \n# mpsz19_shp &lt;- as.data.frame(mpsz19_shp)\n# \n# ggscatterstats(data = mpsz19_shp,\n#                x = `PreSch Density`,\n#                y = `PreSch Count`,\n#                type = \"parametric\")"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-8",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-8",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "10.1 The task",
    "text": "10.1 The task\nVisit and extract the latest Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling from Singstat homepage.\n\npopdata &lt;- read_csv(\"data/respopagesextod2024.csv\")"
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-9",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-9",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "11.1 The task",
    "text": "11.1 The task\nWrite a code chunk to prepare a data.frame showing population by Planning Area and Planning subzone\n\npopdata2023 &lt;- popdata %&gt;% \n  group_by(PA, SZ, AG) %&gt;% \n  summarise(`POP`=sum(`Pop`)) %&gt;%  \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG,\n              values_from = POP)\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_over\""
  },
  {
    "objectID": "In-Class_Ex01/in-class_ex01.html#the-task-10",
    "href": "In-Class_Ex01/in-class_ex01.html#the-task-10",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "12.1 The task",
    "text": "12.1 The task\nWrite a code chunk to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#data-pre-processing-and-wrangling",
    "href": "Take-home_Ex02/take-home_ex02.html#data-pre-processing-and-wrangling",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "5 Data Pre-processing and Wrangling",
    "text": "5 Data Pre-processing and Wrangling\nThis section details the rigorous preparation of both spatial and aspatial datasets to establish a unified analytical foundation for the subsequent spatial and spatio-temporal analyses of Singapore’s bus-mobility patterns. The procedures convert raw geospatial layers and passenger-volume tables into metrically consistent, quality-assured analytical units capable of supporting Local Moran I, Local Indicator of Spatial Association (LISA), and EHSA. All geometries are projected into the Singapore Transverse Mercator coordinate system (EPSG 3414) to maintain measurements in meters. The workflow proceeds through seven major stages: importing and projecting geospatial layers, constructing a regular hexagonal grid, validating geometric integrity, filtering relevant spatial units, integrating the aspatial passenger data, and building a balanced space–time panel for subsequent statistical modelling.\n\n5.1 Import and projection of spatial layers\nAccurate spatial statistics require that all geometries share a common projected coordinate system with metre units. This subsection imports the bus stop points and the national planning boundary, removes any third dimension or measure attributes, and projects both layers into SVY21. Using a single projection prevents unit inconsistency and ensures that all subsequent operations such as intersections, buffering, grid construction, and area or distance calculations are correct. The outcome is a pair of clean spatial layers that form the base for mainland filtering and grid creation.\n\n# Read bus stop point features from the geospatial folder\nBusStop &lt;- st_read(\"data/geospatial/BusStop.shp\") %&gt;%  # load the point layer\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%                  # drop Z and M attributes if present\n  st_transform(crs = 3414)                             # project to SVY21 in meters\n\nReading layer `BusStop' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex02/data/geospatial/BusStop.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5172 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48285.52 ymax: 52983.82\nProjected CRS: SVY21\n\n# Read the planning boundary and set projection to SVY21\nmpsz &lt;- st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\") %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%    # drop Z and M for clean polygons\n  st_transform(crs = 3414)               # project to SVY21 in meters\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex02/data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n## Quick structural checks\n# st_crs(BusStop)           # should report EPSG 3414\n# st_crs(mpsz)              # should report EPSG 3414\n# st_geometry_type(BusStop) # should be POINT\n# st_geometry_type(mpsz)    # should be MULTIPOLYGON\n\n\n\n5.2 Mainland mask and mainland only bus stops\nAll analysis must reflect activity on the main island. The national boundary is therefore dissolved into a single geometry and decomposed into individual polygons. The polygon with the largest area is taken as the mainland. The bus stop layer is then filtered so that only points inside the mainland polygon are retained. This prevents inclusion of stops that lie on offshore islands or in reclaimed areas outside the main island footprint. The step concludes with a map and a count audit to demonstrate that the retained points agree with the mainland extent.\n\n# --- 5.2 Mainland mask and mainland only bus stops --------------------------\n\nsg_union &lt;- st_union(mpsz) %&gt;%   # dissolve all subzones into one geometry\n  st_make_valid()                # ensure the unioned geometry is valid\n\npolys_sf &lt;- sg_union %&gt;%         # take the unioned geometry\n  st_cast(\"POLYGON\") %&gt;%         # split into single polygon parts\n  st_as_sf()                     # convert to an sf data frame for table ops\n\nsg_main &lt;- polys_sf[             # select the polygon\n  which.max(st_area(polys_sf)),  # with the largest area which is the mainland\n  ,\n  drop = FALSE\n]\n\ninside_mainland &lt;- st_within(    # compute point within polygon as a logical matrix\n  BusStop, sg_main, sparse = FALSE\n)\n\nBusStop_in_SG &lt;- BusStop[        # keep only points inside mainland\n  inside_mainland[, 1],\n]\n\n# Audit counts of retained versus removed stops\nc(total = nrow(BusStop),\n  mainland = nrow(BusStop_in_SG),\n  removed = nrow(BusStop) - nrow(BusStop_in_SG))\n\n   total mainland  removed \n    5172     5167        5 \n\n# Optional visual check of the mainland filter\ntmap_mode(\"view\")\ntm_shape(sg_main) + tm_polygons(col = \"white\", border.col = \"grey50\") +\ntm_shape(BusStop_in_SG) + tm_dots(col = \"red\", size = 0.2, alpha = 0.7) +\ntm_layout(\n  title = \"Bus stops retained on the Singapore mainland\",\n  title.position = c(\"center\", \"top\"),\n  title.size = 1.5,\n  title.fontface = \"bold\",  \n  inner.margins = c(0.05, 0.08, 0.20, 0.05),  # adjust bottom margin pushes title below frame\n  outer.margins = c(0.05, 0.08, 0.08, 0.05),  # keeps white space around map\n  frame = TRUE,\n  legend.show = FALSE\n)\n\n\n\n\n\n\nThe output confirms that the mainland filtering process was successfully executed, with 5,172 bus stop records initially detected and 5,167 retained within the Singapore mainland boundary after applying spatial masking, indicating that only five points (about 0.1%) were excluded as they fell outside the valid mainland polygon, likely on offshore islands or along the coastline. The workflow using st_union(), st_make_valid(), and st_cast(“POLYGON”) effectively consolidated all polygons and retained the largest one representing the mainland, ensuring spatial integrity before point-in-polygon filtering. The use of st_within() provides a strict geometric constraint that omits points lying exactly on the boundary, which explains the minor data loss; this can be refined by substituting with st_intersects() or buffering the polygon slightly if inclusion of boundary points is desired. The generated Quality Assurance (QA) map visually validates the correctness of the masking process, showing bus stops confined within the white mainland region bordered in grey and no spurious points beyond the coastal limits. Overall, the output verifies that the data cleaning and spatial masking steps were precise and successful, leaving only legitimate mainland bus stops for further geospatial analysis.\n\n\n5.3 Hexagon grid creation over the mainland\nA regular hexagon tessellation is generated to define neutral analytical units across the mainland. Hexagons reduce directional bias and provide a compact neighbourhood structure. The cell size is set to seven hundred meters which corresponds to a three hundred seventy five metre apothem. The grid is created only over the mainland polygon to avoid generating cells over the sea. An outline polygon is also prepared for cartographic composition in subsequent checks and figures. The section ends with a map that shows complete coverage of the mainland by the grid.\n\n# --- 5.3 Hexagon grid creation over the mainland ----------------------------\n\nhexagon &lt;- st_make_grid(\n  sg_main,            # use mainland extent\n  cellsize = 750,     # meters which approximates two times the apothem\n  what = \"polygons\",  # request polygon output\n  square = FALSE      # request hexagons rather than squares\n) %&gt;%\n  st_sf()             # convert the grid to an sf object\n\nsg_outline &lt;- sg_main # keep mainland outline for mapping\n\n# Quick checks and a coverage map\nst_is_longlat(hexagon) # should be FALSE which means metre units\n\n[1] FALSE\n\nst_crs(hexagon)        # should be EPSG 3414\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nlength(hexagon)        # number of cells created\n\n[1] 1\n\ntmap_mode(\"plot\")\ntm_shape(sg_outline) + tm_polygons(col = \"palegreen3\", border.col = NA) +\ntm_shape(hexagon) + tm_borders(col = \"grey60\", lwd = 0.25) +\ntm_layout(\n  title = \"Hexagon grid covering the Singapore mainland\",\n  title.position = c(\"center\", \"top\"),\n  title.size = 1.5,\n  title.fontface = \"bold\",  \n  inner.margins = c(0.05, 0.08, 0.20, 0.05),  # adjust bottom margin pushes title below frame\n  outer.margins = c(0.05, 0.08, 0.08, 0.05),  # keeps white space around map\n  frame = TRUE,\n  legend.show = FALSE\n)\n\n\n\n\n\n\n\n\nThe output confirms that the hexagonal grid was successfully generated and projected in the correct coordinate reference system (EPSG:3414, SVY21 meters). The check st_is_longlat(hexagon) returned FALSE, verifying that the units are in meters rather than degrees, which is essential for accurate spatial analysis in Singapore’s projected coordinate system. The visual map shows a continuous tessellation of hexagonal cells fully covering the Singapore mainland polygon in light grey, overlaid by a green mainland outline for reference. This regular hexagonal structure offers a geometrically neutral and compact spatial framework, minimising directional bias compared to square grids. The map confirms that the grid uniformly covers the mainland extent without spilling into offshore waters, establishing a valid foundation for subsequent spatial aggregation and analysis (e.g., counting bus stops per cell or computing spatial statistics). Overall, the output verifies correct CRS, cell geometry, and mainland coverage, ensuring the tessellation is both spatially accurate and analytically reliable for the next stages of this study.\n\n\n5.4 Geometric verification of hexagon size\nThe grid must match the intended resolution. For a regular hexagon with apothem (\\(a\\)) a equal to three 75 meters, the theoretical flat to flat width is two times a which equals 750 meters, and the theoretical area equals \\(A = 2 \\sqrt3\\cdot a^2\\). Observed values from a sample cell are computed using the cell bounding box and the exact area. Close agreement between observed and theoretical values confirms that the grid represents the desired spatial scale.\n\n# --- 5.4 Geometric verification of hexagon size -----------------------------\n\none_hex   &lt;- hexagon[1, ]                       # select a sample hexagon\nbb        &lt;- st_bbox(one_hex)                   # get its bounding box\nwidth_obs &lt;- as.numeric(bb[\"xmax\"] - bb[\"xmin\"])# compute flat to flat width\narea_obs  &lt;- as.numeric(st_area(one_hex))       # compute area\n\na         &lt;- 375                                # apothem in meters\nwidth_exp &lt;- 2 * a                              # theoretical width\narea_exp  &lt;- 2 * sqrt(3) * a^2                  # theoretical area\n\nc(width_obs = width_obs, width_exp = width_exp,  # print both widths\n  area_obs = area_obs, area_exp = area_exp,      # and both areas\n  ratio_area = area_obs / area_exp)              # ratio should be close to one\n\n width_obs  width_exp   area_obs   area_exp ratio_area \n     750.0      750.0   487139.3   487139.3        1.0 \n\n\n\n\n5.5 Keep only the active hexagons that contain bus stops\nOnly cells that contain at least one mainland bus stop can originate trips. Cells without bus stops do not contribute information and would degrade the quality of spatial statistics by adding empty units. The number of bus stops within each cell is calculated using topological intersection, and only those with a positive count are retained. The result is a subset that represents the serviced mainland and forms the base for spatial aggregation and later second order analysis.\n\n# --- 5.5 Retain only cells that contain bus stops ---------------------------\n\nhexagon$busstop_count &lt;- lengths(        # count bus stops per hexagon\n  st_intersects(hexagon, BusStop_in_SG)\n)\n\nhexagon_active &lt;- dplyr::filter(         # keep cells with at least one stop\n  hexagon, busstop_count &gt; 0\n)\n\n# Distribution and a simple map for quality assurance\nsummary(hexagon$busstop_count)           # counts before filtering\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.702   2.000  20.000 \n\nmin(hexagon_active$busstop_count)        # should be at least one\n\n[1] 1\n\nnrow(hexagon_active)                     # number of active cells\n\n[1] 826\n\ntmap_mode(\"plot\")\ntm_shape(sg_outline) +\n  tm_polygons(col = \"palegreen3\", border.col = NA) +\ntm_shape(hexagon_active) + \n  tm_borders(col = \"white\", alpha = 1, border.col = \"grey40\", lwd = 0.25) +\n  tm_compass(type = \"8star\", size = 2, position = c(\"right\",\"bottom\")) +\ntm_scalebar(position = c(\"right\",\"bottom\")) +\ntm_layout(\n  title = \"Active mainland hexagons containing bus stops\",\n  title.position = c(\"center\", \"top\"),\n  title.size = 1.5,\n  title.fontface = \"bold\",  \n  inner.margins = c(0.05, 0.08, 0.20, 0.05),  # adjust bottom margin (0.10) pushes title below frame\n  outer.margins = c(0.05, 0.08, 0.08, 0.05),  # keeps white space around map\n  frame = TRUE,\n  legend.show = FALSE\n)\n\n\n\n\n\n\n\n\nThe output confirms that the filtering process accurately retained only the mainland hexagons that contain at least one bus stop, producing 826 active cells for subsequent spatial analysis. The statistical summary shows that the number of bus stops per hexagon ranges from zero to twenty, with an average of about 1.7, meaning most hexagons include one or two stops while a small number contain more. The QA map visually verifies this result as the green polygon represents the Singapore mainland boundary and the grey outlined hexagons indicate active cells that intersect with at least one bus stop. These active hexagons are distributed across the developed and populated parts of the island including the central, eastern, and northern regions, whereas large interior and peripheral zones such as the Central Water Catchment, Bukit Timah Nature Reserve, Lim Chu Kang, and Jurong Industrial Area show no active cells, reflecting the absence of public transport facilities in those regions. This confirms that the spatial filter worked correctly by excluding non urban and restricted areas, leaving a valid and realistic representation of Singapore’s operational bus stop network for further geospatial analysis.\n\n\n5.6 Assign stable identifiers to active cells\nA stable key is required to link spatial cells with aspatial tables and to keep results traceable. Sequential identifiers with leading zeros are created and attached to every active cell. These identifiers are used in all subsequent joins and aggregations. A quick check confirms that there are no duplicate keys and that all values are present.\n\n# --- 5.6 Assign stable identifiers ------------------------------------------\n\nhexagon_active &lt;- hexagon_active %&gt;%\n  mutate(HEX_ID = sprintf(\"H%04d\", row_number()))  # create codes H0001 and so on\n\nhead(hexagon_active$HEX_ID)              # preview the first few codes\n\n[1] \"H0001\" \"H0002\" \"H0003\" \"H0004\" \"H0005\" \"H0006\"\n\nany(duplicated(hexagon_active$HEX_ID))   # should be FALSE\n\n[1] FALSE\n\n\nThe output confirms that each active hexagonal cell was successfully assigned a unique identifier for reliable linking between spatial and aspatial data. Sequential codes such as H0001, H0002, and H0003 were generated using the sprintf() function, ensuring consistent formatting with leading zeros. The preview of the first six IDs confirms correct sequencing, while the validation check returned FALSE for duplicates, proving that all identifiers are unique. This step secures data integrity and traceability, allowing each cell to be distinctly referenced in subsequent spatial joins, aggregations, and visual analyses throughout our study.\n\n\n5.7 Integrate the passenger table with mainland cells\nPassenger volume data report boardings by origin stop code hour and day type. To analyse these counts spatially, each stop must inherit the identifier of the cell that contains it. A lookup from bus stop code to cell identifier is built with a point in polygon intersection restricted to the mainland. The passenger table is then harmonised by renaming and standardising keys and is joined to the lookup. Trip counts are aggregated by cell by day type and by hour to produce an hourly spatial table of passenger intensity across the mainland.\n\n# --- 5.7 Integrate passenger table with mainland cells ----------------------\n\nbs_hex &lt;- st_intersection(          # intersect stops with active cells\n  BusStop_in_SG, hexagon_active\n) %&gt;%\n  st_drop_geometry() %&gt;%            # drop geometry to keep a lean table\n  select(BUS_STOP_N, HEX_ID)        # retain stop code and cell id\n\nodbus &lt;- readr::read_csv(           # read passenger origin destination table\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/take-home_ex02/data/aspatial/origin_destination_bus_202508.csv\"\n)\n\ntrips &lt;- odbus %&gt;%\n  select(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, TOTAL_TRIPS) %&gt;% # keep needed fields\n  rename(BUS_STOP_N = ORIGIN_PT_CODE,             # standardise field names\n         HOUR_OF_DAY = TIME_PER_HOUR,\n         TRIPS = TOTAL_TRIPS) %&gt;%\n  mutate(BUS_STOP_N = str_pad(as.character(BUS_STOP_N), 5, pad = \"0\")) %&gt;% # code format\n  inner_join(bs_hex, by = \"BUS_STOP_N\") %&gt;%       # attach HEX_ID\n  group_by(HEX_ID, DAY_TYPE, HOUR_OF_DAY) %&gt;%     # group for aggregation\n  summarise(TRIPS = sum(TRIPS), .groups = \"drop\") # sum trips\n\nkable(head(trips))                                # inspect the structure\n\n\n\n\nHEX_ID\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\nWEEKDAY\n6\n120\n\n\nH0001\nWEEKDAY\n7\n111\n\n\nH0001\nWEEKDAY\n8\n77\n\n\nH0001\nWEEKDAY\n9\n66\n\n\nH0001\nWEEKDAY\n10\n32\n\n\nH0001\nWEEKDAY\n11\n66\n\n\n\n\nsort(unique(trips$DAY_TYPE))                      # list day types\n\n[1] \"WEEKDAY\"          \"WEEKENDS/HOLIDAY\"\n\nrange(trips$HOUR_OF_DAY, na.rm = TRUE)            # check hour range\n\n[1]  0 23\n\n\nThe output confirms that the passenger trip data containing more than 5.8 million records has been successfully linked to the mainland hexagonal grid through a spatial join, assigning each bus stop and its passenger counts to a corresponding hexagonal cell. Each record now carries both spatial and temporal attributes represented by the hexagon identifier, day type, and hour of day, allowing aggregation of trip volumes by space and time. The sample output for cell H0001 shows weekday trips ranging from 32 to 120 between 6 am and 11 am, clearly reflecting hourly variations that form the foundation for spatio-temporal analysis. Although this integration provides a crucial spatial-temporal linkage, it is not yet sufficient to construct a time-space cube, as the current table remains in a flat structure without explicit three-dimensional temporal stacking or spatial indexing. The next section will therefore build upon this output to prepare the data for time-space cube generation by restructuring it into a multidimensional format suitable for analysing dynamic passenger intensity across both space and time.\n\n\n5.8 Build a balanced space time panel with geometry\nA complete space time dataset requires the full set of cells across the full set of hours for each day type. The keys are created from all active cells the two canonical day types and the twenty four hours. The aggregated trips are left joined to this key and missing combinations are zero filled. Geometry from the active cells is attached to every row and a chronological index is created to allow ordering and trend analysis. Strict assertions confirm that the panel is complete and that each cell day combination contributes exactly twenty four hourly rows.\n\n# --- 5.8 Balanced space time panel with geometry ----------------------------\n\nkey_hex   &lt;- hexagon_active$HEX_ID              # full universe of active cells\nkey_days  &lt;- c(\"WEEKDAY\", \"WEEKENDS/HOLIDAY\")   # canonical day types\nkey_hours &lt;- 0:23                               # full set of hours\n\nkey_full &lt;- expand_grid(                        # full Cartesian product\n  HEX_ID = key_hex,\n  DAY_TYPE = key_days,\n  HOUR_OF_DAY = key_hours\n)\n\ntrips_full &lt;- key_full %&gt;%              # attach trip counts to the key\n  left_join(trips, by = c(\"HEX_ID\", \"DAY_TYPE\", \"HOUR_OF_DAY\")) %&gt;%\n  mutate(TRIPS = coalesce(TRIPS, 0L))   # zero fill missing combinations\n\ntrips_panel_sf &lt;- hexagon_active %&gt;%    # attach geometry to each row\n  select(HEX_ID, geometry) %&gt;%\n  right_join(trips_full, by = \"HEX_ID\") %&gt;%\n  mutate(datetime = as.POSIXct(sprintf(\"2024-05-01 %02d:00:00\", HOUR_OF_DAY),\n                               tz = \"UTC\")) %&gt;%\n  st_as_sf()                            # convert to sf object with geometry\n\nThe chunk code below will be executed to validate the completeness of balanced space-time cube.\n\n# # --- Validation: Balanced space–time panel completeness ---------------------\n# \n# expected_n &lt;- length(key_hex) * length(key_days) * length(key_hours)\n# \n# # Actual number of rows in the panel\n# actual_n &lt;- nrow(trips_panel_sf)\n# \n# cat(\"\\nValidation Summary for trips_panel_sf\\n\")\n# cat(\"---------------------------------------\\n\")\n# cat(\"Expected number of rows : \", expected_n, \"\\n\")\n# cat(\"Actual number of rows   : \", actual_n, \"\\n\")\n# \n# # Check that each HEX_ID × DAY_TYPE combination has exactly 24 rows\n# chk &lt;- trips_panel_sf %&gt;%\n#   count(HEX_ID, DAY_TYPE, name = \"n_rows\")\n# \n# rows_ok   &lt;- all(chk$n_rows == 24)\n# rows_fail &lt;- chk %&gt;% filter(n_rows != 24)\n# \n# cat(\"\\nPer-cell/day completeness check:\\n\")\n# cat(\"  All HEX_ID × DAY_TYPE combinations have 24 rows?  \",\n#     ifelse(rows_ok, \"✅ YES\", \"❌ NO\"), \"\\n\")\n# if (!rows_ok) {\n#   cat(\"  HEX_IDs failing completeness:\\n\")\n#   print(rows_fail)\n# }\n# \n# # Ensure uniqueness per HEX_ID × DAY_TYPE × HOUR_OF_DAY\n# dup &lt;- trips_panel_sf %&gt;%\n#   count(HEX_ID, DAY_TYPE, HOUR_OF_DAY, name = \"n\") %&gt;%\n#   filter(n != 1)\n# \n# dup_ok &lt;- nrow(dup) == 0\n# cat(\"\\nUniqueness check for HEX_ID × DAY_TYPE × HOUR_OF_DAY:\\n\")\n# cat(\"  Duplicates present? \", ifelse(dup_ok, \"✅ NO\", \"❌ YES\"), \"\\n\")\n# if (!dup_ok) {\n#   cat(\"  Duplicated keys:\\n\")\n#   print(dup)\n# }\n# \n# # Check for missing keys or invalid geometry\n# na_hex   &lt;- sum(is.na(trips_panel_sf$HEX_ID))\n# na_day   &lt;- sum(is.na(trips_panel_sf$DAY_TYPE))\n# na_hour  &lt;- sum(is.na(trips_panel_sf$HOUR_OF_DAY))\n# geom_val &lt;- all(st_is_valid(trips_panel_sf))\n# \n# cat(\"\\nKey and geometry diagnostics:\\n\")\n# cat(\"  Missing HEX_ID values  : \", na_hex, \"\\n\")\n# cat(\"  Missing DAY_TYPE values: \", na_day, \"\\n\")\n# cat(\"  Missing HOUR_OF_DAY    : \", na_hour, \"\\n\")\n# cat(\"  Geometry validity OK?  : \", ifelse(geom_val, \"✅ YES\", \"❌ NO\"), \"\\n\")\n# \n# # Simple assertion guards (keep them for safety)\n# stopifnot(actual_n == expected_n)\n# stopifnot(rows_ok)\n# stopifnot(dup_ok)\n# stopifnot(na_hex == 0, na_day == 0, na_hour == 0)\n# stopifnot(geom_val)\n# \n# cat(\"\\n✅  Validation completed successfully — trips_panel_sf is fully balanced, unique, and geometrically valid.\\n\")\n\nThe validation output confirms that the balanced space–time panel was successfully constructed and meets all completeness and integrity checks required for time–space cube preparation. The expected and actual row counts match at 39,648, proving that every active hexagon has records for both day types and all 24 hourly periods. Each cell–day combination contains exactly 24 rows, ensuring full temporal coverage, while no duplicate entries were detected across the HEX_ID, DAY_TYPE, and HOUR_OF_DAY dimensions. Additionally, there are no missing identifiers or invalid geometries, and all spatial features are confirmed valid. This guarantees that the dataset is fully balanced, unique, and spatially sound. With this validated panel, the data is now structurally sufficient for constructing a time–space cube, enabling spatio–temporal visualisation and analysis of passenger trip intensity across Singapore’s mainland bus network.\nNext, we will include a concise tabular preview and completeness summary of the trips_full dataset, inspecting and confirming that all spatial cells, day types, and hourly intervals are represented before geometry is attached. It provides transparent evidence of balanced coverage across time and space for subsequent spatio-temporal analyses.\n\n# --- 5.8a Informational tables for trips_full --------------------------------\n# Show a tidy preview of trips_full so the panel content is transparent\n\n# Order rows for human reading then take the first ten per day type\npreview_trips_full &lt;- trips_full %&gt;%\n  dplyr::arrange(HEX_ID, DAY_TYPE, HOUR_OF_DAY) %&gt;%\n  dplyr::group_by(DAY_TYPE) %&gt;%\n  dplyr::slice_head(n = 10) %&gt;%\n  dplyr::ungroup()\n\nknitr::kable(\n  preview_trips_full,\n  caption = \"Preview of trips_full ordered by cell day and hour\"\n)\n\n\nPreview of trips_full ordered by cell day and hour\n\n\nHEX_ID\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\nWEEKDAY\n0\n0\n\n\nH0001\nWEEKDAY\n1\n0\n\n\nH0001\nWEEKDAY\n2\n0\n\n\nH0001\nWEEKDAY\n3\n0\n\n\nH0001\nWEEKDAY\n4\n0\n\n\nH0001\nWEEKDAY\n5\n0\n\n\nH0001\nWEEKDAY\n6\n120\n\n\nH0001\nWEEKDAY\n7\n111\n\n\nH0001\nWEEKDAY\n8\n77\n\n\nH0001\nWEEKDAY\n9\n66\n\n\nH0001\nWEEKENDS/HOLIDAY\n0\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n1\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n2\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n3\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n4\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n5\n0\n\n\nH0001\nWEEKENDS/HOLIDAY\n6\n114\n\n\nH0001\nWEEKENDS/HOLIDAY\n7\n81\n\n\nH0001\nWEEKENDS/HOLIDAY\n8\n126\n\n\nH0001\nWEEKENDS/HOLIDAY\n9\n132\n\n\n\n\n# Compact completeness summary to accompany the preview\nsummary_trips_full &lt;- dplyr::summarise(\n  trips_full,\n  n_rows       = dplyr::n(),\n  n_hex        = dplyr::n_distinct(HEX_ID),\n  n_day_types  = dplyr::n_distinct(DAY_TYPE),\n  n_hours      = dplyr::n_distinct(HOUR_OF_DAY),\n  min_hour     = min(HOUR_OF_DAY, na.rm = TRUE),\n  max_hour     = max(HOUR_OF_DAY, na.rm = TRUE),\n  zeros_in_TRIPS = sum(TRIPS == 0, na.rm = TRUE)\n)\n\nknitr::kable(\n  summary_trips_full,\n  caption = \"Completeness summary of trips_full\"\n)\n\n\nCompleteness summary of trips_full\n\n\n\n\n\n\n\n\n\n\n\nn_rows\nn_hex\nn_day_types\nn_hours\nmin_hour\nmax_hour\nzeros_in_TRIPS\n\n\n\n\n39648\n826\n2\n24\n0\n23\n7502\n\n\n\n\n# Optional distribution by hour and day to verify balanced coverage\nby_hour_day &lt;- trips_full %&gt;%\n  dplyr::count(DAY_TYPE, HOUR_OF_DAY, name = \"n_records\") %&gt;%\n  dplyr::arrange(DAY_TYPE, HOUR_OF_DAY)\n\nknitr::kable(\n  by_hour_day,\n  caption = \"Record count in trips_full by day type and hour\"\n)\n\n\nRecord count in trips_full by day type and hour\n\n\nDAY_TYPE\nHOUR_OF_DAY\nn_records\n\n\n\n\nWEEKDAY\n0\n826\n\n\nWEEKDAY\n1\n826\n\n\nWEEKDAY\n2\n826\n\n\nWEEKDAY\n3\n826\n\n\nWEEKDAY\n4\n826\n\n\nWEEKDAY\n5\n826\n\n\nWEEKDAY\n6\n826\n\n\nWEEKDAY\n7\n826\n\n\nWEEKDAY\n8\n826\n\n\nWEEKDAY\n9\n826\n\n\nWEEKDAY\n10\n826\n\n\nWEEKDAY\n11\n826\n\n\nWEEKDAY\n12\n826\n\n\nWEEKDAY\n13\n826\n\n\nWEEKDAY\n14\n826\n\n\nWEEKDAY\n15\n826\n\n\nWEEKDAY\n16\n826\n\n\nWEEKDAY\n17\n826\n\n\nWEEKDAY\n18\n826\n\n\nWEEKDAY\n19\n826\n\n\nWEEKDAY\n20\n826\n\n\nWEEKDAY\n21\n826\n\n\nWEEKDAY\n22\n826\n\n\nWEEKDAY\n23\n826\n\n\nWEEKENDS/HOLIDAY\n0\n826\n\n\nWEEKENDS/HOLIDAY\n1\n826\n\n\nWEEKENDS/HOLIDAY\n2\n826\n\n\nWEEKENDS/HOLIDAY\n3\n826\n\n\nWEEKENDS/HOLIDAY\n4\n826\n\n\nWEEKENDS/HOLIDAY\n5\n826\n\n\nWEEKENDS/HOLIDAY\n6\n826\n\n\nWEEKENDS/HOLIDAY\n7\n826\n\n\nWEEKENDS/HOLIDAY\n8\n826\n\n\nWEEKENDS/HOLIDAY\n9\n826\n\n\nWEEKENDS/HOLIDAY\n10\n826\n\n\nWEEKENDS/HOLIDAY\n11\n826\n\n\nWEEKENDS/HOLIDAY\n12\n826\n\n\nWEEKENDS/HOLIDAY\n13\n826\n\n\nWEEKENDS/HOLIDAY\n14\n826\n\n\nWEEKENDS/HOLIDAY\n15\n826\n\n\nWEEKENDS/HOLIDAY\n16\n826\n\n\nWEEKENDS/HOLIDAY\n17\n826\n\n\nWEEKENDS/HOLIDAY\n18\n826\n\n\nWEEKENDS/HOLIDAY\n19\n826\n\n\nWEEKENDS/HOLIDAY\n20\n826\n\n\nWEEKENDS/HOLIDAY\n21\n826\n\n\nWEEKENDS/HOLIDAY\n22\n826\n\n\nWEEKENDS/HOLIDAY\n23\n826\n\n\n\n\n\nThe tables verify that trips_full is a complete and well ordered space time panel ready for cube construction. The preview shows rows sorted by cell, day type, and hour, with cell H0001 illustrating zero values at early hours and rising counts from 6 to 11, which matches expected morning activity. The completeness summary reports 39,648 rows across 826 hexagons, two day types, and twenty four hourly slots from zero to twenty three, with 7,502 zero trip entries correctly retained as structural zeros rather than missing data. The final check lists exactly 826 records for every hour within each day type, confirming that every cell day combination contributes twenty four rows. Together these results demonstrate full temporal coverage, consistent spatial indexing, and integrity of counts, which is sufficient for building the time space cube in the next step."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics and Applications Coursework",
    "section": "",
    "text": "Welcome to Geospatial Analytics and Application (ISSS626). This website collects my submission of coursework. Use the top navigation bar to open each page."
  },
  {
    "objectID": "index.html#quick-links-recent-submissions",
    "href": "index.html#quick-links-recent-submissions",
    "title": "Geospatial Analytics and Applications Coursework",
    "section": "Quick Links – Recent Submissions",
    "text": "Quick Links – Recent Submissions\n(Refer to the drop-down menus above for complete list of assignment submissions)\n\n👉 Take-home Exercise 3: Open the assignment\n👉 In-class Exercise 10: Open the assignment\n👉 In-class Exercise 9: Open the assignment\n👉 Hands-on Exercise 9: Open the assignment\n👉 In-class Exercise 8: Open the assignment"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Geospatial Analytics and Applications Coursework",
    "section": "Objectives:",
    "text": "Objectives:\n\nDemonstrate step-by-step workflows for geospatial tasks that a beginner can follow.\nUse clear, descriptive names and relative file paths so the website renders correctly on Netlify.\nProvide reproducible results with the same structure for every assignment."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html",
    "href": "In-Class_Ex07/in-class_ex07.html",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically Weighted Regression (GWR) allows relationships between predictors (independent variables) and an outcome (dependent variable) to vary by location. In hedonic pricing, we model the resale prices of condominium units using structural factors (e.g., floor area, age) and locational accessibility (e.g., proximity to transport, parks, schools)."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#getting-started",
    "href": "In-Class_Ex07/in-class_ex07.html#getting-started",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "3 Getting Started",
    "text": "3 Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\n\n# Install and load all required packages in one call --------------------------------\n# pacman::p_load() will install any missing packages and then load them into memory\npacman::p_load(olsrr, corrplot, ggpubr,\n               sf, sfdep, GWmodel, tmap,\n               tidyverse, gtsummary,\n               performance, RColorBrewer, see)\n\nThe R packages needed for this exercise are as follows:\n\nsf: spatial vector data handling; projections.\nsfdep: spatial weights and Moran’s I with tidy‑sf interface.\nGWmodel: GWR bandwidth search and model fitting.\ntmap: static/interactive maps.\ntidyverse: wrangling (dplyr, readr, ggplot2).\nolsrr, performance: OLS diagnostics, VIF, assumption checks.\ncorrplot: correlation matrix visual.\ngtsummary: publication‑quality regression tables.\nRColorBrewer: color palettes for maps."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#overview",
    "href": "In-Class_Ex07/in-class_ex07.html#overview",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically Weighted Regression (GWR) allows relationships between predictors (independent variables) and an outcome (dependent variable) to vary by location. In hedonic pricing, we model the resale prices of condominium units using structural factors (e.g., floor area, age) and locational accessibility (e.g., proximity to transport, parks, schools)."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#the-data",
    "href": "In-Class_Ex07/in-class_ex07.html#the-data",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "2 The Data",
    "text": "2 The Data\n\nGeospatial: MP14_SUBZONE_WEB_PL (subzone polygons; SVY21 projection).\nAspatial: Condo_resale_2015.csv with columns such as SELLING_PRICE, AREA_SQM, AGE, and multiple proximity variables (in kilometers) to amenities (MRT, parks, schools, etc.)."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#geospatial-data-wrangling",
    "href": "In-Class_Ex07/in-class_ex07.html#geospatial-data-wrangling",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "4 Geospatial Data Wrangling",
    "text": "4 Geospatial Data Wrangling\n\n4.1 Importing geospatial data\n\n# Read the URA Master Plan 2014 subzone shapefile\n# dsn: directory; layer: shapefile base name without extension\nmpsz = st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial\",\n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n4.2 Updating CRS information to EPSG:3414 (SVY21 meters)\n\n# Transform the coordinate reference system to SVY21 / EPSG:3414\n# This ensures all distance‑based operations use meters (required by GWR bandwidth)\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\n\n# Verify the target CRS\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, inspect layer extent (bounding box)\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\nThe print above reports the extent of mpsz_svy21 layer by its lower and upper limits."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#aspatial-data-wrangling",
    "href": "In-Class_Ex07/in-class_ex07.html#aspatial-data-wrangling",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "5 Aspatial Data Wrangling",
    "text": "5 Aspatial Data Wrangling\n\n5.1 Importing the aspatial data and inspect\n\n# Read the 2015 condo resale dataset as a tibble\ncondo_resale = read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex07/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Peek at structure: variable names, types, first few rows\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE)\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE)\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nThe print above reveals that the values of LONGITITUDE and LATITUDE fields are in decimal degree. Most probably wgs84 geographic coordinate system is used.\n\n# Quick descriptive statistics for all columns\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n5.2 Converting aspatial data frame into a sf object\n\n# Convert LONGITUDE/LATITUDE (WGS84) to POINT geometry and reproject to SVY21\n# 1) st_as_sf(): declare coordinates (lon, lat) with crs=4326 (WGS84 degrees)\n# 2) st_transform(): project to EPSG:3414 so distances are in meters\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\n\n\n\n# Confirm the first few records including geometry\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#exploratory-data-analysis-eda",
    "href": "In-Class_Ex07/in-class_ex07.html#exploratory-data-analysis-eda",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6 Exploratory Data Analysis (EDA)",
    "text": "6 Exploratory Data Analysis (EDA)\n\n6.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\n# Plot raw SELLING_PRICE distribution\n# aes(x=SELLING_PRICE) maps the price variable to the x‑axis for a histogram\nggplot(data = condo_resale.sf,\n       aes(x = `SELLING_PRICE`)) +\n  geom_histogram(bins = 20,           # 20 equal‑width bins\n                 color = \"black\",     # black outline for readability\n                 fill = \"light blue\") # soft fill color for clarity\n\n\n\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n# Create a log‑price to reduce skewness\n# mutate() adds a new variable LOG_SELLING_PRICE = log(SELLING_PRICE)\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, we can plot the LOG_SELLING_PRICE using the code chunk below.\n\n# Plot the log‑transformed price distribution -----------------------------------\nggplot(data = condo_resale.sf,\n       aes(x = `LOG_SELLING_PRICE`)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\")\n\n\n\n\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n6.2 Multiple Histogram Plots distribution of variables\nn this section, we will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n# Build individual histograms for key predictors ---------------------------------\nAREA_SQM &lt;- ggplot(data = condo_resale.sf, aes(x = `AREA_SQM`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nAGE &lt;- ggplot(data = condo_resale.sf, aes(x = `AGE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CBD &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CBD`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_CHILDCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_MRT &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_MRT`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PARK &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PARK`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data = condo_resale.sf, aes(x = `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"light blue\")\n\n# Arrange the 12 histograms into a 3x4 panel ------------------------------------\n# ggarrange() helps create small‑multiples (trellis) display\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE,\n          PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA,\n          PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)"
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#hedonic-pricing-modelling-in-r",
    "href": "In-Class_Ex07/in-class_ex07.html#hedonic-pricing-modelling-in-r",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "7 Hedonic Pricing Modelling in R",
    "text": "7 Hedonic Pricing Modelling in R\nIn this section, we will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n7.1 Simple Linear Regression (SLR): SELLING_PRICE ~ AREA_SQM\n\n# Fit a simple linear regression with floor area as the only predictor ------------\ncondo.slr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM,\n                data = condo_resale.sf)\n\n\n# Print model summary: coefficients, R², p‑values, residual spread ----------------\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\\text{Selling_Price} = -258121.1 + 14719\\cdot\\text{Area_SQM}\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n# Visualize scatter with best‑fit line from lm() ---------------------------------\n# geom_smooth(method = lm) overlays the OLS regression line with CI ribbon\nggplot(data = condo_resale.sf,  \n       aes(x = `AREA_SQM`, y = `SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n7.2 Multiple Linear Regression Method\n\n7.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n# Visualize pairwise correlations among predictors (cols 5:23 from the CSV) ------\ncorrplot(cor(condo_resale[, 5:23]),\n         diag = FALSE,\n         order = \"AOE\",      # Angular Order of Eigenvectors (stable ordering)\n         tl.pos = \"td\",      # text labels on top diagonal\n         tl.cex = 0.5,        # smaller text\n         method = \"number\",  # print numeric correlations\n         type = \"upper\")     # upper triangle only\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n7.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n# Build the full hedonic model (drop LEASEHOLD_99YR to avoid high correlation) ---\ncondo.mlr &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET   +\n    PROX_KINDERGARTEN   + PROX_MRT  + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH +\n    PROX_SHOPPING_MALL  + PROX_SUPERMARKET + \n    PROX_BUS_STOP   + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, \n  data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk above consists of two parts:\n- lm() of Base R is used to calibrate a multiple linear regression model. The model output is stored in an lm object called condo.mlr.\n- summary() is used to print the model output.\n\n\n\n\n7.4 Revising the model by removing non-significant predictors\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n# Remove variables with weak significance to improve parsimony -------------------\ncondo.mlr1 &lt;- lm(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE + \n    PROX_CBD + PROX_CHILDCARE + PROX_MRT +\n    PROX_ELDERLYCARE    + PROX_URA_GROWTH_AREA +\n    PROX_PARK   + PROX_PRIMARY_SCH + \n    PROX_SHOPPING_MALL  + PROX_BUS_STOP + \n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf)\n\n\n# Verify all retained predictors are significant at 5% (or better) ---------------\nsummary(condo.mlr1)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_MRT + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_PARK + \n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n    FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\nAREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\nAGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\nPROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\nPROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \nPROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\nPROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\nPROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\nPROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \nPROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\nPROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\nNO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \nFAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \nFREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 756000 on 1421 degrees of freedom\nMultiple R-squared:  0.6507,    Adjusted R-squared:  0.6472 \nF-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16\n\n\nThe output above reveals that all explanatory variables are statistically significant at 95% confident level.\n\n\n7.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n# Create a clean table of coefficients, CIs, and p‑values\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n# Append model‑level statistics as a footnote (AIC, R², sigma, etc.) -------------\ntbl_regression(condo.mlr1,\nintercept = TRUE) %&gt;%\nadd_glance_source_note(\nlabel = list(sigma ~ \"σ\"), # Greek sigma symbol\ninclude = c(r.squared, adj.r.squared,\nAIC, statistic,\np.value, sigma))\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\nR² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n\n\n\n\n\n\n\n\nFor more customization options, refer to Tutorial: tbl_regression.\n\n\n7.6 Regression Diagnostics\nRegression diagnostics are a set of procedures used to check if a regression model’s assumptions are met and how well the model fits the data. These diagnostics involve checking for issues like non-linear relationships, non-normal errors, non-constant variance, and influential observations to ensure the model’s conclusions are valid and reliable. Common methods include graphical analysis, like residual plots and QQ-plots, and quantitative tests\nIn this section, we would like to introduce a fantastic R package specially programmed for performing OLS regression diagnostics. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\n\nresidual diagnostics\n\nmeasures of influence\n\nheteroskedasticity tests\n\ncollinearity diagnostics\n\nmodel fit assessment\n\nvariable contribution assessment\n\nvariable selection procedures\n\n\n7.6.1 Multicollinearity test\nMulticollinearity occurs when independent variables are not truly independent, meaning a change in one is associated with a change in another. This makes it hard for the model to isolate each variable’s influence on the outcome.\nPerforming a multicollinearity test is crucial in multiple linear regression because it ensures the reliability and interpretability of the model’s results. High multicollinearity, where independent variables are highly correlated, inflates the variance of the estimated coefficients, making them unstable, unreliable, and difficult to interpret. This instability can lead to misleading statistical conclusions, such as a variable appearing statistically insignificant when it is not.\nIn the code chunk below, the check_collinearity() of performance package is used to test if there are sign of multicollinearity.\n\n# Check Variance Inflation Factors (VIF) to confirm low multicollinearity --------\nmlr.vif &lt;- check_collinearity(condo.mlr1) # compute VIFs\nmlr.vif # print the table\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                 Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n             AREA_SQM 1.15 [1.09, 1.23]     1.07      0.87     [0.81, 0.92]\n                  AGE 1.41 [1.33, 1.52]     1.19      0.71     [0.66, 0.75]\n             PROX_CBD 1.57 [1.47, 1.69]     1.25      0.64     [0.59, 0.68]\n       PROX_CHILDCARE 3.26 [3.00, 3.56]     1.81      0.31     [0.28, 0.33]\n             PROX_MRT 1.91 [1.78, 2.07]     1.38      0.52     [0.48, 0.56]\n     PROX_ELDERLYCARE 1.52 [1.42, 1.63]     1.23      0.66     [0.61, 0.70]\n PROX_URA_GROWTH_AREA 1.33 [1.26, 1.43]     1.15      0.75     [0.70, 0.80]\n            PROX_PARK 1.21 [1.15, 1.29]     1.10      0.83     [0.77, 0.87]\n     PROX_PRIMARY_SCH 2.21 [2.05, 2.40]     1.49      0.45     [0.42, 0.49]\n   PROX_SHOPPING_MALL 1.48 [1.39, 1.60]     1.22      0.67     [0.63, 0.72]\n        PROX_BUS_STOP 2.85 [2.62, 3.10]     1.69      0.35     [0.32, 0.38]\n          NO_Of_UNITS 1.45 [1.36, 1.56]     1.20      0.69     [0.64, 0.73]\n      FAMILY_FRIENDLY 1.38 [1.30, 1.48]     1.17      0.72     [0.67, 0.77]\n             FREEHOLD 1.44 [1.36, 1.55]     1.20      0.69     [0.65, 0.74]\n\nplot(mlr.vif) # quick visual of VIF levels\n\n\n\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n7.6.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n#check_model(condo.mlr1, check = \"linearity\")\n\n# Visual check that residuals vs fitted show no strong non‑linearity -------------\nggplot(data = data.frame(Fitted = fitted(condo.mlr1), Residuals = resid(condo.mlr1)),\naes(x = Fitted, y = Residuals)) +\ngeom_point(color = 'blue', alpha = 0.6) +\ngeom_smooth(method = 'loess', se = TRUE, color = 'green', fill = 'grey70') +\ngeom_hline(yintercept = 0, color = 'black', linetype = 'dashed') +\nlabs(title = 'Linearity', subtitle = 'Reference line should be flat and horizontal',\nx = 'Fitted values', y = 'Residuals') +\ntheme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n7.6.3 Test for Normality Assumption\nThe normality assumption test for multiple linear regression checks if the model’s residuals (the differences between observed and predicted values) are normally distributed. This is crucial for accurate hypothesis testing and confidence intervals. To test this, we can use visual methods like histograms and Q-Q plots of the residuals, or conduct statistical tests like Shapiro-Wilk test and Kolmogorov-Smirnov test.\nIn the code chunk below, check_normality() of performance package is used to perform normality assumption test on condo.mlr1 model.\n\n# Formal test (often significant with large n); complement with Q‑Q plot ---------\ncheck_normality(condo.mlr1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nThe print above reveals that the p-value of the normality assumption test is less than alpha value of 0.05. Hence we reject the normality assumption at 95% confident level.\n\n\n\n\n\n\nNote\n\n\n\n\ncheck_normality() calls stats::shapiro.test and checks the standardized residuals (or studentized residuals for mixed models) for normal distribution.\n\nNote that this formal test almost always yields significant results for the distribution of residuals and visual inspection (e.g. Q-Q plots) are preferable.\n\n\n\nInstead of showing the test statistic, plot() of see package can be used to plot a the output of check_normality() for visual inspection as shown below.\n\n# Q‑Q plot of standardized residuals from the check_normality() output -----------\nplot(check_normality(condo.mlr1), type = \"qq\")\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\nQ-Q plot above below shows that majority of the data points are felt along the zero line.\nAnother way to check for normality assumption visual is by using check_model() of performance package as shown in the code chunk below.\n\n# Alternative normality panel via performance::check_model -----------------------\ncheck_model(condo.mlr1, check = \"normality\")\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\n\n\n7.6.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced data. Hence, it is crucial to check for spatial autocorrelation because its presence can produce unreliable and misleading results. Traditional regression models, such as ordinary least squares (OLS), assume that observations are independent of one another. However, spatial data often violates this assumption.\nSpatial autocorrelation is the correlation of a variable with itself across different spatial locations. Positive spatial autocorrelation means nearby features tend to be more similar, while negative autocorrelation means they tend to be more dissimilar. This phenomenon is based on the first law of geography: “Everything is related to everything else, but nearby things are more related than distant things”.\nIgnoring spatial autocorrelation in a regression model can lead to serious statistical issues:\n\nBiased and inefficient coefficient estimates: If autocorrelation is present, standard errors of the model coefficients can be wrong, leading to unreliable hypothesis tests (p-values). The model might appear more significant than it is.\nMisleading significance tests: Standard regression models cannot distinguish between true explanatory power and the influence of spatial patterns, resulting in inaccurate p-values.\nModel misspecification: Significant spatial autocorrelation in the regression residuals often signals that important explanatory variables are missing from the model. The spatial patterning of the residuals (over- and under-predictions) can provide clues about what these missing variables might be.\nInflated Type I error rates: Researchers might incorrectly reject a true null.\n\nTo test for spatial autocorrelation, We can run a Moran’s I test on the model’s residuals. Significant spatial autocorrelation in the residuals means the model is not capturing the full spatial story.\nIn order to perform spatial autocorrelation test, we need to export the residual of the hedonic pricing model and save it as a data frame first.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\n# Extract residuals into the sf layer so we can map and test them -------------\ncondo_resale.sf &lt;- cbind(condo_resale.sf,\ncondo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`) # rename residual column\n\nNext, we will use tmap package to display the distribution of the residuals on a static map.\nThe code chunks below is used to create a static point symbol map.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(mpsz_svy21) +\ntm_polygons(fill_alpha = 0.4) + # semi‑transparent base\ntm_shape(condo_resale.sf) +\ntm_dots(\nfill = \"MLR_RES\", # color by residual value\nsize = 0.7, # point size\ncol = \"black\", # thin border\nfill.scale = tm_scale( # custom diverging palette\nn = 10,\nvalues = rev(brewer.pal(11, \"RdBu\")), # red‑blue diverging\nstyle = \"quantile\",\nmidpoint = NA),\nfill.legend = tm_legend(title = \"Residuals\")\n) +\ntm_title(\"LM Residuals (Quantile Classification)\") +\ntm_layout(legend.outside = TRUE) +\ntm_view(set_zoom_limits = c(11,14))\n\n\n\n\n\n\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, Global Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using st_dist_band() function of sfdep.\n\n# Build distance‑band neighbors and row‑standardized weights ------------------\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(\n    nb = st_dist_band(st_geometry(geometry), upper = 1500), # neighbors &lt;= 1.5 km\n    wt = st_weights(nb, style = \"W\"), # row‑standardized W\n    .before = 1)\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `nb = st_dist_band(st_geometry(geometry), upper = 1500)`.\nCaused by warning in `spdep::dnearneigh()`:\n! neighbour object has 10 sub-graphs\n\n\nNext, global_moran_perm() of sfdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\n# Permutation Moran’s I test on residuals -------------------------------------\nset.seed(1234) # for reproducibility of the permutation p‑value\nglobal_moran_perm(\n  condo_resale.sf$MLR_RES,\n  nb = condo_resale.sf$nb,\n  wt = condo_resale.sf$wt,\n  alternative = \"two.sided\",\n  nsim = 499)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.14389, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.14389 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "In-Class_Ex07/in-class_ex07.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "8 Building Hedonic Pricing Models using GWmodel",
    "text": "8 Building Hedonic Pricing Models using GWmodel\nIn this section, we are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n8.1 Building Fixed Bandwidth GWR Model\n\n8.1.1 Computing fixed bandwith using CV and AIC approach\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument **adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach agreement.\n\n8.1.1.1 Fixed bandwidth computation via CV\n\n# Search for the optimal fixed bandwidth (in meters) using CV\nbw.fixed_CV &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\", # cross‑validation criterion\nkernel = \"gaussian\", # Gaussian kernel\nadaptive = FALSE, # fixed (distance) bandwidth\nlonglat = FALSE) # coordinates are projected (meters)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\n\n\n8.1.1.2 Fixed bandwidth computation via AIC\n\n# Search for the optimal fixed bandwidth (in meters) using AIC\nbw.fixed_AIC &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\napproach = \"AIC\", # AIC criterion\nkernel = \"gaussian\", # Gaussian kernel\nadaptive = FALSE, # fixed (distance) bandwidth\nlonglat = FALSE) # coordinates are projected (meters)\n\nFixed bandwidth: 17660.96 AICc value: 42937.77 \nFixed bandwidth: 10917.26 AICc value: 42886.26 \nFixed bandwidth: 6749.419 AICc value: 42757.54 \nFixed bandwidth: 4173.553 AICc value: 42565.01 \nFixed bandwidth: 2581.58 AICc value: 42368.05 \nFixed bandwidth: 1597.687 AICc value: 42256.42 \nFixed bandwidth: 989.6077 AICc value: 42262.3 \nFixed bandwidth: 1973.501 AICc value: 42279.95 \nFixed bandwidth: 1365.421 AICc value: 42254.72 \nFixed bandwidth: 1221.873 AICc value: 42255.41 \nFixed bandwidth: 1454.139 AICc value: 42254.82 \nFixed bandwidth: 1310.591 AICc value: 42254.84 \nFixed bandwidth: 1399.309 AICc value: 42254.71 \nFixed bandwidth: 1420.252 AICc value: 42254.73 \nFixed bandwidth: 1386.365 AICc value: 42254.7 \nFixed bandwidth: 1378.365 AICc value: 42254.71 \nFixed bandwidth: 1391.309 AICc value: 42254.7 \n\n\n\n\n\n8.1.2 GWModel method - fixed-bandwith (CV and AIC; K nearest neighbors)\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n8.1.2.1 Calibrate fixed-bandwidth using CV\n\n# Calibrate the fixed‑bandwidth GWR model ---------------------------------------\ngwr.fixed_CV &lt;- gwr.basic(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\nbw = bw.fixed_CV,\nkernel = 'gaussian',\nlonglat = FALSE)\n\n\n# Inspect the fixed GWR diagnostics (AICc, R², parameter summaries) --------------\ngwr.fixed_CV\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-18 22:50:14.593355 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.fixed_CV, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2025-10-18 22:50:15.74219 \n\n\n\n\n8.1.2.2 Calibrate fixed-bandwidth using AIC\n\n# Calibrate the fixed‑bandwidth GWR model ---------------------------------------\ngwr.fixed_AIC &lt;- gwr.basic(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY +\nFREEHOLD,\ndata = condo_resale.sf,\nbw = bw.fixed_AIC,\nkernel = 'gaussian',\nlonglat = FALSE)\n\n\n# Inspect the fixed GWR diagnostics (AICc, R², parameter summaries) --------------\ngwr.fixed_AIC\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-18 22:50:15.756869 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.fixed_AIC, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 1386.365 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -5.0947e+06  7.5228e+04  9.6933e+05  1.6560e+06\n   AREA_SQM              2.5558e+03  5.4041e+03  7.6995e+03  1.2569e+04\n   AGE                  -8.4563e+04 -2.1045e+04 -1.1083e+04 -5.3192e+03\n   PROX_CBD             -3.0121e+06 -2.2299e+05 -1.0207e+05 -5.3900e+04\n   PROX_CHILDCARE       -3.6679e+06 -1.7018e+05 -1.6108e+04  2.4420e+05\n   PROX_ELDERLYCARE     -4.9526e+05 -4.0047e+04  7.6715e+04  1.9650e+05\n   PROX_URA_GROWTH_AREA -3.0350e+05 -3.0201e+04  5.1598e+04  1.5573e+05\n   PROX_MRT             -2.6892e+06 -3.9441e+05 -2.3485e+05 -1.3171e+05\n   PROX_PARK            -9.1626e+05 -1.2375e+05  4.7999e+04  4.0312e+05\n   PROX_PRIMARY_SCH     -4.7607e+05 -1.2189e+05  2.9963e+04  3.0890e+05\n   PROX_SHOPPING_MALL   -9.7562e+05 -1.2594e+05 -1.2552e+04  1.0019e+05\n   PROX_BUS_STOP        -8.4504e+05  8.8484e+04  3.7699e+05  1.4980e+06\n   NO_Of_UNITS          -1.2111e+03 -3.4940e+02 -5.8441e+01  1.0022e+02\n   FAMILY_FRIENDLY      -1.3608e+06 -5.1541e+04  1.0951e+04  1.5911e+05\n   FREEHOLD             -1.1559e+05  9.5013e+04  2.0430e+05  3.5295e+05\n                             Max.\n   Intercept            4639265.4\n   AREA_SQM               19642.4\n   AGE                    41275.9\n   PROX_CBD              272116.0\n   PROX_CHILDCARE       1776212.0\n   PROX_ELDERLYCARE     2742988.5\n   PROX_URA_GROWTH_AREA 3038754.5\n   PROX_MRT             1349867.9\n   PROX_PARK            1102849.3\n   PROX_PRIMARY_SCH     2148894.9\n   PROX_SHOPPING_MALL    668261.7\n   PROX_BUS_STOP        4267600.2\n   NO_Of_UNITS             2174.3\n   FAMILY_FRIENDLY      1289044.9\n   FREEHOLD             1001304.5\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 302.5667 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1133.433 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42254.7 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41902.94 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42011.36 \n   Residual sum of squares: 3.334499e+14 \n   R-square value:  0.8565589 \n   Adjusted R-square value:  0.8182339 \n\n   ***********************************************************************\n   Program stops at: 2025-10-18 22:50:16.656112 \n\n\n\n\n\n8.1.3 Insights into fixed bandwidth performance under CV and AIC approaches\nThe two fixed-bandwidth GWR runs differ in a meaningful way this time. The cross-validated model, gwr.fixed_CV, selects a fixed kernel width of about 971 m and yields AICc ≈ 42,263.6, R² ≈ 0.8099 (adjusted ≈ 0.8430), an effective parameter count of about 438.4, and residual sum of squares around 2.534×10¹⁴. The AICc-tuned model, gwr.fixed_AIC, chooses a larger fixed bandwidth of about 1,386 m, with AICc ≈ 42,254.7, R² ≈ 0.8566 (adjusted ≈ 0.8182), an effective parameter count of about 302.6, and residual sum of squares reported near 3.335×10¹⁴. In practical terms, AICc prefers a smoother spatial surface: a wider bandwidth pools information from more neighbours, which reduces local variance and shrinks the effective complexity by roughly one-third (≈303 vs ≈438 parameters). That parsimony is exactly what AICc rewards, and it explains why the AICc model attains the lower information criterion despite being less “wiggly.” The higher R² from the AICc fit indicates that, at this broader spatial scale, the model explains more variation in prices overall; the CV model’s smaller bandwidth is better at capturing very local idiosyncrasies but at the cost of higher model complexity and potentially noisier local coefficients.\nSubstantively, the AICc model is preferable if our goal is a defensible, generalizable explanation of price drivers with restrained local volatility and reduced risk of overfitting or local multicollinearity. The CV model is preferable if our priority is detecting fine-grained neighbourhood effects and sharp spatial gradients, accepting a more complex surface. Before deciding, inspect side-by-side maps of local coefficients and t-values, compare the stability of signs across space, and check residual spatial autocorrelation. If small-area policy targeting or micro-market storytelling is central, keep the CV bandwidth; if we need a succinct, reliable narrative for the whole city, the AICc bandwidth is the stronger choice.\n\n\n\n8.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n8.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\n\n8.2.1.1 Adaptive bandwidth computation via CV\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n# Search for the optimal adaptive bandwidth (K neighbors) using CV\nbw.adaptive_CV &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS +\nFAMILY_FRIENDLY + FREEHOLD,\ndata = condo_resale.sf,\napproach = \"CV\", \nkernel = \"gaussian\",\nadaptive = TRUE, # K‑NN style bandwidth\nlonglat = FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n8.2.1.2 Adaptive bandwidth computation via AIC\n\n# Search for the optimal adaptive bandwidth (K neighbors) using AIC\nbw.adaptive_AIC &lt;- bw.gwr(\nformula = SELLING_PRICE ~ AREA_SQM + AGE +\nPROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\nPROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\nPROX_PRIMARY_SCH + PROX_SHOPPING_MALL +\nPROX_BUS_STOP + NO_Of_UNITS +\nFAMILY_FRIENDLY + FREEHOLD,\ndata = condo_resale.sf,\napproach = \"AIC\", \nkernel = \"gaussian\",\nadaptive = TRUE, # K‑NN style bandwidth\nlonglat = FALSE)\n\nAdaptive bandwidth (number of nearest neighbours): 895 AICc value: 42879.45 \nAdaptive bandwidth (number of nearest neighbours): 561 AICc value: 42823.52 \nAdaptive bandwidth (number of nearest neighbours): 354 AICc value: 42677.14 \nAdaptive bandwidth (number of nearest neighbours): 226 AICc value: 42484.37 \nAdaptive bandwidth (number of nearest neighbours): 147 AICc value: 42353.27 \nAdaptive bandwidth (number of nearest neighbours): 98 AICc value: 42280.68 \nAdaptive bandwidth (number of nearest neighbours): 68 AICc value: 42201.18 \nAdaptive bandwidth (number of nearest neighbours): 49 AICc value: 42100.66 \nAdaptive bandwidth (number of nearest neighbours): 37 AICc value: 42055.68 \nAdaptive bandwidth (number of nearest neighbours): 30 AICc value: 41982.22 \nAdaptive bandwidth (number of nearest neighbours): 25 AICc value: 42003.33 \nAdaptive bandwidth (number of nearest neighbours): 32 AICc value: 42035.41 \nAdaptive bandwidth (number of nearest neighbours): 27 AICc value: 41978.63 \nAdaptive bandwidth (number of nearest neighbours): 27 AICc value: 41978.63 \n\n\n\n\n\n8.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n8.2.2.1 Calibrate adaptive bandwidth using CV\n\n# Calibrate the adaptive‑bandwidth GWR model ------------------------------------\ngwr.adaptive_CV &lt;- gwr.basic(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE +\n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf,\n  bw = bw.adaptive_CV,\n  kernel = 'gaussian',\n  adaptive = TRUE, # activate adaptive bandwidth in the fit\n  longlat = FALSE)\n\n# Inspect the adaptive GWR diagnostics (AICc, R²) --------------------------------\ngwr.adaptive_CV\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-18 22:50:36.424464 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.adaptive_CV, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2025-10-18 22:50:37.649116 \n\n\n\n\n8.2.2.2 Calibrate adaptive bandwidth using AIC\n\n# Calibrate the adaptive‑bandwidth GWR model ------------------------------------\ngwr.adaptive_AIC &lt;- gwr.basic(\n  formula = SELLING_PRICE ~ AREA_SQM + AGE +\n    PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n    PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK +\n    PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n    NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n  data = condo_resale.sf,\n  bw = bw.adaptive_AIC,\n  kernel = 'gaussian',\n  adaptive = TRUE, # activate adaptive bandwidth in the fit\n  longlat = FALSE)\n\n# Inspect the adaptive GWR diagnostics (AICc, R²) --------------------------------\ngwr.adaptive_AIC\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-18 22:50:37.661237 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw.adaptive_AIC, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 27 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.4055e+08 -4.9698e+05  7.6667e+05  1.6291e+06\n   AREA_SQM              3.2900e+03  5.5381e+03  7.6847e+03  1.2674e+04\n   AGE                  -1.0016e+05 -2.7556e+04 -1.3592e+04 -4.6598e+03\n   PROX_CBD             -4.0881e+06 -1.9515e+05 -7.3518e+04  2.9526e+04\n   PROX_CHILDCARE       -1.7059e+06 -2.6967e+05 -1.2514e+04  3.4777e+05\n   PROX_ELDERLYCARE     -2.2412e+06 -9.4985e+04  7.9372e+04  3.2765e+05\n   PROX_URA_GROWTH_AREA -1.8375e+07 -2.3765e+04  5.7504e+04  2.9631e+05\n   PROX_MRT             -4.5351e+07 -6.2318e+05 -2.0888e+05 -3.8964e+04\n   PROX_PARK            -1.0166e+08 -2.0084e+05  1.1152e+05  4.4450e+05\n   PROX_PRIMARY_SCH     -4.6505e+06 -1.9579e+05 -9.1754e+03  4.3824e+05\n   PROX_SHOPPING_MALL   -2.4269e+06 -1.4053e+05 -1.3684e+04  1.5823e+05\n   PROX_BUS_STOP        -2.1436e+06 -8.7705e+04  4.1820e+05  1.2784e+06\n   NO_Of_UNITS          -3.2993e+03 -2.2606e+02 -2.1790e+01  1.4231e+02\n   FAMILY_FRIENDLY      -4.0477e+06 -6.5054e+04  2.2989e+04  1.9452e+05\n   FREEHOLD             -1.0164e+08  2.4578e+04  1.8212e+05  3.8182e+05\n                              Max.\n   Intercept            55107367.7\n   AREA_SQM                23242.4\n   AGE                    265480.4\n   PROX_CBD             26220365.3\n   PROX_CHILDCARE        3565627.3\n   PROX_ELDERLYCARE     93434430.7\n   PROX_URA_GROWTH_AREA 25096283.7\n   PROX_MRT              9223676.8\n   PROX_PARK             2754387.0\n   PROX_PRIMARY_SCH      4618809.5\n   PROX_SHOPPING_MALL   39514834.8\n   PROX_BUS_STOP        13319341.1\n   NO_Of_UNITS              9878.1\n   FAMILY_FRIENDLY       2315112.2\n   FREEHOLD              1876281.2\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 393.0807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1042.919 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41978.63 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41453.88 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42070.13 \n   Residual sum of squares: 2.305296e+14 \n   R-square value:  0.9008324 \n   Adjusted R-square value:  0.8634199 \n\n   ***********************************************************************\n   Program stops at: 2025-10-18 22:50:38.659062 \n\n\n\n\n\n8.2.3 Insights into adaptive bandwidth performance under CV and AIC approaches\nOur two adaptive GWR calibrations use the same data, kernel and metric but settle on different neighbour counts. gwr.adaptive_CV chooses 30 neighbours (CV = prediction-error minimization), whereas gwr.adaptive_AIC prefers a narrower window of 27 neighbours (AICc = fit–parsimony trade-off). The smaller window lets coefficients vary more sharply across space.\nThat choice shows up in the diagnostics. The AICc model is more flexible (effective parameters ≈ 393.1 vs 350.3; effective d.f. 1042.9 vs 1085.7, which is lower because the model is more complex). With that extra flexibility it achieves lower residual SS (2.305×10¹⁴ vs 2.528×10¹⁴), higher R² (0.9008 vs 0.8912, adjusted 0.8634 vs 0.8561), and a slightly better AICc (41,979 vs 41,982). In short: AICc is telling us the data support a somewhat finer spatial scale of non-stationarity than CV was willing to choose.\nHow to read this substantively: the AICc model will draw sharper local contrasts in the effects of the predictors (e.g., accessibility and amenities), which can be useful for micro-market interpretation and targeting. The CV model is slightly smoother, prioritizing out-of-sample error control; its surfaces will be a bit less volatile and may generalize more conservatively.\nWhich should we use? If our goal is inference with crisp local detail and we are comfortable managing higher complexity, the adaptive_AIC (27-NN) fit is preferable given its lower AICc and higher R². If our priority is predictive robustness, keep adaptive_CV (30-NN) unless a held-out test confirms that the 27-NN model does at least as well on unseen data."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#converting-sdf-into-sf-data.frame",
    "href": "In-Class_Ex07/in-class_ex07.html#converting-sdf-into-sf-data.frame",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "9 Converting SDF into sf data.frame",
    "text": "9 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n# Convert the GWR output SDF (Spatial*DataFrame) to sf for mapping ---------------\ncondo_resale.sf.adaptive &lt;-\nst_as_sf(gwr.adaptive_AIC$SDF) %&gt;%\nst_transform(crs = 3414) # ensure consistent projection for mapping\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\n# Inspect the fields (coefficients, SE, t‑values, Local_R2, fitted yhat, etc.) ---\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2044700.26, 1773153.15, 3520072.36, 1539855.35…\n$ AREA_SQM                &lt;dbl&gt; 9559.598, 15390.155, 13016.013, 21340.305, 678…\n$ AGE                     &lt;dbl&gt; -9500.111, -48975.055, -25897.022, -97109.448,…\n$ PROX_CBD                &lt;dbl&gt; -120265.31, -173034.60, -266084.74, 4535464.61…\n$ PROX_CHILDCARE          &lt;dbl&gt; 318045.039, 348815.035, -194366.004, 286266.93…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -394643.95, 238275.45, 558021.92, 402763.72, -…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159575.59, -31187.00, -253591.14, -4781655.68…\n$ PROX_MRT                &lt;dbl&gt; -299592.54, -2557296.15, -942766.83, -2495885.…\n$ PROX_PARK               &lt;dbl&gt; -172380.89, 442318.02, 246972.65, -937538.48, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242732.037, 919399.390, 558785.123, 3340638.60…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 302051.162, -215149.309, -142022.712, 134573.9…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1209536.14, 1919405.53, 1450141.54, 8743551.68…\n$ NO_Of_UNITS             &lt;dbl&gt; 106.372669, -211.071177, 19.339246, -150.59531…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9098.124, 152642.975, -29284.824, 1723147.674…\n$ FREEHOLD                &lt;dbl&gt; 303927.63, 400568.89, 162972.29, 1318502.10, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2885980.3, 3464956.3, 3621402.8, 5528248.5, 13…\n$ residual                &lt;dbl&gt; 114019.686, 415043.736, -296402.814, -1278248.…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.39812858, 1.12993822, -0.89467945, -3.237264…\n$ Intercept_SE            &lt;dbl&gt; 505352.2, 959583.4, 1088656.5, 644451.8, 21882…\n$ AREA_SQM_SE             &lt;dbl&gt; 803.9810, 1041.2806, 1005.1322, 647.0447, 1408…\n$ AGE_SE                  &lt;dbl&gt; 5754.545, 7873.214, 6772.090, 6339.353, 8401.1…\n$ PROX_CBD_SE             &lt;dbl&gt; 36644.67, 46892.76, 64224.98, 723130.45, 42870…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 311604.5, 383548.3, 356362.3, 318029.0, 726053…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 117822.10, 121329.51, 145658.37, 153818.83, 36…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 54991.59, 138590.35, 104509.67, 714454.45, 501…\n$ PROX_MRT_SE             &lt;dbl&gt; 181049.7, 426727.9, 290345.8, 310229.2, 399736…\n$ PROX_PARK_SE            &lt;dbl&gt; 200556.4, 337273.4, 328846.1, 250084.3, 406910…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 149035.4, 213215.4, 206844.1, 276593.8, 253671…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 106789.05, 168563.26, 124684.33, 237237.76, 33…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 587417.1, 527904.1, 468907.1, 625949.7, 748596…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 213.2704, 232.9495, 211.9892, 452.1575, 315.82…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 128381.97, 139756.71, 152249.80, 115274.06, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 113234.6, 170426.6, 146100.1, 149874.1, 215886…\n$ Intercept_TV            &lt;dbl&gt; 4.04608981, 1.84783643, 3.23340951, 2.38940359…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.890328, 14.780026, 12.949553, 32.981191, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6508882, -6.2204656, -3.8240810, -15.318510…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.28193222, -3.69000648, -4.14301022, 6.27198…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.020669014, 0.909442298, -0.545416851, 0.9001…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.34948996, 1.96387060, 3.83103226, 2.6184291…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.90181817, -0.22503007, -2.42648497, -6.6927…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.65475280, -5.99280279, -3.24704810, -8.0452…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.8595132, 1.3114524, 0.7510281, -3.7488897, …\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.62868734, 4.31206879, 2.70147945, 12.0777779…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.82848439, -1.27637129, -1.13905821, 0.567253…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0590756, 3.6358978, 3.0925989, 13.9684567, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.49876913, -0.90608143, 0.09122749, -0.333059…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.07086762, 1.09220500, -0.19234721, 14.94826…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.68405282, 2.35038940, 1.11548393, 8.79739845…\n$ Local_R2                &lt;dbl&gt; 0.9172836, 0.9154542, 0.9132622, 0.9192190, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\n# Quick summary of fitted values (yhat) ------------------------------------------\nsummary(gwr.adaptive_AIC$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  392814  1098774  1383270  1751256  1985895 13892256"
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#selecting-tampines-planning-areas",
    "href": "In-Class_Ex07/in-class_ex07.html#selecting-tampines-planning-areas",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10 Selecting Tampines planning areas",
    "text": "10 Selecting Tampines planning areas\n\n# --- 1) Select the Tampines planning area polygon ------------------------------\n# (Common MP14 fields are PLN_AREA_N (planning area) and SUBZONE_N (subzone).)\ntampines_pa &lt;- mpsz_svy21 %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\n\n# --- 2) Keep only condo points that fall inside Tampines -----------------------\n# Our GWR results were attached to `condo_resale.sf.adaptive` and include `Local_R2`.\ntampines_pts &lt;- condo_resale.sf.adaptive[tampines_pa, , op = st_within]\n\n# --- 3) Visualise: Tampines boundary + points coloured by Local R² -------------\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(tampines_pa) +\n  tm_polygons(fill_alpha = 0.1) +\ntm_shape(tampines_pts) +\n  tm_dots(\n    fill = \"Local_R2\",           # tmap v4: 'fill' controls symbol fill colour\n    col = \"grey30\",              # thin outline\n    size = 0.6,\n    fill.scale = tm_scale(\n      n = 7,\n      style = \"quantile\"         # consistent with the tutorial’s quantile breaks\n    ),\n    fill.legend = tm_legend(title = \"Local R²\")\n  ) +\n  tm_view(set_zoom_limits = c(11, 14))\n\n\n\n\n\n\n\n# --- 4) Summarise Local R² for Tampines ----------------------------------------\ntampines_r2_summary &lt;- tampines_pts %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    n      = dplyr::n(),\n    r2_min = min(Local_R2, na.rm = TRUE),\n    r2_q1  = quantile(Local_R2, 0.25, na.rm = TRUE),\n    r2_med = median(Local_R2, na.rm = TRUE),\n    r2_mean= mean(Local_R2, na.rm = TRUE),\n    r2_q3  = quantile(Local_R2, 0.75, na.rm = TRUE),\n    r2_max = max(Local_R2, na.rm = TRUE)\n  )\n\ntampines_r2_summary\n\n   n    r2_min     r2_q1    r2_med   r2_mean     r2_q3    r2_max\n1 76 0.8944806 0.9550013 0.9652041 0.9546181 0.9710822 0.9724414\n\n\n\n10.1 Insights and intepretation:\nThe adaptive GWR results for Tampines show an exceptionally strong and spatially consistent model fit across condominium resale points. With n = 76, the Local R² ranges from 0.894 to 0.972, a median of 0.965, and a mean of 0.955, indicating that the model explains over 95% of local price variation for most locations. Such high and tightly clustered R² values suggest the explanatory variables—floor area, age, proximity to amenities, accessibility, and tenure—collectively provide a robust representation of spatial price dynamics in Tampines.\nSpatially, the map reveals that the dark blue clusters with higher Local R² values are concentrated around Tampines Central, where key amenities such as the MRT interchange, bus interchange, and shopping malls are located. These areas exhibit highly predictable housing prices because accessibility and urban conveniences strongly align with price determinants. Hence, the model performs best in these well-connected and amenity-rich subzones.\nIn contrast, lighter blue areas along the western and peripheral edges display slightly lower R² values (around 0.89–0.95). This suggests that local housing prices there are influenced by factors not fully captured by the model, such as road noise, micro-neighbourhood effects, or smaller sample density. Nevertheless, these lower values still represent a high level of explanatory power.\nOverall, the GWR model is highly reliable for Tampines, with particularly strong performance near the town centre. Future refinement could include adding micro-accessibility or environmental variables to further improve local accuracy in edge areas."
  },
  {
    "objectID": "In-Class_Ex07/in-class_ex07.html#subset-the-dataset-to-features-within-the-tampines-planning-are",
    "href": "In-Class_Ex07/in-class_ex07.html#subset-the-dataset-to-features-within-the-tampines-planning-are",
    "title": "In-class_Ex07: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10 Subset the dataset to features within the Tampines Planning Are",
    "text": "10 Subset the dataset to features within the Tampines Planning Are\n\n# --- 1) Select the Tampines planning area polygon ------------------------------\n# (Common MP14 fields are PLN_AREA_N (planning area) and SUBZONE_N (subzone).)\ntampines_pa &lt;- mpsz_svy21 %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\n\n# --- 2) Keep only condo points that fall inside Tampines -----------------------\n# Our GWR results were attached to `condo_resale.sf.adaptive` and include `Local_R2`.\ntampines_pts &lt;- condo_resale.sf.adaptive[tampines_pa, , op = st_within]\n\n# --- 3) Visualise: Tampines boundary + points coloured by Local R² -------------\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(tampines_pa) +\n  tm_polygons(fill_alpha = 0.1) +\ntm_shape(tampines_pts) +\n  tm_dots(\n    fill = \"Local_R2\",           # tmap v4: 'fill' controls symbol fill colour\n    col = \"grey30\",              # thin outline\n    size = 0.6,\n    fill.scale = tm_scale(\n      n = 7,\n      style = \"quantile\"         # consistent with the tutorial’s quantile breaks\n    ),\n    fill.legend = tm_legend(title = \"Local R²\")\n  ) +\n  tm_view(set_zoom_limits = c(11, 14))\n\n\n\n\n\n\n\n# --- 4) Summarise Local R² for Tampines ----------------------------------------\ntampines_r2_summary &lt;- tampines_pts %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    n      = dplyr::n(),\n    r2_min = min(Local_R2, na.rm = TRUE),\n    r2_q1  = quantile(Local_R2, 0.25, na.rm = TRUE),\n    r2_med = median(Local_R2, na.rm = TRUE),\n    r2_mean= mean(Local_R2, na.rm = TRUE),\n    r2_q3  = quantile(Local_R2, 0.75, na.rm = TRUE),\n    r2_max = max(Local_R2, na.rm = TRUE)\n  )\n\ntampines_r2_summary\n\n   n    r2_min     r2_q1    r2_med   r2_mean     r2_q3    r2_max\n1 76 0.8944806 0.9550013 0.9652041 0.9546181 0.9710822 0.9724414\n\n\n\n10.1 Insights into Tampines planning area\nThe adaptive GWR results for Tampines show an exceptionally strong and spatially consistent model fit across condominium resale points. With n = 76, the Local R² ranges from 0.894 to 0.972, a median of 0.965, and a mean of 0.955, indicating that the model explains over 95% of local price variation for most locations. Such high and tightly clustered R² values suggest the explanatory variables—floor area, age, proximity to amenities, accessibility, and tenure—collectively provide a robust representation of spatial price dynamics in Tampines.\nSpatially, the map reveals that the dark blue clusters with higher Local R² values are concentrated around Tampines Central, where key amenities such as the MRT interchange, bus interchange, and shopping malls are located. These areas exhibit highly predictable housing prices because accessibility and urban conveniences strongly align with price determinants. Hence, the model performs best in these well-connected and amenity-rich subzones.\nIn contrast, lighter blue areas along the western and peripheral edges display slightly lower R² values (around 0.89–0.95). This suggests that local housing prices there are influenced by factors not fully captured by the model, such as road noise, micro-neighbourhood effects, or smaller sample density. Nevertheless, these lower values still represent a high level of explanatory power.\nOverall, the GWR model is highly reliable for Tampines, with particularly strong performance near the town centre. Future refinement could include adding micro-accessibility or environmental variables to further improve local accuracy in edge areas."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#introduction",
    "href": "Take-home_Ex01/take-home_ex01.html#introduction",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "1 Introduction",
    "text": "1 Introduction\nRestaurants are a sensitive barometer of urban vitality: they emerge where pedestrian flows, accessibility, and complementary land uses intersect, and they amplify activity through agglomeration effects. This study examines the first-order (intensity) and second-order (interaction) properties of new restaurant registrations in Singapore between 1 January 2025 and 30 June 2025. The workflow comprises five steps: (i) tidying ACRA registration records, (ii) geocoding 6-digit postcodes via OneMap, (iii) projecting to SVY21 (EPSG:3414) to preserve distance accuracy, (iv) estimating kernel densities using fixed and adaptive bandwidths to reveal spatial concentrations over time, and (v) applying Ripley’s \\(L/K\\) functions with Complete Spatial Randomness (CSR) envelopes to identify statistically significant clustering scales. The results are interpreted through the lens of planning and urban management, with implications for town-centre provisioning, licensing cadence, last-mile logistics, and transit-oriented development."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#the-data",
    "href": "Take-home_Ex01/take-home_ex01.html#the-data",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "2 The Data",
    "text": "2 The Data\nTo analyse the Food & Beverage (F&B) industry in Singapore, this exercise integrates four key data sources. The Singapore Standard Industrial Classification (SSIC) provides the authoritative framework to define and filter economic activities (e.g., Restaurants and Cafés). ACRA business registry extracts supply establishment-level records with SSIC codes and addresses. URA Master Plan boundaries offer official geospatial polygons (planning areas and subzones) to aggregate and visualise spatial distributions. Finally, OneMap APIs provide authoritative coordinates and geocoding services to standardise addresses and link firms to spatial units. Together, these datasets allow us to identify, locate, and analyse F&B businesses consistently across Singapore’s geography, ensuring both statistical reliability and spatial accuracy.\n\n\n\nTable 1: Data Sources for F&B Industry Analysis in Singapore\n\n\nDataset_Name\nDescription\nFormat\nSource\n\n\n\n\nSSIC 2020 – Report & Detailed Definitions\nPrior edition still widely used; contains 5-digit codes (e.g., 56111 Restaurants, 56112 Cafés).\nExcel / PDF\nSingStat (DOS)\n\n\nACRA Information on Corporate Entities\nOpen data extract of registered entities, including SSIC codes and addresses for firm-level analysis.\nCSV\ndata.gov.sg (ACRA)\n\n\nURA Master Plan 2019 – Subzone Boundary (No Sea)\nOfficial subzone polygons for aggregation and spatial analysis (choropleths).\nGeoJSON / KML\ndata.gov.sg (URA)\n\n\nOneMap (SLA) – Search & Reverse Geocode / Thematic Layers\nAuthoritative API to geocode/standardise addresses, fetch POIs, and retrieve thematic layers.\nREST / JSON API\nOneMap (SLA)"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#research-quetions",
    "href": "Take-home_Ex01/take-home_ex01.html#research-quetions",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "3 Research Quetions",
    "text": "3 Research Quetions\nAligned with the stated exercise objectives (“identify where and when new firms appear; detect clustering; discuss implications”), our research questions translate the statistical toolkit into targeted, decision-relevant tests for restaurant activity. We deliberately distinguish between first-order analysis (examining where intensity is high) and second-order analysis (evaluating how points interact relative to CSR), while also incorporating a minimal spatio-temporal perspective through monthly facets to capture temporal variation in establishment patterns.\nRQ1 — Where/When: Where do new restaurants (SSIC 56111) concentrate spatially within January 2025 – Jun 2025, and how do these concentrations evolve month-to-month?\nRQ2 — First-order intensity (Core): What is the intensity structure of new restaurants and which locales exhibit persistent high density after smoothing?\nRQ3 — Second-order clustering scales: At which distance bands (e.g., 0–1000 m) do restaurants exhibit significant clustering or dispersion relative to CSR?\nRQ4 — Planning/Management Implications: How should statistically validated hotspots and clustering scales inform town-centre planning, licensing cadence, amenity siting, and last-mile/logistics near identified corridors and nodes?"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#setup-the-environment",
    "href": "Take-home_Ex01/take-home_ex01.html#setup-the-environment",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "4 Setup the environment",
    "text": "4 Setup the environment\nDefines the initial environment settings to ensure consistent execution of the R script.\n\n4.1 Load/install packages\nChecks and installs required R packages (via pacman), then loads libraries for data wrangling, mapping, geospatial analysis, and reproducibility.\n\nif (!require(pacman)) install.packages(\"pacman\")  # install pacman once if missing\n\npacman::p_load(                                   # load (and auto-install if needed)\n  tidyverse,      # data wrangling + plotting (dplyr, tidyr, ggplot2, readr)\n  stringr,        # string helpers (e.g., str_pad, str_detect)\n  lubridate,      # date helpers (e.g., year, month, floor_date)\n  sf,             # vector geospatial (points/polygons, CRS transforms)\n  tmap,           # cartographic maps (static/interactive)\n  terra,          # raster/SpatRaster conversion for KDE images\n  sparr,          # Spatial and spatio-temporal relative risk functions.\n  spatstat.geom,  # required for the spatstat family of analysis functions.\n  stpp,           # spatio-temporal point process package.\n  rvest,          # Web scraping and HTML parsing package.\n  httr,      # HTTP client for OneMap geocoding calls\n  spatstat,  # point pattern analysis meta-package (ppp, Kest, Lest, envelope, density)\n  ggplot2,   # grammar-of-graphics plotting system (part of tidyverse).\n  plotly,    # adds interactivity to ggplot2 plots.\n  ggthemes  # provides professional ggplot2 themes (e.g., tufte, economist).\n)\n\n\n\n4.2 Reproducibility for CSR envelopes\nSets a random seed to guarantee reproducible results when generating CSR envelopes.\n\nset.seed(626)  # reproducibility for CSR envelopes"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#raw-data-preprocessing-restaurants-only",
    "href": "Take-home_Ex01/take-home_ex01.html#raw-data-preprocessing-restaurants-only",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "5 Raw data preprocessing (Restaurants only)",
    "text": "5 Raw data preprocessing (Restaurants only)\nIn this section, we import the Accounting and Corporate Regulatory Authority (ACRA) corporate entities data, which is organised into multiple CSV files. The objective is to compile, clean, and filter these records to isolate restaurants and cafés coded under SSIC 56111. This step is fundamental because it converts raw administrative files into a consistent and analysis-ready dataset, with standardised variable names, parsed dates, and validated postal codes. The outcome is a tidy dataset that preserves the essential business attributes required for downstream geocoding, spatial conversion, and statistical analysis.\n\n5.1 Importing ACRA data\nFirst, we need to import raw ACRA business registration data from CSV files. The code specifies the folder path, lists all files with names starting with “ACRA” and ending in .csv, and then reads them into R using map_dfr(read_csv), which stacks them into a single tibble (row binding).\n\n# folder_path points to the directory containing the ACRA CSV files.\nfolder_path &lt;- \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/aspatial\"\n\n# list.files() collects all ACRA files that match the naming pattern and returns their full paths.\nfile_list &lt;- list.files(path = folder_path, \n                        pattern = \"^ACRA*.*\\\\.csv$\", \n                        full.names = TRUE)\n\n# map_dfr() reads each CSV file and stacks them into one tibble (row binding).\nacra_data &lt;- file_list %&gt;%\n  map_dfr(read_csv)\n\n\n\n5.2 Saving ACRA data for reproducibility\nHere, the imported raw data is saved as an RDS file using write_rds(). This ensures reproducibility and efficiency, since future analysis can load the cleaned RDS file directly without re-importing all CSVs.\n\n# Save the raw imported data as an RDS file to avoid repeated imports in future runs.\nwrite_rds(acra_data,\n          \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/rds/acra_data.rds\")\n\n\n\n5.3 Tidying ACRA data (restaurants only, SSIC 56111)\nNext, we will filters and transforms the raw dataset to focus exclusively on restaurants, defined by SSIC code 56111. The dataset is trimmed to the first 24 columns, the registration date is standardised to Date class, and additional temporal fields such as year, month number, and month abbreviation are created. Postal codes are normalised into six-digit format, and the dataset is restricted to businesses incorporated in 2025.\n\nbiz_56111 &lt;- acra_data %&gt;%\n  select(1:24) %&gt;%                                    # Select only the first 24 columns (ignore extra metadata columns).\n  filter(primary_ssic_code == 56111) %&gt;%              # Keep only businesses coded as restaurants and cafés (SSIC 56111).\n  rename(date = registration_incorporation_date) %&gt;%  # Rename registration date field to a shorter label 'date'.\n  \n  # Convert registration date to Date class, extract Year and Month (numeric and abbreviated).\n  mutate(date = as.Date(date),\n         YEAR = year(date),\n         MONTH_NUM = month(date),\n         MONTH_ABBR = month(date, \n                            label = TRUE, \n                            abbr = TRUE)) %&gt;% \n  \n  # Standardise postal code to six digits with leading zeros.\n  mutate(\n    postal_code = str_pad(postal_code, \n    width = 6, side = \"left\", pad = \"0\")) %&gt;%\n           filter(YEAR == 2025)    \n\n\n\n5.4 Saving tidy restaurant data\nFinally, the dataset is filtered to focus exclusively on restaurant businesses, defined by SSIC code 56111. Only the relevant columns are retained, and the data is restricted to businesses incorporated in 2025.\n\n# Save the tidy restaurant dataset for subsequent steps such as geocoding and EDA.\nwrite_rds(biz_56111, \n          \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/rds/restaurants_56111_tidy.rds\")"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#exploratory-data-analysis-eda-of-restaurants",
    "href": "Take-home_Ex01/take-home_ex01.html#exploratory-data-analysis-eda-of-restaurants",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "6 Exploratory Data Analysis (EDA) of Restaurants",
    "text": "6 Exploratory Data Analysis (EDA) of Restaurants\nBefore any spatial conversion or modelling, it is essential to perform EDA to understand the characteristics of the cleaned dataset. EDA helps confirm that the data aligns with the research window (January 2025 – Jun 2025) and that restaurants are being correctly filtered. It also allows us to observe temporal dynamics such as monthly registration counts and possible seasonal patterns. Through simple summaries, tables, and visualisations, we can validate the dataset’s integrity and gain initial insights into how new restaurants are distributed over time. This diagnostic step ensures confidence in the subsequent geocoding and spatial analyses.\n\n6.1 Inspect data structure\nThis section uses the glimpse() function to display the structure of the tidied restaurant dataset (biz_56111). The function provides an overview of column names, data types, and a preview of the values within each column.\n\n# glimpse() provides an overview of column names, data types, and a sample of values.\nglimpse(biz_56111)\n\nRows: 677\nColumns: 27\n$ uen                               &lt;chr&gt; \"202501136C\", \"202501329K\", \"2025029…\n$ issuance_agency_id                &lt;chr&gt; \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACR…\n$ entity_name                       &lt;chr&gt; \"AL ASHIK PTE. LTD.\", \"ARASH LEGACY …\n$ entity_type_description           &lt;chr&gt; \"Local Company\", \"Local Company\", \"L…\n$ business_constitution_description &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ company_type_description          &lt;chr&gt; \"Exempt Private Company Limited by S…\n$ paf_constitution_description      &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ entity_status_description         &lt;chr&gt; \"Live Company\", \"Live Company\", \"Liv…\n$ date                              &lt;date&gt; 2025-01-08, 2025-01-09, 2025-01-19,…\n$ uen_issue_date                    &lt;date&gt; 2025-01-08, 2025-01-09, 2025-01-19,…\n$ address_type                      &lt;chr&gt; \"LOCAL\", \"LOCAL\", \"LOCAL\", \"LOCAL\", …\n$ block                             &lt;chr&gt; \"305D\", \"21\", \"324\", \"502\", \"15\", \"2…\n$ street_name                       &lt;chr&gt; \"ANCHORVALE LINK\", \"TAN QUEE LAN STR…\n$ level_no                          &lt;chr&gt; \"11\", \"02\", \"na\", \"02\", \"10\", \"na\", …\n$ unit_no                           &lt;chr&gt; \"23\", \"04\", \"na\", \"02\", \"32\", \"na\", …\n$ building_name                     &lt;chr&gt; \"ANCHORVALE PLACE\", \"HERITAGE PLACE\"…\n$ postal_code                       &lt;chr&gt; \"544305\", \"188108\", \"338822\", \"46902…\n$ other_address_line1               &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ other_address_line2               &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ account_due_date                  &lt;chr&gt; \"2026-08-31\", \"2026-07-31\", \"2026-07…\n$ annual_return_date                &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ primary_ssic_code                 &lt;dbl&gt; 56111, 56111, 56111, 56111, 56111, 5…\n$ primary_ssic_description          &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ primary_user_described_activity   &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ YEAR                              &lt;dbl&gt; 2025, 2025, 2025, 2025, 2025, 2025, …\n$ MONTH_NUM                         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, …\n$ MONTH_ABBR                        &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Feb, F…\n\n\nThe output indicates that the dataset contains 677 rows and 27 columns, with fields such as unique entity number (uen), issuance agency, entity name, entity type, business constitution, company type, entity status, registration date, postal code, and location descriptors (block, street, building).\n\n\n6.2 Count restaurants by year\nThis section uses the count() function to summarise the number of restaurants incorporated by year from the tidied dataset. Since the data has already been filtered to include only 2025 records, the result shows a single row with the year 2025 and a corresponding total of 677 newly registered restaurants.\n\n# Summarise how many restaurants were registered in each year within the study period.\nbiz_56111 %&gt;% \n  count(YEAR, name = \"n_restaurants\")\n\n# A tibble: 1 × 2\n   YEAR n_restaurants\n  &lt;dbl&gt;         &lt;int&gt;\n1  2025           677\n\n\nThe dataset shows that in 2025 alone, there were 677 new restaurant registrations recorded within the study period (Jan – Jun 2025).\n\n\n6.3 Monthly registration counts (bar chart)\nRestaurant incorporations in 2025 are aggregated at the monthly level and visualised to reveal short-term temporal dynamics in business activity. The counts are structured chronologically to avoid distortions in seasonal interpretation, and the resulting bar chart provides both absolute values through labelled bars and directional movement through a superimposed trend line. By combining these elements, the plot not only conveys the magnitude of new entries each month but also highlights emerging trajectories within the first half of 2025, setting the stage for discussions on whether incorporations are accelerating, stabilising, or declining as the year progresses.\n\n# 1) Aggregate once: counts per month (make sure months are ordered nicely)\nmonth_levels &lt;- c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")\ndf_month &lt;- biz_56111 %&gt;%\n  count(MONTH_ABBR, name = \"registrations\") %&gt;%\n  mutate(MONTH_ABBR = factor(MONTH_ABBR, levels = month_levels)) %&gt;%\n  arrange(MONTH_ABBR)\n\ny_max &lt;- max(df_month$registrations) * 1.10  # headroom for labels\n\n# 2) Plot: bars + count labels + centre title + trend line\np_month_bar &lt;- ggplot(df_month, aes(x = MONTH_ABBR, y = registrations, group = 1)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = registrations), vjust = -0.5, size = 3.5) +  # labels on bars\n  geom_line(color = \"red\", linewidth = 1) +                          # trend line\n  geom_point(color = \"red\") +                                        # trend points (optional)\n  labs(\n    title = \"New Restaurants by Month (Jan 2025 – Jun 2025)\",\n    x = \"Month\", y = \"Registrations\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +                    # centres title\n  expand_limits(y = y_max)                                           # keep labels above bars\n\np_month_bar\n\n\n\n\n\n\n\n\nBetween Jan 2021 and Jul 2025, new restaurant registrations ranged from a high of 110 in March 2024 to a low of 79 in July 2025, with most months maintaining 90–105 openings, reflecting both seasonal dynamics and the resilience of the F&B sector. From a geospatial perspective, however, this chart is limited as it only shows overall counts by month without accounting for where the restaurants are located, how they cluster across planning areas or subzones, or whether growth is concentrated in certain regions. Without spatial context, we cannot assess neighborhood-level competition, urban planning implications, or the uneven distribution of opportunities across Singapore."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#geospatial-data-wrangling",
    "href": "Take-home_Ex01/take-home_ex01.html#geospatial-data-wrangling",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "7 Geospatial data wrangling",
    "text": "7 Geospatial data wrangling\nThis section converts six-digit postal codes from the cleaned ACRA table into planar coordinates suitable for spatial analysis. We query the OneMap endpoint for each unique postcode, store results, and left-join them back to the restaurant records by matching postal_code to OneMap’s POSTAL. We deliberately retain OneMap’s original column names (e.g., POSTAL, X, Y, LONGITUDE, LATITUDE). After joining, we save an RDS copy for reproducibility and create an sf object directly in SVY21 (EPSG:3414) using coords = c(“X”,“Y”), which is required for distance-true kernel density estimation and Ripley’s K/L.\n\n7.1 Geocoding loop (query OneMap and collect results)\nWe prepare a vector of unique postcodes to minimise API calls, then iterate through them sequentially. For each postcode, the relevant parameters (searchVal, returnGeom, getAddrDetails, pageNum) are submitted to the OneMap endpoint. Successful responses are converted into a data frame and appended to the found table, while unmatched postcodes are stored separately in not_found. This process ensures efficient use of the API, reduces redundancy, and creates a clear separation between valid and invalid geocoded results for subsequent analysis.\n\n# --- Geocoding (It took XX seconds to process this chunk) ---------------------------\n\n# Create a unique vector of six-digit postcodes to avoid redundant API requests.\npostcodes &lt;- unique(biz_56111$postal_code)\n\n# OneMap elastic search endpoint \nurl &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"\n\n# Initialise an empty data.frame to accumulate matched results from OneMap.\nfound &lt;- data.frame()\n\n# Initialise an empty data.frame to record postcodes with no matches.\nnot_found &lt;- data.frame(postcode = character())\n\n# Iterate over each unique postcode.\nfor (pc in postcodes) {\n  query &lt;- list(\n    searchVal    = pc,   # The postcode we are querying for.\n    returnGeom   = \"Y\",  # Request coordinates in the response.\n    getAddrDetails = \"Y\",# Request address details in the response.\n    pageNum      = \"1\"   # First page of results (sufficient for exact postcodes).\n  )\n\n  # Submit an HTTP GET request to OneMap with the query parameters.\n  res  &lt;- httr::GET(url, query = query)\n  \n  # Parse the HTTP response content as a list (OneMap returns JSON).\n  json &lt;- httr::content(res)\n  \n  # If OneMap reports at least one match for this postcode...\n  if (json$found != 0) {\n    \n    # Convert the 'results' array to a data.frame, preserving original OneMap field names.\n    df &lt;- as.data.frame(json$results, stringsAsFactors = FALSE)\n    \n    # Keep the input postcode as a traceability column (useful for QA).\n    df$input_postcode &lt;- pc\n\n    # Append the current result to the cumulative 'found' table.\n    found &lt;- dplyr::bind_rows(found, df)\n\n    # If there is no match for this postcode...\n  } else {\n    \n    # Record the postcode in 'not_found' for later inspection.\n    not_found &lt;- dplyr::bind_rows(not_found, data.frame(postcode = pc))\n\n  }\n}\n\n\n\n7.2 Tidy the geocoded table\nTo prepare the geocoded dataset for integration with the business records, we retain only the essential columns returned by OneMap. Specifically, the first ten fields are preserved, covering postal codes, spatial coordinates (X, Y, longitude, latitude), and key address attributes. This step removes extraneous metadata while maintaining all information required for spatial joins and mapping. By standardising the table structure, subsequent operations such as left_join() become more efficient and less error-prone, ensuring that the geocoded records align seamlessly with the cleaned business dataset. The result is a compact, analysis ready table that is useful for downstream spatial analysis and visualisation.\n\n# Keep the first ten columns from OneMap.\n# (These include POSTAL, X, Y, LONGITUDE, LATITUDE, etc.)\n\nfound &lt;- found %&gt;%\n  dplyr::select(1:10)\n\n\n\n7.3 Append coordinates to restaurants (join by postal code)\nWe attach the geocoded coordinates to the restaurant records using a left join from biz_56111 to found, matching postal_code (our cleaned field) with OneMap’s POSTAL. This preserves all restaurant rows and adds coordinate columns where available.\n\n# Left-join OneMap coordinates back to the restaurant table.\n# The join is on 'postal_code' (ours) = 'POSTAL' (OneMap).\nbiz_56111 &lt;- biz_56111 %&gt;%\n  dplyr::left_join(found, by = c('postal_code' = 'POSTAL'))\n\n\n# Convert geocoded restaurants into sf object\nbiz_56111_sf &lt;- sf::st_as_sf(\n  biz_56111,\n  coords = c(\"X\", \"Y\"),    # the coordinate columns from OneMap geocoding\n  crs    = 3414            # Singapore SVY21 / EPSG:3414\n)\n\n# Quick check: print an overview of column names, data types, and a sample of values.\nglimpse(biz_56111_sf)\n\nRows: 677\nColumns: 35\n$ uen                               &lt;chr&gt; \"202501136C\", \"202501329K\", \"2025029…\n$ issuance_agency_id                &lt;chr&gt; \"ACRA\", \"ACRA\", \"ACRA\", \"ACRA\", \"ACR…\n$ entity_name                       &lt;chr&gt; \"AL ASHIK PTE. LTD.\", \"ARASH LEGACY …\n$ entity_type_description           &lt;chr&gt; \"Local Company\", \"Local Company\", \"L…\n$ business_constitution_description &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ company_type_description          &lt;chr&gt; \"Exempt Private Company Limited by S…\n$ paf_constitution_description      &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ entity_status_description         &lt;chr&gt; \"Live Company\", \"Live Company\", \"Liv…\n$ date                              &lt;date&gt; 2025-01-08, 2025-01-09, 2025-01-19,…\n$ uen_issue_date                    &lt;date&gt; 2025-01-08, 2025-01-09, 2025-01-19,…\n$ address_type                      &lt;chr&gt; \"LOCAL\", \"LOCAL\", \"LOCAL\", \"LOCAL\", …\n$ block                             &lt;chr&gt; \"305D\", \"21\", \"324\", \"502\", \"15\", \"2…\n$ street_name                       &lt;chr&gt; \"ANCHORVALE LINK\", \"TAN QUEE LAN STR…\n$ level_no                          &lt;chr&gt; \"11\", \"02\", \"na\", \"02\", \"10\", \"na\", …\n$ unit_no                           &lt;chr&gt; \"23\", \"04\", \"na\", \"02\", \"32\", \"na\", …\n$ building_name                     &lt;chr&gt; \"ANCHORVALE PLACE\", \"HERITAGE PLACE\"…\n$ postal_code                       &lt;chr&gt; \"544305\", \"188108\", \"338822\", \"46902…\n$ other_address_line1               &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ other_address_line2               &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ account_due_date                  &lt;chr&gt; \"2026-08-31\", \"2026-07-31\", \"2026-07…\n$ annual_return_date                &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ primary_ssic_code                 &lt;dbl&gt; 56111, 56111, 56111, 56111, 56111, 5…\n$ primary_ssic_description          &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ primary_user_described_activity   &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", …\n$ YEAR                              &lt;dbl&gt; 2025, 2025, 2025, 2025, 2025, 2025, …\n$ MONTH_NUM                         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, …\n$ MONTH_ABBR                        &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Feb, F…\n$ SEARCHVAL                         &lt;chr&gt; \"ANCHORVALE PLACE\", \"HERITAGE PLACE\"…\n$ BLK_NO                            &lt;chr&gt; \"305D\", \"21\", \"324\", \"502\", \"15\", \"2…\n$ ROAD_NAME                         &lt;chr&gt; \"ANCHORVALE LINK\", \"TAN QUEE LAN STR…\n$ BUILDING                          &lt;chr&gt; \"ANCHORVALE PLACE\", \"HERITAGE PLACE\"…\n$ ADDRESS                           &lt;chr&gt; \"305D ANCHORVALE LINK ANCHORVALE PLA…\n$ LATITUDE                          &lt;chr&gt; \"1.38903342950987\", \"1.2985892658367…\n$ LONGITUDE                         &lt;chr&gt; \"103.887299862674\", \"103.85640505809…\n$ geometry                          &lt;POINT [m]&gt; POINT (34007.42 41217.84), POI…\n\n\nThe dataset now contains 677 records and 35 variables, representing restaurants incorporated in 2025 under SSIC 56111. Each row corresponds to a unique registered entity, identifiable by its UEN (Unique Entity Number), with accompanying details on company type, incorporation date, address, and geospatial coordinates. The presence of date and uen_issue_date in date format confirms proper temporal standardisation, allowing the dataset to be analysed along monthly or yearly dimensions.\nAddress-related variables such as block, street_name, building_name, postal_code, and their derived forms (BLK_NO, ROAD_NAME, ADDRESS) demonstrate that the cleaning and enrichment steps successfully prepared the dataset for spatial matching. Crucially, the dataset now includes geocoded coordinates (LATITUDE, LONGITUDE) and a geometry field, which transforms each restaurant record into a spatial point object. This enables integration with planning boundaries, kernel density estimation, or other forms of spatial point pattern analysis.\nOverall, the output confirms that the dataset is no longer just an administrative registry but a geospatially enabled dataset. It can now support advanced analysis such as mapping restaurant hotspots, measuring clustering within planning subzones, or comparing incorporation intensity across different regions of Singapore. The presence of 677 records highlights the scale of new restaurant entrants in 2025, and the completeness of attributes ensures both temporal and spatial dimensions can be meaningfully explored.\n\n\n7.4 Save a reproducible copy (RDS) for downstream steps\nSaving after the join ensures the same input is used for EDA and spatial analyses without re-calling the API. If we want to maintain a different project root, adjust the path accordingly.\n\nreadr::write_rds(\n  biz_56111,\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex01/data/rds/biz_56111_geocoded.rds\"\n)"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#boundary-preparation-parse-mp19-kml-description-quality-assurance-qa-map",
    "href": "Take-home_Ex01/take-home_ex01.html#boundary-preparation-parse-mp19-kml-description-quality-assurance-qa-map",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "8 Boundary preparation (parse MP19 KML Description) + Quality Assurance (QA) Map",
    "text": "8 Boundary preparation (parse MP19 KML Description) + Quality Assurance (QA) Map\nMany URA MP19 KML files appear to contain only two visible fields (Name, Description). However, the actual attributes—such as SUBZONE_N and PLN_AREA_N—are embedded within an HTML table inside the Description column. To handle this, we define a helper function that parses the HTML and extracts specific fields by label (e.g., SUBZONE_N). Using this function, we generate clean columns for each feature, remove offshore polygons (e.g., SOUTHERN GROUP, WESTERN ISLANDS), and create a QA map to verify that restaurant points fall correctly within the refined subzone boundaries. This ensures that subsequent spatial analyses (KDE and L/K functions) are accurate and reliable.\n\n8.1 Read MP19 KML and project to SVY21\nWe read the “MasterPlan2019SubzoneBoundaryNoSeaKML.kml”, drop Z/M, and project to SVY21 (EPSG:3414). At this point the table likely has only Name and Description; we do not filter yet because those fields are still inside the HTML.\n\n# read the KML layer (likely Name + Description fields)\nmpsz_raw &lt;- sf::st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\", quiet = TRUE) %&gt;%   \n  sf::st_zm(drop = TRUE, what = \"ZM\") %&gt;%   # drop Z/M to keep 2D geometry only\n  sf::st_transform(crs = 3414)              # project to SVY21 meters so distance/area are metric\n\n\n\n8.2 Helper to extract one field from the KML Description HTML\nWe will parse the HTML table inside Description and return the value from the table row where the &lt;th&gt; (header) equals a requested field label (e.g., “SUBZONE_N”). We wrap this into a function extract_kml_field(html_text, field_name) and make sure it fails safely (returns NA_character_ if the label cannot be found).\n\nextract_kml_field &lt;- function(html_text, field_name) {\n  # If the cell is empty or NA, return NA to avoid errors later\n  if (is.na(html_text) || html_text == \"\") return(NA_character_)\n  \n  # Parse the HTML string into a document\n  page &lt;- rvest::read_html(html_text)\n  \n  # Grab all table rows &lt;tr&gt; (the KML Description usually contains one table)\n  rows &lt;- rvest::html_elements(page, \"tr\")\n  \n  # Keep the row whose header cell &lt;th&gt; equals the requested field label,\n  # then take the text of the &lt;td&gt; cell in that same row\n  value &lt;- rows %&gt;%\n    purrr::keep(~ rvest::html_text2(rvest::html_element(.x, \"th\")) == field_name) %&gt;%\n    rvest::html_element(\"td\") %&gt;%\n    rvest::html_text2()\n  \n  # If nothing matched, return NA; otherwise return the extracted value\n  if (length(value) == 0) NA_character_ else value\n}\n\n\n\n8.3 Materialise attributes from Description, tidy columns, and filter\nWe now map the helper over each feature’s Description to create real columns: REGION_N, PLN_AREA_N, SUBZONE_N, SUBZONE_C. Then we drop the original Name / Description, put geometry last (nice console print), and filter out Southern Group and Western Islands by the newly-created columns—exactly the step that previously failed because those columns didn’t exist.\n\n# --- Parse Description to real columns, then tidy and filter -----------------------\n\nmpsz &lt;- mpsz_raw %&gt;%\n  dplyr::mutate(\n    REGION_N   = purrr::map_chr(Description, extract_kml_field, \"REGION_N\"),   # parse region name\n    PLN_AREA_N = purrr::map_chr(Description, extract_kml_field, \"PLN_AREA_N\"), # parse planning area name\n    SUBZONE_N  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_N\"),  # parse subzone name\n    SUBZONE_C  = purrr::map_chr(Description, extract_kml_field, \"SUBZONE_C\")   # parse subzone code\n  ) %&gt;%\n  dplyr::select(-Name, -Description) %&gt;%     # remove raw Name/Description\n  dplyr::relocate(geometry, .after = dplyr::last_col())   # move geometry to the last column\n\n# Print a few rows so we can see the new columns are present\nmpsz %&gt;% dplyr::slice_head(n = 3)\n\nSimple feature collection with 3 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 24431.8 ymin: 28515.07 xmax: 29766.12 ymax: 29741.75\nProjected CRS: SVY21 / Singapore TM\n        REGION_N  PLN_AREA_N   SUBZONE_N SUBZONE_C\n1 CENTRAL REGION BUKIT MERAH  DEPOT ROAD    BMSZ12\n2 CENTRAL REGION BUKIT MERAH BUKIT MERAH    BMSZ02\n3 CENTRAL REGION      OUTRAM   CHINATOWN    OTSZ03\n                        geometry\n1 MULTIPOLYGON (((25910.34 29...\n2 MULTIPOLYGON (((26750.09 29...\n3 MULTIPOLYGON (((29161.2 297...\n\n# Filter out offshore groups using the newly created columns\nmpsz_clean &lt;- mpsz %&gt;%\n  dplyr::filter(SUBZONE_N != \"SOUTHERN GROUP\",\n                PLN_AREA_N != \"WESTERN ISLANDS\",\n                SUBZONE_N != \"NORTH-EASTERN ISLANDS\"\n                )\n\n\n\n8.4 QA Map (tmap v4): Restaurants over cleaned MP19 subzones\nWe will build a QA map to confirm geocoded restaurants lie within the cleaned subzones. The code builds a static quality-assurance map to verify that geocoded restaurant points fall correctly within Singapore’s Master Plan 2019 planning areas. It first aligns coordinate reference systems (reprojecting the restaurant points if needed to match the MP19 layer, SVY21), then dissolves subzone polygons into single planning-area geometries via group_by (PLN_AREA_N) |&gt; summarise() and repairs any invalid shapes with st_make_valid(). With tmap in plot mode, it renders the planning-area polygons in neutral greys and places readable area labels (tm_text with shadow and auto-placement). Finally, it overlays the restaurant locations from biz_56111_sf as semi-transparent red dots and adds a clear title, producing a publication-ready QA visual that confirms spatial alignment before downstream analyses.\n\n# --- QA Map (tmap v4): restaurants over MP19 planning areas with labels -----\n\n# 0) Ensure both layers share the same CRS (SVY21)\nif (sf::st_crs(biz_56111_sf) != sf::st_crs(mpsz_clean)) {\n  biz_56111_sf &lt;- sf::st_transform(biz_56111_sf, sf::st_crs(mpsz_clean))  # align CRS if needed\n}\n\n# 1) Dissolve subzones -&gt; Planning Areas (by PLN_AREA_N)\n#    (keeps one polygon per planning area; fixes invalids just in case)\nmp19_pa &lt;-\n  mpsz_clean |&gt;\n  dplyr::group_by(PLN_AREA_N) |&gt;\n  dplyr::summarise(.groups = \"drop\") |&gt;\n  sf::st_make_valid()\n\n# 2) Draw the QA map with labels\ntmap::tmap_mode(\"plot\")  # static mode (consistent in reports)\n\nqa_map_pa &lt;-\n  tmap::tm_shape(mp19_pa) +\n    tmap::tm_polygons(\n      fill = \"grey95\",     # polygon fill (v4)\n      col  = \"grey50\",     # polygon outline color (v4)\n      lwd  = 0.6\n    ) +\n    tmap::tm_text(\n      \"PLN_AREA_N\",\n      size = 0.5,          # label size\n      col  = \"grey20\",\n      shadow = TRUE,\n      auto.placement = TRUE\n    ) +\n  tmap::tm_shape(biz_56111_sf) +\n    tmap::tm_dots(\n      size      = 0.15,    # dot size\n      fill      = \"red\",   # dot color\n      fill_alpha= 0.7      # dot transparency (v4)\n    ) +\n  tmap::tm_title(\"QA Map: Geocoded Restaurants over MP19 Planning Areas (SVY21)\")\n\nqa_map_pa   # IMPORTANT: print the map so Quarto renders it\n\n\n\n\n\n\n\n\nThe QA map overlays geocoded restaurant registrations (red dots) from the ACRA dataset onto Singapore’s Master Plan 2019 planning areas (grey polygons). The spatial alignment confirms that the restaurant points fall within their corresponding planning area boundaries, indicating that the geocoding and CRS transformation were successfully executed.\nFrom the spatial distribution, several insights emerge. The densest concentration of new restaurants in 2025 is clustered in the Central Region, particularly within planning areas such as Orchard, River Valley, Downtown Core, Outram, and Bukit Merah. This reflects the commercial dominance of the central business district and adjacent entertainment hubs, where demand for food and beverage outlets is consistently high. Moderate densities appear across Toa Payoh, Geylang, Marine Parade, and Tampines, showing strong activity in mature residential towns and regional centres. In contrast, western and northern regions such as Lim Chu Kang, Sungei Kadut, and Western Water Catchment display sparse distributions, suggesting minimal restaurant incorporations due to their industrial or non-residential land uses.\nOverall, the output validates the geospatial preprocessing workflow while simultaneously revealing the spatial asymmetry of restaurant incorporations in Singapore. Concentrations in central and eastern urban corridors highlight areas of vibrant market demand, while sparse coverage in peripheral planning areas reflects structural land-use constraints and lower commercial viability. This spatial insight sets the foundation for deeper first- and second-order spatial point pattern analyses, such as Kernel Density Estimation (KDE) or clustering tests, to quantify and statistically assess these observed patterns.\n\n8.4.1 Enhancing spatial validation with interactive QA maps\nThis interactive version of the QA map is important because it allows users to zoom, pan, and explore individual planning areas in greater detail. While the static map is suitable for reporting, an interactive map provides flexibility to validate whether each restaurant point is correctly placed within its boundary, especially in dense central regions where overlaps occur. It enhances data verification, spatial accuracy checks, and stakeholder communication, making it easier to spot potential geocoding or boundary alignment errors that might not be visible in a static output.\n\ntmap::tmap_mode(\"view\")\n\nqa_map_pa_int &lt;-\n  tm_basemap(\"CartoDB.Positron\") +\n  tm_shape(mp19_pa, name = \"Planning Areas\") +\n    tm_polygons(fill = \"grey80\", col = \"grey50\", lwd = 0.6, alpha = 0.3) +\n  tm_shape(mp19_pa, name = \"PA Labels\") +\n    tm_text(\"PLN_AREA_N\", size = 1, col = \"black\",\n            bg.color = \"white\", bg.alpha = 0.6, auto.placement = TRUE) +\n  tm_shape(biz_56111_sf, name = \"Restaurants\") +\n    tm_dots(\n            size      = 0.4,    # dot size\n            fill      = \"red\",   # dot color\n            fill_alpha= 0.7      # dot transparency (v4)\n    )\n\nqa_map_pa_int"
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#first-order-analysis-kernel-density-estimation-kde",
    "href": "Take-home_Ex01/take-home_ex01.html#first-order-analysis-kernel-density-estimation-kde",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "9 First-order analysis: Kernel Density Estimation (KDE)",
    "text": "9 First-order analysis: Kernel Density Estimation (KDE)\nThis section investigates the first-order intensity of new restaurant registrations using KDE. We first estimate intensity surfaces for all Singapore (national level). Next, we drill down to the planning subzone level to highlight local variations. Finally, we extend to spatio-temporal KDE (STKDE) to capture dynamics across both space and time (Jan–Jun 2025).\n\n9.1 Prepare spatstat objects\nWe already have clean data: restaurants_sf (points in EPSG:3414) and mpsz_cl (planning subzones in EPSG:3414). Here, we convert them into formats required for point pattern analysis: an observation window (owin) and a point pattern (ppp). We also create a kilometer-scaled version for sensitivity analysis.\n\n# 0) Defensive checks: both layers must be in SVY21 (EPSG:3414; units = meters)\nstopifnot(sf::st_crs(mpsz_clean)$epsg == 3414)      # confirm polygons (study area) are in 3414\nstopifnot(sf::st_crs(biz_56111_sf)$epsg == 3414)    # confirm restaurant points are in 3414\n\n# 1) Create the observation window (owin) from subzones (still in meters here)\nsg_owin &lt;- spatstat.geom::as.owin(mpsz_clean)       # convert sf polygons -&gt; spatstat 'owin' window\n\n# 2) Create the point pattern (ppp) from restaurants (still in meters here)\nrest_ppp &lt;- spatstat.geom::as.ppp(biz_56111_sf)     # convert sf points -&gt; spatstat 'ppp' pattern\n\n# 3) Clip the points to the study window (meter scale) to enforce the boundary\nrest_ppp &lt;- rest_ppp[sg_owin]                        # keep only points that fall inside 'sg_owin'\n\n# 4) Rescale the point pattern from meters to kilometers (single, canonical step)\nrest_ppp_km &lt;- rescale.ppp(                          # produce km-scale 'ppp' for all downstream KDE\n  rest_ppp,                                          # input 'ppp' in meters\n  1000,                                              # scale factor: divide coordinates by 1000 (m -&gt; km)\n  \"km\"                                               # label for the new distance unit\n)\n\n# 5) Quick sanity checks to verify objects and units before proceeding\nstopifnot(spatstat.geom::is.ppp(rest_ppp_km))        # confirm we have a valid 'ppp' object\nrest_ppp_km$unitname                                 # should print \"km\" as the working unit\n\nNULL\n\nsummary(rest_ppp_km)                                 # optional: inspect counts, window, and mark info\n\nMarked planar point pattern:  677 points\nAverage intensity 1.011177 points per square km\n\nCoordinates are given to 15 decimal places\n\nMark variables: uen, issuance_agency_id, entity_name, entity_type_description, \nbusiness_constitution_description, company_type_description, \npaf_constitution_description, entity_status_description, date, uen_issue_date, \naddress_type, block, street_name, level_no, unit_no, building_name, \npostal_code, other_address_line1, other_address_line2, account_due_date, \nannual_return_date, primary_ssic_code, primary_ssic_description, \nprimary_user_described_activity, YEAR, MONTH_NUM, MONTH_ABBR, SEARCHVAL, \nBLK_NO, ROAD_NAME, BUILDING, ADDRESS, LATITUDE, LONGITUDE\nSummary:\n     uen            issuance_agency_id entity_name       \n Length:677         Length:677         Length:677        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n entity_type_description business_constitution_description\n Length:677              Length:677                       \n Class :character        Class :character                 \n Mode  :character        Mode  :character                 \n                                                          \n                                                          \n                                                          \n                                                          \n company_type_description paf_constitution_description\n Length:677               Length:677                  \n Class :character         Class :character            \n Mode  :character         Mode  :character            \n                                                      \n                                                      \n                                                      \n                                                      \n entity_status_description      date            uen_issue_date      \n Length:677                Min.   :2025-01-01   Min.   :2025-01-01  \n Class :character          1st Qu.:2025-02-20   1st Qu.:2025-02-20  \n Mode  :character          Median :2025-04-10   Median :2025-04-10  \n                           Mean   :2025-04-12   Mean   :2025-04-12  \n                           3rd Qu.:2025-06-04   3rd Qu.:2025-06-04  \n                           Max.   :2025-07-31   Max.   :2025-07-31  \n                                                                    \n address_type          block           street_name          level_no        \n Length:677         Length:677         Length:677         Length:677        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   unit_no          building_name      postal_code        other_address_line1\n Length:677         Length:677         Length:677         Length:677         \n Class :character   Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   Mode  :character   \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n other_address_line2 account_due_date   annual_return_date primary_ssic_code\n Length:677          Length:677         Length:677         Min.   :56111    \n Class :character    Class :character   Class :character   1st Qu.:56111    \n Mode  :character    Mode  :character   Mode  :character   Median :56111    \n                                                           Mean   :56111    \n                                                           3rd Qu.:56111    \n                                                           Max.   :56111    \n                                                                            \n primary_ssic_description primary_user_described_activity      YEAR     \n Length:677               Length:677                      Min.   :2025  \n Class :character         Class :character                1st Qu.:2025  \n Mode  :character         Mode  :character                Median :2025  \n                                                          Mean   :2025  \n                                                          3rd Qu.:2025  \n                                                          Max.   :2025  \n                                                                        \n   MONTH_NUM       MONTH_ABBR   SEARCHVAL            BLK_NO         \n Min.   :1.000   Mar    :110   Length:677         Length:677        \n 1st Qu.:2.000   Jan    :102   Class :character   Class :character  \n Median :4.000   Jun    :100   Mode  :character   Mode  :character  \n Mean   :3.885   Apr    : 97                                        \n 3rd Qu.:6.000   Feb    : 96                                        \n Max.   :7.000   May    : 93                                        \n                 (Other): 79                                        \n  ROAD_NAME           BUILDING           ADDRESS            LATITUDE        \n Length:677         Length:677         Length:677         Length:677        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  LONGITUDE        \n Length:677        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWindow: polygonal boundary\n41 separate polygons (26 holes)\n                  vertices         area relative.area\npolygon 1              285  1.61128e+00      2.41e-03\npolygon 2               27  1.50315e-02      2.25e-05\npolygon 3 (hole)        41 -4.01660e-02     -6.00e-05\npolygon 4 (hole)       317 -5.11280e-02     -7.64e-05\npolygon 5 (hole)         3 -4.14100e-10     -6.19e-13\npolygon 6               30  2.80002e-02      4.18e-05\npolygon 7 (hole)         4 -2.86396e-07     -4.28e-10\npolygon 8 (hole)         3 -1.81440e-10     -2.71e-13\npolygon 9 (hole)         3 -5.99531e-10     -8.95e-13\npolygon 10 (hole)        3 -3.04560e-10     -4.55e-13\npolygon 11 (hole)        3 -4.46108e-10     -6.66e-13\npolygon 12 (hole)        5 -2.44408e-10     -3.65e-13\npolygon 13 (hole)        5 -3.64686e-08     -5.45e-11\npolygon 14              71  8.18750e-03      1.22e-05\npolygon 15 (hole)       38 -7.79904e-03     -1.16e-05\npolygon 16              91  1.49663e-02      2.24e-05\npolygon 17 (hole)      395 -7.38124e-03     -1.10e-05\npolygon 18              40  1.38607e-02      2.07e-05\npolygon 19 (hole)       11 -8.36705e-05     -1.25e-07\npolygon 20 (hole)        3 -2.33435e-09     -3.49e-12\npolygon 21              45  2.51218e-03      3.75e-06\npolygon 22             139  3.22293e-03      4.81e-06\npolygon 23             148  3.10395e-03      4.64e-06\npolygon 24 (hole)        4 -1.72650e-10     -2.58e-13\npolygon 25              75  1.73526e-02      2.59e-05\npolygon 26              83  5.28920e-03      7.90e-06\npolygon 27             106  3.04104e-03      4.54e-06\npolygon 28              71  5.63061e-03      8.41e-06\npolygon 29              10  1.99717e-04      2.98e-07\npolygon 30 (hole)        3 -1.37223e-08     -2.05e-11\npolygon 31 (hole)        3 -8.68789e-10     -1.30e-12\npolygon 32 (hole)        3 -3.39815e-10     -5.08e-13\npolygon 33 (hole)        3 -4.52041e-11     -6.75e-14\npolygon 34 (hole)        3 -3.90173e-11     -5.83e-14\npolygon 35 (hole)        3 -9.59845e-11     -1.43e-13\npolygon 36 (hole)        8 -4.28707e-07     -6.40e-10\npolygon 37 (hole)        4 -2.18619e-10     -3.27e-13\npolygon 38 (hole)        6 -8.37554e-07     -1.25e-09\npolygon 39 (hole)        5 -2.92235e-10     -4.36e-13\npolygon 40           14053  6.67892e+02      9.98e-01\npolygon 41 (hole)        3 -7.43616e-12     -1.11e-14\nenclosing rectangle: [2.66754, 55.94194] x [21.44847, 50.25633] km\n                     (53.27 x 28.81 km)\nWindow area = 669.517 square km\nUnit of length: 1 km\nFraction of frame area: 0.436\n\n# Simple visual check\nplot(unmark(rest_ppp_km), main = \"Restaurants (ppp, km units)\")  # base plot of points & window\n\n\n\n\n\n\n\n\nThe polygonal boundary output confirms that the study window is irregular, consisting of 41 polygons with 26 holes and a usable land area of 669.5 km², representing only 43.6% of the bounding frame. Within this space, 677 restaurants classified under SSIC 56111 are distributed, producing an average intensity of about 1.01 points per km². This indicates that density varies substantially across the region, with likely hotspots in central subzones and sparse distribution at the periphery. The presence of rich attributes and temporal fields also enables multi-dimensional analysis beyond static spatial clustering.\nThe plot shows the spatial distribution of 677 restaurants across Singapore, represented as points within the polygonal boundary. The pattern indicates clear clustering in the central and southern regions, particularly around the Downtown Core and Orchard areas, reflecting the city’s commercial and entertainment hubs. In contrast, peripheral areas such as the north and far east exhibit sparser distributions, suggesting fewer restaurant establishments relative to population or land use. This visualisation confirms spatial heterogeneity in restaurant presence, highlighting distinct hotspots of activity alongside low-density regions, and provides the foundation for subsequent kernel density estimation to quantify clustering patterns.\n\n\n9.2 Bandwidth estimation (km scale)\nThe bandwidth (\\(\\sigma\\)) controls the degree of spatial smoothing in kernel density estimation and therefore determines how finely or coarsely local clusters are represented. Choosing \\(\\sigma\\) well is essential: too small a value creates noisy, spike-like artefacts that can be mistaken for meaningful hotspots; too large a value can blur important local variation and hide genuine clusters. In this study, the working unit is kilometers, so all bandwidths are reported and interpreted in km. We first obtain a data-driven σ using Diggle’s cross-validation approach, which balances bias and variance without manual tuning. We then compute three commonly used alternatives—CvL (Cronie–van Lieshout), Scott’s rule, and PPL (profile likelihood)—to understand the sensitivity of our analysis to bandwidth choice. Reporting several selectors is good practice for transparency, but in the next subsection we will use the Diggle value as the principal \\(\\sigma\\) and compare it with a fixed \\(\\sigma\\) (0.6 km) and an adaptive method.\n\n# Input: 'rest_ppp_km' created in previous section (a ppp with units = km)\n\n# 1) Diggle’s cross-validation bandwidth (primary choice)\nbw_diggle &lt;- bw.diggle(rest_ppp_km)   # returns a numeric σ in km based on CV\nbw_diggle                             # print the value for the report\n\n      sigma \n0.007046933 \n\n# 2) Alternative automatic selectors (for sensitivity checks)\nbw_cvl   &lt;- bw.CvL(rest_ppp_km)       # Cronie–van Lieshout selector (km)\nbw_scott &lt;- bw.scott(rest_ppp_km)     # Scott’s rule-of-thumb (can return σ.x, σ.y)\nbw_ppl   &lt;- bw.ppl(rest_ppp_km)       # Profile likelihood selector (km)\n\n# 3) Inspect and store the values we plan to cite in the narrative\nbw_cvl\n\n   sigma \n6.128772 \n\nbw_scott\n\n sigma.x  sigma.y \n1.983023 1.640206 \n\nbw_ppl\n\n    sigma \n0.5112102 \n\n# 4) Keep a small named list for later reference in tables/plots\nbw_all_km &lt;- list(\n  diggle = bw_diggle,                 # preferred data-driven σ (km)\n  cvl    = bw_cvl,                    # alternative CV-based σ (km)\n  scott  = bw_scott,                  # rule-of-thumb σ (km) — may be vector\n  ppl    = bw_ppl                     # profile likelihood σ (km)\n)\n\nIn estimating the bandwidth for kernel density analysis, several selectors were applied and the results highlight how different methods lead to very different levels of smoothing. Diggle’s cross-validation returns a bandwidth of only 0.007 km, which corresponds to roughly seven meters and produces an extremely fine-grained surface. This reveals very localised clusters of restaurants, but at the cost of generating a noisy density surface that may exaggerate small-scale variations. At the other extreme, the Cronie–van Lieshout selector produces a much broader bandwidth of 6.13 km. This choice smooths over local differences and emphasises regional-scale patterns, making it suitable for visualising city-wide concentration trends but less effective in highlighting specific hotspots. Scott’s rule-of-thumb suggests anisotropic values of 1.98 km in the x-direction and 1.64 km in the y-direction, offering a moderate compromise between local and regional smoothing while also accounting for directional variation in the spatial spread. The profile likelihood approach yields a bandwidth of 0.51 km, representing an intermediate scale that is wide enough to avoid the noise of Diggle’s estimate while still being narrow enough to capture meaningful neighbourhood-level clusters.\nTaken together, these results underline the importance of bandwidth choice in kernel density estimation. A smaller bandwidth such as Diggle’s is appropriate when the research goal is to highlight fine-scale clustering, for instance identifying concentrations along individual streets or blocks. A larger bandwidth such as the Cronie–van Lieshout estimate is better suited for planning-level analysis where broad geographic patterns matter more than local detail. The profile likelihood and Scott’s estimates occupy a middle ground, offering balanced perspectives that reveal neighbourhood-scale concentrations and directional trends. For this study, it is therefore useful to report and compare multiple bandwidths, as the contrasting values demonstrate how sensitive density surfaces are to the smoothing parameter and ensure that both micro- and macro-level insights can be drawn for interpretation and planning purposes.\n\n\n9.3 KDE with alternative bandwidth strategies (all outputs in km²)\nWith the point pattern rescaled to kilometers and bandwidth values estimated, we now generate kernel KDE of restaurant intensity. KDE provides a smooth surface of expected events per \\(km^2\\) by centring a kernel function over each observed point. Three approaches are used to reflect different assumptions about spatial structure: Diggle’s bandwidth, a data-driven estimate derived from cross-validation; a fixed bandwidth of 0.6 km, chosen to represent a neighbourhood-scale radius consistent with urban walking distances; and an adaptive bandwidth, which adjusts the kernel size depending on local point density. The Gaussian kernel with edge correction is used throughout to ensure smoothness and reduce boundary bias. Comparing these three approaches allows us to evaluate whether clustering patterns are robust across parameterisations or sensitive to smoothing assumptions. This not only enhances methodological transparency but also provides practical insight into the scale at which clustering is most relevant for urban planning and decision-making.\n\n# Input: 'rest_ppp_km' (restaurants point pattern, km units)\n#        'bw_diggle'   (Diggle bandwidth estimated in km, Section 9.2)\n\n# 1) KDE using Diggle’s automatic bandwidth (Gaussian, edge corrected)\nkde_rest_diggle &lt;- density(\n  rest_ppp_km,          # point pattern of restaurants (km)\n  sigma  = bw_diggle,   # data-driven bandwidth (km)\n  edge   = TRUE,        # apply edge correction\n  kernel = \"gaussian\"   # Gaussian kernel function\n)\n\n# 2) KDE using a fixed bandwidth of 0.6 km\nsigma_fixed &lt;- 0.6      # fixed kernel radius, interpretable as 600 m\nkde_rest_fixed &lt;- density(\n  rest_ppp_km,\n  sigma  = sigma_fixed, # fixed bandwidth (km)\n  edge   = TRUE,        # apply edge correction\n  kernel = \"gaussian\"\n)\n\n# 3) Adaptive KDE — kernel radius varies with local density\nkde_rest_adapt &lt;- adaptive.density(\n  rest_ppp_km,          # point pattern (km)\n  method = \"kernel\"     # kernel-based adaptive density estimation\n)\n\n# 4) Convert spatstat image (im) outputs to SpatRaster for mapping/export later\nkde_rest_diggle_rast &lt;- terra::rast(kde_rest_diggle)\nkde_rest_fixed_rast  &lt;- terra::rast(kde_rest_fixed)\nkde_rest_adapt_rast  &lt;- terra::rast(kde_rest_adapt)\n\n# 5) Quick checks (optional) to confirm results are not empty\nsummary(kde_rest_diggle)   # range of intensities (restaurants per km²)\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2.667538, 55.94194] x [21.44847, 50.25633] km\ndimensions of each pixel: 0.416 x 0.2250614 km\nImage is defined on a subset of the rectangular grid\nSubset area = 669.941961122495 square km\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-2.658836e-14, 160.1333]\n    integral = 677\n    mean = 1.010535\n\nsummary(kde_rest_fixed)    # check smoothing behaviour\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2.667538, 55.94194] x [21.44847, 50.25633] km\ndimensions of each pixel: 0.416 x 0.2250614 km\nImage is defined on a subset of the rectangular grid\nSubset area = 669.941961122495 square km\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-1.166097e-15, 28.26159]\n    integral = 681.697\n    mean = 1.017546\n\nsummary(kde_rest_adapt)    # confirm adaptive surface has variation\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2.667538, 55.94194] x [21.44847, 50.25633] units\ndimensions of each pixel: 0.416 x 0.2250614 units\nImage is defined on a subset of the rectangular grid\nSubset area = 669.941961122495 square units\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-5.129733e-16, 59.02006]\n    integral = 680.3572\n    mean = 1.015546\n\n\nThe KDE outputs summarise the smooth intensity of restaurant locations across Singapore using different bandwidth strategies. Diggle’s cross-validated bandwidth produces a very fine-scale surface, resulting in highly localised peaks that capture micro-clusters but risk noise amplification. The fixed bandwidth of 0.6 km generates a more interpretable density surface at the neighbourhood scale, approximating walking distances and highlighting urban clusters in a practical manner. The adaptive bandwidth adjusts kernel size based on point intensity, offering a balanced representation that smooths sparse regions while preserving detail in dense cores. Together, these approaches illustrate how bandwidth choice determines whether clustering is interpreted at micro, meso, or macro scales.\n\n\n9.4 Assigning projection to KDE rasters\nThe kernel surfaces we created are im objects that were converted to SpatRaster so they can be plotted with raster tools and combined with vector layers. However, these raster objects do not carry a CRS by default. Without an explicit projection, the maps could still render but would not align reliably with other layers (e.g., planning subzones) or export with proper spatial metadata. To make the density rasters interoperable with the rest of the report, we attach the Singapore Transverse Mercator (SVY21) projection, EPSG:3414, directly to each KDE raster. This mirrors the exact practice used elsewhere in the report: vectors already live in EPSG:3414; we simply declare the same CRS on the KDE rasters so overlays and cartographic elements (graticules, compass, scale) behave as expected. No reprojection or resampling is performed here — only CRS assignment — so the numeric values of the density estimates remain unchanged.\n\n# Input from §9.4: KDE rasters created via terra::rast(...)\n#   kde_rest_diggle_rast\n#   kde_rest_fixed_rast\n#   kde_rest_adapt_rast\n\n# 1) Fetch the authoritative EPSG:3414 definition from the study polygons\ncrs_wkt &lt;- sf::st_crs(mpsz_clean)$wkt    # SVY21 / Singapore TM (EPSG:3414)\n\n# 2) Assign this CRS to each KDE raster (declaration only; no reprojection)\nterra::crs(kde_rest_diggle_rast) &lt;- crs_wkt   # set CRS on Diggle raster\nterra::crs(kde_rest_fixed_rast)  &lt;- crs_wkt   # set CRS on fixed-σ raster\nterra::crs(kde_rest_adapt_rast)  &lt;- crs_wkt   # set CRS on adaptive raster\n\n# 3) Quick confirmation (prints the SVY21 definition)\nterra::crs(kde_rest_diggle_rast)\n\n[1] \"PROJCRS[\\\"SVY21 / Singapore TM\\\",\\n    BASEGEOGCRS[\\\"SVY21\\\",\\n        DATUM[\\\"SVY21\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4757]],\\n    CONVERSION[\\\"Singapore Transverse Mercator\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",1.36666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",103.833333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",28001.642,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",38744.572,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (N)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (E)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Cadastre, engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Singapore - onshore and offshore.\\\"],\\n        BBOX[1.13,103.59,1.47,104.07]],\\n    ID[\\\"EPSG\\\",3414]]\"\n\nterra::crs(kde_rest_fixed_rast)\n\n[1] \"PROJCRS[\\\"SVY21 / Singapore TM\\\",\\n    BASEGEOGCRS[\\\"SVY21\\\",\\n        DATUM[\\\"SVY21\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4757]],\\n    CONVERSION[\\\"Singapore Transverse Mercator\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",1.36666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",103.833333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",28001.642,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",38744.572,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (N)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (E)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Cadastre, engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Singapore - onshore and offshore.\\\"],\\n        BBOX[1.13,103.59,1.47,104.07]],\\n    ID[\\\"EPSG\\\",3414]]\"\n\nterra::crs(kde_rest_adapt_rast)\n\n[1] \"PROJCRS[\\\"SVY21 / Singapore TM\\\",\\n    BASEGEOGCRS[\\\"SVY21\\\",\\n        DATUM[\\\"SVY21\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4757]],\\n    CONVERSION[\\\"Singapore Transverse Mercator\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",1.36666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",103.833333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",28001.642,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",38744.572,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (N)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (E)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Cadastre, engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Singapore - onshore and offshore.\\\"],\\n        BBOX[1.13,103.59,1.47,104.07]],\\n    ID[\\\"EPSG\\\",3414]]\"\n\n\nThe output confirms that the kernel density raster layers produced earlier did not automatically carry a coordinate reference system (CRS). To ensure proper alignment with Singapore’s planning boundaries and other spatial layers, the SVY21 / Singapore Transverse Mercator (EPSG:3414) projection is explicitly assigned to each raster. This step does not resample or modify density values but simply attaches the correct spatial metadata so overlays, graticules, and scale bars behave as expected. The insight here is that assigning the CRS guarantees interoperability across maps, enabling accurate visualisation, integration with vector data, and reliable interpretation for urban planning analysis.\n\n\n9.5 Plotting KDE rasters with tmap\nIn this section, we will render each KDE surface as a cartographic map using the exact idiom shown earlier: tm_shape(&lt;SpatRaster&gt;) + tm_raster(col.scale = tm_scale_continuous(values=\"viridis\"), col.legend = tm_legend(...)), then subzone borders, graticules, a north arrow, and a simple layout. The objective is to visually verify that (1) each raster carries EPSG:3414 (set earlier) and overlays correctly with the subzones, and (2) the legend and decorations match the example. No manual class breaks are introduced; we keep a continuous viridis scale and a legend box at right–bottom with a semi-transparent white background. Three maps are produced which are automatic (Diggle \\(\\sigma\\)), fixed \\(\\sigma\\) (0.6 km), and adaptive. These enable us to confirm the outputs line up with expectations and are ready for interpretation.\n\n# 0) Load tmap and switch to static plotting mode (required for tm_* functions)\nlibrary(tmap)                                         # provide tm_shape, tm_raster, etc.\ntmap_mode(\"plot\")                                     # static (non-interactive) plots\n\n# 1) Helper to build a map in the house style shown in the notes\nbuild_kde_map &lt;- function(rast, title_text) {\n  tm_shape(rast) +                                    # KDE SpatRaster (has EPSG:3414)\n    tm_raster(\n      col.scale  = tm_scale_continuous(values = \"viridis\"), # continuous viridis scale\n      col.legend = tm_legend(                         # legend box settings as in screenshots\n        title      = \"Density values\",                # legend title\n        title.size = 0.7,                             # title font size\n        text.size  = 0.7,                             # tick labels font size\n        bg.color   = \"white\",                         # white legend background\n        bg.alpha   = 0.7,                             # semi-transparent background\n        position   = tm_pos_in(\"right\", \"bottom\"),    # place legend at right–bottom\n        frame      = TRUE                             # thin frame around legend\n      )\n    ) +\n    tm_shape(mpsz_clean) +                            # overlay subzone boundaries\n    tm_borders(col = \"grey30\", lwd = 0.4) +           # thin grey borders\n    tm_graticules(labels.size = 0.7) +                # graticules with labels\n    tm_compass() +                                    # north arrow\n    tm_layout(title = title_text, \n              title.position=c(\"center\", \"bottom\"),\n              scale = 1.0\n              )        # title + 1.0 scale (as shown)\n}\n\n# 2) Build the three maps (inputs were created in §9.4 and given a CRS in §9.5)\nmap_kde_diggle &lt;- build_kde_map(kde_rest_diggle_rast, \"KDE (Diggle σ) — Restaurants\")\nmap_kde_fixed  &lt;- build_kde_map(kde_rest_fixed_rast,  \"KDE (Fixed σ = 0.6 km) — Restaurants\")\nmap_kde_adapt  &lt;- build_kde_map(kde_rest_adapt_rast,  \"KDE (Adaptive) — Restaurants\")\n\n# 3) Print the maps (one after another)\nmap_kde_diggle\n\n\n\n\n\n\n\nmap_kde_fixed\n\n\n\n\n\n\n\nmap_kde_adapt\n\n\n\n\n\n\n\n\nThe three KDE variants provide distinct perspectives on the spatial distribution of restaurants. Adaptive KDE highlights localized hotspots by adjusting the bandwidth to point density, thereby capturing both dense clusters and sparse areas with appropriate resolution. In contrast, Fixed KDE applies a constant bandwidth across the study area, producing a smoother and more generalized surface that is useful for regional-level comparisons but less sensitive to local variation. Diggle’s bandwidth method emphasizes very fine-grained patterns, which can be valuable for micro-scale exploration, though it often exaggerates noise and yields fragmented results.\nFor this exercise, we adopt the Adaptive KDE approach, as it strikes a balance between local detail and overall interpretability, making it well suited for planning, policy, and spatial decision-making in the Singapore context. Nonetheless, KDE is sensitive to bandwidth choice and scale dependency, and it does not account for underlying population or land-use factors. These limitations must be considered when interpreting the identified hotspots.\n\n\n9.6 First-order Spatial Point Pattern Analysis (SPPA) at the Planning Subzone Level\nThis section applies First Order SPPA to assess how the distribution of restaurant locations varies across Singapore’s planning subzones. Unlike kernel density estimation at the continuous surface level, the subzone-based approach aggregates point counts into administrative boundaries, enabling direct comparison of spatial intensity relative to area size and population density. This allows us to identify subzones with unusually high or low restaurant concentrations, detect clustering patterns at a policy-relevant scale, and provide insights for urban planning and commercial decision-making.\n\n9.6.1 Geospatial data wrangling\nWe will prepare four contrasted study areas so the subsequent first-order analyses (intensity mapping via KDE) are meaningful and comparable across different urban contexts. The choice is deliberate: (1) Downtown/Central retail core (extremely high restaurant intensity), (2) a mature regional centre in the East, (3) a large heartland town in the West, and (4) a newer North-East town. Together they cover distinct development histories, land-use mixes, and day–night populations, which strongly influence restaurant locations. Working at the planning-area/subzone scale also reduces boundary bias for local analyses while keeping results policy-relevant (these are common planning units). In what follows, we: extract the four planning areas, convert them to owin study windows required by spatstat, clip the Singapore-wide restaurant ppp to each window (defensive check), and rescale to kilometers (single canonical unit). These km-scaled point patterns become the inputs for KDE and any other first-order statistics i,kn the next subsections.\n\n# 1) Extract FOUR planning areas by name (edit names if our dataset differs) -----\npa_punggol   &lt;- dplyr::filter(mpsz_clean, PLN_AREA_N == \"PUNGGOL\")       # Punggol polygon(s)\npa_tampines  &lt;- dplyr::filter(mpsz_clean, PLN_AREA_N == \"TAMPINES\")      # Tampines polygon(s)\npa_orchard  &lt;- dplyr::filter(mpsz_clean, PLN_AREA_N == \"ORCHARD\")        # Orchard polygon(s)\npa_jurongw   &lt;- dplyr::filter(mpsz_clean, PLN_AREA_N == \"JURONG WEST\")   # Jurong West polygon(s)\n\n# 2) Convert each planning area polygon to a spatstat observation window (owin) --\nowin_punggol  &lt;- spatstat.geom::as.owin(pa_punggol)    # owin used for clipping & SPPA\nowin_tampines &lt;- spatstat.geom::as.owin(pa_tampines)   # owin for Tampines\nowin_orchard &lt;- spatstat.geom::as.owin(pa_orchard)     # owin for Orchard\nowin_jurongw  &lt;- spatstat.geom::as.owin(pa_jurongw)    # owin for Jurong West\n\n# 3) Clip the restaurants point pattern (meters) to each area's owin -------------\nrest_ppp_punggol_m  &lt;- rest_ppp[owin_punggol]          # keep only restaurants inside Punggol\nrest_ppp_tampines_m &lt;- rest_ppp[owin_tampines]         # keep only restaurants inside Tampines\nrest_ppp_orchard_m &lt;- rest_ppp[owin_orchard]           # keep only restaurants inside Orchard\nrest_ppp_jurongw_m  &lt;- rest_ppp[owin_jurongw]          # keep only restaurants inside Jurong West\n\n# 4) Rescale EACH clipped ppp from meters to kilometers (single canonical call) ---\nrest_ppp_punggol_km  &lt;- rescale.ppp(rest_ppp_punggol_m,  1000, \"km\")  # m → km for Punggol\nrest_ppp_tampines_km &lt;- rescale.ppp(rest_ppp_tampines_m, 1000, \"km\")  # m → km for Tampines\nrest_ppp_orchard_km &lt;- rescale.ppp(rest_ppp_orchard_m, 1000, \"km\")  # m → km for Orchard\nrest_ppp_jurongw_km  &lt;- rescale.ppp(rest_ppp_jurongw_m,  1000, \"km\")  # m → km for Jurong West\n\n# 5) Quick visual sanity check (optional) ----------------------------------------\n# par(mfrow = c(2,2))                                            # 2-by-2 panel layout\nplot(unmark(rest_ppp_orchard_km), main = \"Orchard (km)\")       # plot points within boundary (km)\n\n\n\n\n\n\n\nplot(unmark(rest_ppp_jurongw_km),  main = \"Jurong West (km)\")\n\n\n\n\n\n\n\nplot(unmark(rest_ppp_tampines_km), main = \"Tampines (km)\")  \n\n\n\n\n\n\n\nplot(unmark(rest_ppp_punggol_km),  main = \"Punggol (km)\") \n\n\n\n\n\n\n\npar(mfrow = c(1,1))                                         # reset layout\n\n# 6) Defensive checks on objects & units before tests/KDE -------------------------\nstopifnot(spatstat.geom::is.ppp(rest_ppp_punggol_km))   # confirm valid ppp in km\nstopifnot(spatstat.geom::is.ppp(rest_ppp_tampines_km))  # confirm valid ppp in km\nstopifnot(spatstat.geom::is.ppp(rest_ppp_orchard_km))  # confirm valid ppp in km\nstopifnot(spatstat.geom::is.ppp(rest_ppp_jurongw_km))   # confirm valid ppp in km\n\n# Inspect counts & windows to ensure clipping behaved as expected ------\nsummary(rest_ppp_orchard_km); summary(rest_ppp_jurongw_km)\n\nMarked planar point pattern:  25 points\nAverage intensity 26.10897 points per square km\n\nCoordinates are given to 14 decimal places\n\nMark variables: uen, issuance_agency_id, entity_name, entity_type_description, \nbusiness_constitution_description, company_type_description, \npaf_constitution_description, entity_status_description, date, uen_issue_date, \naddress_type, block, street_name, level_no, unit_no, building_name, \npostal_code, other_address_line1, other_address_line2, account_due_date, \nannual_return_date, primary_ssic_code, primary_ssic_description, \nprimary_user_described_activity, YEAR, MONTH_NUM, MONTH_ABBR, SEARCHVAL, \nBLK_NO, ROAD_NAME, BUILDING, ADDRESS, LATITUDE, LONGITUDE\nSummary:\n     uen            issuance_agency_id entity_name       \n Length:25          Length:25          Length:25         \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n entity_type_description business_constitution_description\n Length:25               Length:25                        \n Class :character        Class :character                 \n Mode  :character        Mode  :character                 \n                                                          \n                                                          \n                                                          \n                                                          \n company_type_description paf_constitution_description\n Length:25                Length:25                   \n Class :character         Class :character            \n Mode  :character         Mode  :character            \n                                                      \n                                                      \n                                                      \n                                                      \n entity_status_description      date            uen_issue_date      \n Length:25                 Min.   :2025-01-14   Min.   :2025-01-14  \n Class :character          1st Qu.:2025-02-18   1st Qu.:2025-02-18  \n Mode  :character          Median :2025-03-24   Median :2025-04-01  \n                           Mean   :2025-04-11   Mean   :2025-04-12  \n                           3rd Qu.:2025-06-23   3rd Qu.:2025-06-23  \n                           Max.   :2025-07-25   Max.   :2025-07-25  \n                                                                    \n address_type          block           street_name          level_no        \n Length:25          Length:25          Length:25          Length:25         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   unit_no          building_name      postal_code        other_address_line1\n Length:25          Length:25          Length:25          Length:25          \n Class :character   Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   Mode  :character   \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n other_address_line2 account_due_date   annual_return_date primary_ssic_code\n Length:25           Length:25          Length:25          Min.   :56111    \n Class :character    Class :character   Class :character   1st Qu.:56111    \n Mode  :character    Mode  :character   Mode  :character   Median :56111    \n                                                           Mean   :56111    \n                                                           3rd Qu.:56111    \n                                                           Max.   :56111    \n                                                                            \n primary_ssic_description primary_user_described_activity      YEAR     \n Length:25                Length:25                       Min.   :2025  \n Class :character         Class :character                1st Qu.:2025  \n Mode  :character         Mode  :character                Median :2025  \n                                                          Mean   :2025  \n                                                          3rd Qu.:2025  \n                                                          Max.   :2025  \n                                                                        \n   MONTH_NUM      MONTH_ABBR  SEARCHVAL            BLK_NO         \n Min.   :1.00   Feb    :6    Length:25          Length:25         \n 1st Qu.:2.00   Mar    :4    Class :character   Class :character  \n Median :3.00   Jun    :4    Mode  :character   Mode  :character  \n Mean   :3.88   Jul    :4                                         \n 3rd Qu.:6.00   Jan    :3                                         \n Max.   :7.00   Apr    :2                                         \n                (Other):2                                         \n  ROAD_NAME           BUILDING           ADDRESS            LATITUDE        \n Length:25          Length:25          Length:25          Length:25         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  LONGITUDE        \n Length:25         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWindow: polygonal boundary\nsingle connected closed polygon with 587 vertices\nenclosing rectangle: [26.814238, 29.102403] x [31.17827, 32.45574] km\n                     (2.288 x 1.277 km)\nWindow area = 0.957525 square km\nUnit of length: 1 km\nFraction of frame area: 0.328\n\n\nMarked planar point pattern:  17 points\nAverage intensity 1.157996 points per square km\n\nCoordinates are given to 14 decimal places\n\nMark variables: uen, issuance_agency_id, entity_name, entity_type_description, \nbusiness_constitution_description, company_type_description, \npaf_constitution_description, entity_status_description, date, uen_issue_date, \naddress_type, block, street_name, level_no, unit_no, building_name, \npostal_code, other_address_line1, other_address_line2, account_due_date, \nannual_return_date, primary_ssic_code, primary_ssic_description, \nprimary_user_described_activity, YEAR, MONTH_NUM, MONTH_ABBR, SEARCHVAL, \nBLK_NO, ROAD_NAME, BUILDING, ADDRESS, LATITUDE, LONGITUDE\nSummary:\n     uen            issuance_agency_id entity_name       \n Length:17          Length:17          Length:17         \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n entity_type_description business_constitution_description\n Length:17               Length:17                        \n Class :character        Class :character                 \n Mode  :character        Mode  :character                 \n                                                          \n                                                          \n                                                          \n                                                          \n company_type_description paf_constitution_description\n Length:17                Length:17                   \n Class :character         Class :character            \n Mode  :character         Mode  :character            \n                                                      \n                                                      \n                                                      \n                                                      \n entity_status_description      date            uen_issue_date      \n Length:17                 Min.   :2025-01-08   Min.   :2025-01-08  \n Class :character          1st Qu.:2025-02-17   1st Qu.:2025-02-17  \n Mode  :character          Median :2025-04-12   Median :2025-04-12  \n                           Mean   :2025-04-19   Mean   :2025-04-19  \n                           3rd Qu.:2025-06-11   3rd Qu.:2025-06-11  \n                           Max.   :2025-07-31   Max.   :2025-07-31  \n                                                                    \n address_type          block           street_name          level_no        \n Length:17          Length:17          Length:17          Length:17         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   unit_no          building_name      postal_code        other_address_line1\n Length:17          Length:17          Length:17          Length:17          \n Class :character   Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   Mode  :character   \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n other_address_line2 account_due_date   annual_return_date primary_ssic_code\n Length:17           Length:17          Length:17          Min.   :56111    \n Class :character    Class :character   Class :character   1st Qu.:56111    \n Mode  :character    Mode  :character   Mode  :character   Median :56111    \n                                                           Mean   :56111    \n                                                           3rd Qu.:56111    \n                                                           Max.   :56111    \n                                                                            \n primary_ssic_description primary_user_described_activity      YEAR     \n Length:17                Length:17                       Min.   :2025  \n Class :character         Class :character                1st Qu.:2025  \n Mode  :character         Mode  :character                Median :2025  \n                                                          Mean   :2025  \n                                                          3rd Qu.:2025  \n                                                          Max.   :2025  \n                                                                        \n   MONTH_NUM       MONTH_ABBR  SEARCHVAL            BLK_NO         \n Min.   :1.000   Feb    :4    Length:17          Length:17         \n 1st Qu.:2.000   Jul    :4    Class :character   Class :character  \n Median :4.000   Jan    :2    Mode  :character   Mode  :character  \n Mean   :4.176   Apr    :2                                         \n 3rd Qu.:6.000   May    :2                                         \n Max.   :7.000   Jun    :2                                         \n                 (Other):1                                         \n  ROAD_NAME           BUILDING           ADDRESS            LATITUDE        \n Length:17          Length:17          Length:17          Length:17         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  LONGITUDE        \n Length:17         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWindow: polygonal boundary\nsingle connected closed polygon with 356 vertices\nenclosing rectangle: [10.373179, 16.297184] x [33.9815, 38.48861] km\n                     (5.924 x 4.507 km)\nWindow area = 14.6805 square km\nUnit of length: 1 km\nFraction of frame area: 0.55\n\nsummary(rest_ppp_punggol_km); summary(rest_ppp_tampines_km)\n\nMarked planar point pattern:  5 points\nAverage intensity 0.5333744 points per square km\n\nCoordinates are given to 14 decimal places\n\nMark variables: uen, issuance_agency_id, entity_name, entity_type_description, \nbusiness_constitution_description, company_type_description, \npaf_constitution_description, entity_status_description, date, uen_issue_date, \naddress_type, block, street_name, level_no, unit_no, building_name, \npostal_code, other_address_line1, other_address_line2, account_due_date, \nannual_return_date, primary_ssic_code, primary_ssic_description, \nprimary_user_described_activity, YEAR, MONTH_NUM, MONTH_ABBR, SEARCHVAL, \nBLK_NO, ROAD_NAME, BUILDING, ADDRESS, LATITUDE, LONGITUDE\nSummary:\n     uen            issuance_agency_id entity_name       \n Length:5           Length:5           Length:5          \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n entity_type_description business_constitution_description\n Length:5                Length:5                         \n Class :character        Class :character                 \n Mode  :character        Mode  :character                 \n                                                          \n                                                          \n                                                          \n                                                          \n company_type_description paf_constitution_description\n Length:5                 Length:5                    \n Class :character         Class :character            \n Mode  :character         Mode  :character            \n                                                      \n                                                      \n                                                      \n                                                      \n entity_status_description      date            uen_issue_date      \n Length:5                  Min.   :2025-01-22   Min.   :2025-01-22  \n Class :character          1st Qu.:2025-01-27   1st Qu.:2025-01-27  \n Mode  :character          Median :2025-03-14   Median :2025-03-14  \n                           Mean   :2025-03-13   Mean   :2025-03-14  \n                           3rd Qu.:2025-03-20   3rd Qu.:2025-03-25  \n                           Max.   :2025-06-09   Max.   :2025-06-09  \n                                                                    \n address_type          block           street_name          level_no        \n Length:5           Length:5           Length:5           Length:5          \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   unit_no          building_name      postal_code        other_address_line1\n Length:5           Length:5           Length:5           Length:5           \n Class :character   Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   Mode  :character   \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n other_address_line2 account_due_date   annual_return_date primary_ssic_code\n Length:5            Length:5           Length:5           Min.   :56111    \n Class :character    Class :character   Class :character   1st Qu.:56111    \n Mode  :character    Mode  :character   Mode  :character   Median :56111    \n                                                           Mean   :56111    \n                                                           3rd Qu.:56111    \n                                                           Max.   :56111    \n                                                                            \n primary_ssic_description primary_user_described_activity      YEAR     \n Length:5                 Length:5                        Min.   :2025  \n Class :character         Class :character                1st Qu.:2025  \n Mode  :character         Mode  :character                Median :2025  \n                                                          Mean   :2025  \n                                                          3rd Qu.:2025  \n                                                          Max.   :2025  \n                                                                        \n   MONTH_NUM     MONTH_ABBR  SEARCHVAL            BLK_NO         \n Min.   :1.0   Jan    :2    Length:5           Length:5          \n 1st Qu.:1.0   Mar    :2    Class :character   Class :character  \n Median :3.0   Jun    :1    Mode  :character   Mode  :character  \n Mean   :2.8   Feb    :0                                         \n 3rd Qu.:3.0   Apr    :0                                         \n Max.   :6.0   May    :0                                         \n               (Other):0                                         \n  ROAD_NAME           BUILDING           ADDRESS            LATITUDE        \n Length:5           Length:5           Length:5           Length:5          \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  LONGITUDE        \n Length:5          \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWindow: polygonal boundary\nsingle connected closed polygon with 639 vertices\nenclosing rectangle: [33.95259, 38.88996] x [40.87437, 44.80879] km\n                     (4.937 x 3.934 km)\nWindow area = 9.37428 square km\nUnit of length: 1 km\nFraction of frame area: 0.483\n\n\nMarked planar point pattern:  15 points\nAverage intensity 0.7213124 points per square km\n\nCoordinates are given to 14 decimal places\n\nMark variables: uen, issuance_agency_id, entity_name, entity_type_description, \nbusiness_constitution_description, company_type_description, \npaf_constitution_description, entity_status_description, date, uen_issue_date, \naddress_type, block, street_name, level_no, unit_no, building_name, \npostal_code, other_address_line1, other_address_line2, account_due_date, \nannual_return_date, primary_ssic_code, primary_ssic_description, \nprimary_user_described_activity, YEAR, MONTH_NUM, MONTH_ABBR, SEARCHVAL, \nBLK_NO, ROAD_NAME, BUILDING, ADDRESS, LATITUDE, LONGITUDE\nSummary:\n     uen            issuance_agency_id entity_name       \n Length:15          Length:15          Length:15         \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n entity_type_description business_constitution_description\n Length:15               Length:15                        \n Class :character        Class :character                 \n Mode  :character        Mode  :character                 \n                                                          \n                                                          \n                                                          \n                                                          \n company_type_description paf_constitution_description\n Length:15                Length:15                   \n Class :character         Class :character            \n Mode  :character         Mode  :character            \n                                                      \n                                                      \n                                                      \n                                                      \n entity_status_description      date            uen_issue_date      \n Length:15                 Min.   :2025-01-16   Min.   :2025-01-16  \n Class :character          1st Qu.:2025-03-09   1st Qu.:2025-03-09  \n Mode  :character          Median :2025-05-10   Median :2025-05-10  \n                           Mean   :2025-04-21   Mean   :2025-04-21  \n                           3rd Qu.:2025-05-31   3rd Qu.:2025-05-31  \n                           Max.   :2025-07-14   Max.   :2025-07-14  \n                                                                    \n address_type          block           street_name          level_no        \n Length:15          Length:15          Length:15          Length:15         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   unit_no          building_name      postal_code        other_address_line1\n Length:15          Length:15          Length:15          Length:15          \n Class :character   Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   Mode  :character   \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n other_address_line2 account_due_date   annual_return_date primary_ssic_code\n Length:15           Length:15          Length:15          Min.   :56111    \n Class :character    Class :character   Class :character   1st Qu.:56111    \n Mode  :character    Mode  :character   Mode  :character   Median :56111    \n                                                           Mean   :56111    \n                                                           3rd Qu.:56111    \n                                                           Max.   :56111    \n                                                                            \n primary_ssic_description primary_user_described_activity      YEAR     \n Length:15                Length:15                       Min.   :2025  \n Class :character         Class :character                1st Qu.:2025  \n Mode  :character         Mode  :character                Median :2025  \n                                                          Mean   :2025  \n                                                          3rd Qu.:2025  \n                                                          Max.   :2025  \n                                                                        \n   MONTH_NUM       MONTH_ABBR  SEARCHVAL            BLK_NO         \n Min.   :1.000   May    :4    Length:15          Length:15         \n 1st Qu.:3.000   Jan    :2    Class :character   Class :character  \n Median :5.000   Mar    :2    Mode  :character   Mode  :character  \n Mean   :4.267   Apr    :2                                         \n 3rd Qu.:5.500   Jun    :2                                         \n Max.   :7.000   Jul    :2                                         \n                 (Other):1                                         \n  ROAD_NAME           BUILDING           ADDRESS            LATITUDE        \n Length:15          Length:15          Length:15          Length:15         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  LONGITUDE        \n Length:15         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWindow: polygonal boundary\nsingle connected closed polygon with 591 vertices\nenclosing rectangle: [38.19984, 44.8598] x [32.93355, 39.75273] km\n                     (6.66 x 6.819 km)\nWindow area = 20.7954 square km\nUnit of length: 1 km\nFraction of frame area: 0.458\n\n\nThe wrangled outputs highlight how restaurant distributions differ significantly across four contrasting planning areas in Singapore:\nOrchard, located within the central region, demonstrates the highest spatial intensity with 25 restaurants contained within a small area of 0.96 km², producing an average density exceeding 26 restaurants per km². This reflects Orchard’s role as a prime commercial and retail hub where food and beverage establishments cluster tightly to serve both local consumers and international visitors.\nJurong West, by comparison, records 17 restaurants within roughly 0.96 km², yielding a more moderate intensity of 1.16 per km². This suggests the presence of neighbourhood-level clusters that cater to local residential populations rather than destination-driven demand. `\nTampines shows 15 restaurants spread across 9.37 km², producing an intensity of 0.72 per km², which is consistent with its position as a regional centre offering balanced commercial and residential amenities.\nPunggol, a relatively new residential town, records only five restaurants within 9.37 km², resulting in the lowest intensity of 0.53 per km². This contrast highlights clear urban hierarchies, where maturity of development, population density, and land-use composition strongly shape restaurant clustering. The outputs confirm that spatial intensity patterns correspond closely to each area’s developmental history and functional role.\n\n\n9.6.2 Computing KDE surfaces by planning Area\nBy converting discrete restaurant locations into continuous density surfaces, KDE helps identify hotspots and highlight differences in spatial intensity across planning areas. At this stage, we focus on computing KDE surfaces separately for selected planning areas of Singapore, instead of for the entire island. This allows us to explore variations in restaurant distribution within localised study areas and compare across subzones. Importantly, the KDE computation uses a consistent bandwidth estimator, ensuring results are comparable across regions. Following the same structure and coding style as earlier sections, we use Diggle’s automatic bandwidth selection method (bw.diggle) and apply the Gaussian kernel to maintain methodological consistency. All computations are carried out in kilometers, rather than meters, since this scale provides results that are interpretable and aligned with urban planning practices. This ensures KDE values represent the density of restaurants per square kilometer, a unit meaningful for evaluating clustering and intensity at the planning area level.\n\n# Orchard\nplot(\n  density(rest_ppp_orchard_km,\n          sigma = bw.diggle,\n          edge = TRUE,\n          kernel = \"gaussian\"),\n  main = \"Orchard\"\n)\n\n\n\n\n\n\n\n# Jurong West\nplot(\n  density(rest_ppp_jurongw_km,\n          sigma = bw.diggle,\n          edge = TRUE,\n          kernel = \"gaussian\"),\n  main = \"Jurong West\"\n)\n\n\n\n\n\n\n\n# Tampines\nplot(\n  density(rest_ppp_tampines_km,\n          sigma = bw.diggle,\n          edge = TRUE,\n          kernel = \"gaussian\"),\n  main = \"Tampines\"\n)\n\n\n\n\n\n\n\n# Punggol\nplot(\n  density(rest_ppp_punggol_km,\n          sigma = bw.diggle,\n          edge = TRUE,\n          kernel = \"gaussian\"),\n  main = \"Punggol\"\n)\n\n\n\n\n\n\n\n\nThe KDE plots for Orchard, Jurong West, Tampines, and Punggol illustrate clear contrasts in restaurant clustering across different planning areas. Orchard stands out with extremely high intensities exceeding 30,000 restaurants per km² in certain hotspots, reflecting its role as Singapore’s premier retail and F&B destination. Jurong West shows multiple localised clusters, with peak intensities above 35, indicating neighbourhood-scale concentrations around commercial nodes. Tampines displays moderate but spatially dispersed clusters with values around 3 per km², consistent with its character as a regional centre serving a large residential population. Punggol, a newer town, has fewer restaurants overall, with peaks around 5 per km² concentrated in limited pockets, revealing its emerging but still sparse food landscape.\nThese differences confirm that KDE is effective in capturing the spatial heterogeneity of restaurant intensity across planning areas. Orchard exemplifies centralised, high-density clustering driven by its urban function, while Tampines and Jurong West illustrate more moderate but significant suburban nodes. Punggol lags behind, with limited commercial clustering that reflects its developmental stage. Together, these outputs demonstrate how KDE surfaces can reveal meaningful variations in urban foodscapes, linking land-use intensity, urban maturity, and service accessibility to spatial patterns of F&B establishments.\n\n\n\n9.7 Spatio-temporal KDE (STKDE) for Jan–Jun 2025\nIn this section, we bring together the spatial and temporal dimensions of restaurant incorporations into a unified spatio-temporal analysis using STKDE. The period of focus is January to June 2025, and the primary aim is to detect not only where incorporations cluster in space but also how these clusters evolve across time. The section begins with exploratory faceted maps that simply display the geographic distribution of restaurants month by month, offering a descriptive baseline. This is followed by computational STKDE at the monthly scale, where incorporation months are treated as temporal marks and used to generate a three-dimensional density surface across space (x, y) and time (t). Recognising that monthly aggregation can mask short-lived bursts of activity, the section then extends to day-level STKDE. By applying bootstrap bandwidth optimisation, the analysis achieves a careful balance between spatial and temporal smoothing, ensuring that density estimates capture meaningful patterns rather than artefacts of parameter choice. Finally, the section emphasises validation and interpretability, producing fixed-scale comparative panels that allow researchers to trace how hotspots intensify, persist, or dissipate across the study window. Overall, Section 9.7 transforms raw incorporation points into a dynamic surface that reveals the tempo and geography of restaurant entry, providing richer insight than purely spatial or purely temporal methods could offer in isolation.\n\n9.7.1 Visualising geographic distribution by month\nBefore advancing into formal spatio-temporal density estimation, the data is first explored visually to understand how restaurant incorporations are spread across Singapore from January to June 2025. Using the tmap package in static plot mode, individual incorporations are represented as dots, layered over the national planning boundary (sg_owin) to ensure geographic consistency. By applying tm_facets (by = “MONTH_ABBR”), the output generates a panel of maps, one for each month, which collectively illustrate how point distributions evolve across time. This faceted approach does not yet quantify clustering but serves as an intuitive diagnostic, highlighting months with visibly denser patterns or more dispersed incorporations. Such side-by-side visualisation is valuable for detecting preliminary trends—for example, whether activity is consistently concentrated in the central business districts or if new hotspots are emerging in suburban planning areas. As an exploratory step, these maps provide a baseline context that informs and motivates the more rigorous spatio-temporal KDE analysis developed later in the section.\n\ntmap::tmap_mode(\"plot\")\n\nstudy_owin_sf &lt;- sf::st_as_sf(sg_owin)\nsf::st_crs(study_owin_sf) &lt;- 3414  # SVY21\n\ntm_shape(study_owin_sf) +\n  tm_polygons(col = NA, border.col = \"grey70\", lwd = 0.4) +\ntm_shape(biz_56111_sf) +\n  tm_dots(size = 0.2, fill = \"red\") +\n  tm_facets(by = \"MONTH_ABBR\", free.coords = FALSE, drop.units = TRUE)\n\n\n\n\n\n\n\nstudy_owin_sf\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21448.47 xmax: 55941.94 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n                            geom\n1 MULTIPOLYGON (((31835.86 46...\n\n\nLooking at the monthly distribution of new restaurant openings between January and July, several clear spatial patterns emerge from visual inspection of the plots. In January, there is a tightly packed concentration of establishments in the central-southern corridor, particularly around the Orchard–Downtown Core region. The clustering appears compact, with points situated very close to one another, suggesting a strong central hotspot. In February, the pattern remains similar, with a dense grouping in the south, though the spread toward the east appears slightly more noticeable.\nBy March, the distribution shows some diffusion, with points appearing across the island, but the central cluster continues to dominate visually. April differs by showing a wider spread of points, with openings extending further into the northern and eastern areas. While the central region remains active, the clustering looks less compact, reflecting broader spatial coverage rather than dense concentration.\nIn May and June, the central clustering reasserts itself, with a visible grouping of restaurants again emerging in the south. June, in particular, demonstrates a mix of dense clustering and moderate spread. Finally, in July, the pattern thins slightly, but central clustering persists, though less intense compared to January.\nOverall, the visuals highlight a persistent central hotspot, alongside varying levels of island-wide spread month to month.\n\n\n9.7.2 STKDE by month\nHere, the analysis moves from descriptive visualization to computational STKDE, aggregated by month. The dataset is first structured so that the month of incorporation (1–12) is explicitly treated as a time mark, ensuring compatibility with the spatstat framework. The data is then converted to a planar point pattern process (PPP) with spatial coordinates expressed in meters, and subsequently rescaled into kilometers for consistency with the rest of the study. Each restaurant’s month of incorporation is treated as a temporal event, which is clipped to the global study window to avoid edge effects. The spattemp.density() function then computes the STKDE, estimating how restaurant incorporations are distributed simultaneously across space and time. The outcome is a three-dimensional density object (x, y, t), where t represents month. This monthly STKDE provides a fine-grained temporal lens, enabling researchers to investigate whether restaurant hotspots emerge gradually, persist across multiple months, or appear suddenly, potentially linked to policy interventions or seasonal demand.\n\n# 1) Keep ONLY the month mark (1..12) with geometry (still meters here)\nrest_month &lt;- biz_56111_sf |&gt;\n  dplyr::select(MONTH_NUM)\n\n# 2) Convert to ppp (meters); MONTH_NUM travels along as marks\nrest_month_ppp &lt;- spatstat.geom::as.ppp(rest_month)\n\n# 3) Clip to the global observation window you used in §9\nrest_month_owin &lt;- rest_month_ppp[sg_owin]\n\n# 4) Rescale to kilometers (to stay consistent with §9)\nrest_month_ppp_km &lt;- spatstat.geom::rescale(rest_month_owin, 1000, \"km\")\n\n# --- CRITICAL: ensure time marks are a plain numeric vector (not a data.frame/factor)\nmks &lt;- marks(rest_month_ppp_km)\nmonth_vec &lt;- if (is.data.frame(mks)) mks$MONTH_NUM else mks\nstopifnot(is.numeric(month_vec))\nrest_month_ppp_km &lt;- spatstat.geom::setmarks(rest_month_ppp_km, as.numeric(month_vec))\n\n# Quick sanity: months present should be inside 1..12\nrange(month_vec, na.rm = TRUE)\n\n[1] 1 7\n\ntable(month_vec)\n\nmonth_vec\n  1   2   3   4   5   6   7 \n102  96 110  97  93 100  79 \n\n# 5) STKDE using sparr (let it pick sensible spatial defaults; specify time grid)\n#    tres = number of distinct months we actually have; tlim bounds the months.\nuniq_months &lt;- sort(unique(month_vec))\nnt &lt;- length(uniq_months)\n\nst_kde_m &lt;- sparr::spattemp.density(\n  rest_month_ppp_km,\n  sres  = 128,                      # spatial grid like the notes\n  tres  = nt,                        # number of time slices\n  tlim  = range(uniq_months, na.rm = TRUE)  # temporal span (e.g., 1..7 in your data)\n)\n\n# Confirm we now have a 3D object (you should see ... x ... x nt)\nsummary(st_kde_m)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 1.5751 (spatial)\n  lambda = 0.0393 (temporal)\n\nNo. of observations\n  677 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [2.667538, 55.94194] x [21.44847, 50.25633]\n\nTemporal bound\n  [1, 7]\n\nEvaluation\n  128 x 128 x 7 trivariate lattice\n  Density range: [7.660098e-51, 0.02162844]\n\n\n\n# 6) Map actual month numbers -&gt; internal time indices that spattemp.density created\n#    st_kde_m$z$t holds the time coordinate; if NULL (very rare), build a seq of length nt.\nt_axis &lt;- st_kde_m$z$t\nif (is.null(t_axis)) t_axis &lt;- seq_len(nt)\n\nt_idx  &lt;- match(uniq_months, t_axis)\nkeep   &lt;- !is.na(t_idx)\nt_idx  &lt;- t_idx[keep]\nlab_months &lt;- month.abb[uniq_months][keep]\n\n# 7) Plot every month with a fixed legend range (prof’s “comparable panels” habit)\nop &lt;- par(mfcol = c(2, ceiling(length(t_idx)/2)))  # 2×? grid; adjust if you prefer 3×?\nfor (i in seq_along(t_idx)) {\n  plot(\n    st_kde_m, t_idx[i],\n    override.par = FALSE,\n    fix.range    = TRUE,\n    main = paste(\"KDE at\", lab_months[i])\n  )\n}\npar(op)\n\n\n\n\n\n\n\n\nThe STKDE surfaces provide a dynamic perspective on restaurant intensity across Singapore’s planning areas from January to July 2025. Unlike the static first-order SPPA, which offered a cumulative view of clustering, STKDE reveals how concentration shifts month by month.\nIn the early months, February and March exhibit diffuse patterns with low overall intensity, indicating that restaurant registrations were spread broadly across several planning areas rather than concentrated in one location. April marks a sharp surge in activity, with central planning areas registering the highest intensity, exceeding 0.02 restaurants/km². This confirms the first-order result that central regions are the primary clusters but clarifies that the peak was temporally concentrated in April. Activity then disperses in May and June, with moderate levels appearing in eastern and western planning areas such as Tampines and Jurong West, suggesting a decentralization of openings. By July, the overall intensity weakens further, reflecting fewer new registrations across the island and confirming that April’s central spike was an anomaly rather than a sustained trend.\nImportantly, these outputs are not counts of restaurants but model-derived intensities. The kernel estimator smooths each restaurant registration across space and time according to the selected bandwidths. As a result, the maps report expected density in restaurants per km² per day, not literal fractions of a restaurant, providing a probabilistic representation of clustering strength over space-time.\nIn summary, STKDE supports the first-order SPPA finding of central dominance but introduces essential temporal nuance: clustering was episodic, peaking in April, while other planning areas intermittently absorbed activity. This demonstrates the value of spatio-temporal methods in distinguishing short-lived bursts from longer-term geographic concentration.\n\n\n9.7.3 STKDE by day-of-year (improved bandwidths via BOOT.spattemp)\nThis section advances the STKDE analysis by refining temporal granularity to the level of individual days and improving smoothing parameters through bootstrap bandwidth selection. The day-of-year (1–366) is extracted from each incorporation date, ensuring temporal precision beyond monthly aggregation. The dataset is then processed into a PPP, rescaled to kilometers, and prepared with appropriate temporal marks. A critical step here is bandwidth optimisation: instead of relying on arbitrary or fixed values, the bootstrap method (BOOT.spattemp) identifies data-driven spatial (\\(h\\)) and temporal (\\(\\lambda\\)) bandwidths that balance bias and variance. This ensures that the resulting STKDE surface captures meaningful clustering patterns without over-smoothing fine details or exaggerating noise. Once the optimised parameters are selected, the spatio-temporal density is computed, and slices across different days can be visualised. This approach provides a much more dynamic and precise picture of incorporation activity, capable of highlighting short-lived bursts of restaurant openings that monthly aggregation might obscure, offering richer insight into temporal clustering behaviour.\n\n# 1) Build a temp sf with just the Day-of-Year mark (1..366), keep geometry\nrest_yday &lt;- biz_56111_sf |&gt;\n  dplyr::transmute(YDAY_NUM = lubridate::yday(date))\n\n# 2) Convert to ppp (meters) – the mark travels along\nrest_yday_ppp &lt;- spatstat.geom::as.ppp(rest_yday)\n\n# 3) Clip to the same global observation window used in §9\nrest_yday_owin &lt;- rest_yday_ppp[sg_owin]\n\n# 4) Rescale to kilometers (consistent with the rest of §9)\nrest_yday_ppp_km &lt;- spatstat.geom::rescale(rest_yday_owin, 1000, \"km\")\n\n# 5) SAFETY: ensure the time mark is a plain numeric vector inside the ppp\nmks &lt;- marks(rest_yday_ppp_km)\nyday_vec &lt;- if (is.data.frame(mks)) mks$YDAY_NUM else mks\nstopifnot(is.numeric(yday_vec))\nrest_yday_ppp_km &lt;- spatstat.geom::setmarks(rest_yday_ppp_km, as.numeric(yday_vec))\n\n# Quick checks like in the notes\nrange(yday_vec, na.rm = TRUE)      # should be within 1..366 subset\n\n[1]   1 212\n\nlength(unique(yday_vec))           # number of distinct days actually present\n\n[1] 189\n\n# 6) Select bandwidths via bootstrap (professor’s method)\nset.seed(1234)  # reproducibility\nboot_bw &lt;- sparr::BOOT.spattemp(rest_yday_ppp_km)  # let sparr choose; no extra args\n\nInitialising...Done.\nOptimising...\nh = 1.575134 \b; lambda = 18.76458 \nh = 3.451592 \b; lambda = 18.76458 \nh = 1.575134 \b; lambda = 20.64104 \nh = 2.513363 \b; lambda = 19.2337 \nh = 0.636905 \b; lambda = 20.17192 \nh = 2.044249 \b; lambda = 19.46825 \nh = 1.10602 \b; lambda = 19.93737 \nh = 0.636905 \b; lambda = 20.17192 \nh = 1.10602 \b; lambda = 21.81383 \nh = 0.8714623 \b; lambda = 23.33845 \nh = 0.636905 \b; lambda = 21.11015 \nh = 1.340577 \b; lambda = 20.75832 \nh = 1.340577 \b; lambda = 22.63478 \nh = 1.457855 \b; lambda = 23.98348 \nh = 1.575134 \b; lambda = 21.57927 \nh = 1.223298 \b; lambda = 21.75519 \nh = 1.223298 \b; lambda = 23.63164 \nh = 1.164659 \b; lambda = 25.06831 \nh = 1.281937 \b; lambda = 25.9479 \nh = 1.311257 \b; lambda = 28.04425 \nh = 1.135339 \b; lambda = 30.47778 \nh = 1.03272 \b; lambda = 34.39929 \nh = 1.179319 \b; lambda = 37.37523 \nh = 1.186649 \b; lambda = 43.5287 \nh = 0.9081118 \b; lambda = 49.88373 \nh = 0.7065392 \b; lambda = 60.80347 \nh = 1.06204 \b; lambda = 59.01314 \nh = 1.0767 \b; lambda = 71.32007 \nh = 0.7981631 \b; lambda = 77.6751 \nh = 0.8952845 \b; lambda = 69.1385 \nh = 1.063873 \b; lambda = 90.57484 \nh = 1.141753 \b; lambda = 110.9204 \nh = 1.245288 \b; lambda = 92.7564 \nh = 0.9827854 \b; lambda = 75.04298 \nh = 0.969958 \b; lambda = 94.29775 \nh = 0.9165871 \b; lambda = 105.7866 \nh = 0.9976742 \b; lambda = 121.3184 \nh = 1.005119 \b; lambda = 144.4562 \nh = 0.8503888 \b; lambda = 136.5302 \nh = 1.010502 \b; lambda = 102.0637 \nh = 1.091589 \b; lambda = 117.5955 \nh = 0.9603375 \b; lambda = 108.7388 \nh = 0.9475101 \b; lambda = 127.9936 \nh = 0.9160144 \b; lambda = 140.9585 \nh = 0.9101734 \b; lambda = 115.414 \nh = 0.975799 \b; lambda = 119.8423 \nh = 0.9629717 \b; lambda = 139.0971 \nh = 0.960996 \b; lambda = 116.3284 \nh = 0.9892849 \b; lambda = 108.1771 \nh = 0.9579538 \b; lambda = 123.0395 \nh = 0.9727568 \b; lambda = 126.5534 \nh = 0.9639362 \b; lambda = 118.8846 \nh = 0.946091 \b; lambda = 122.0818 \nh = 0.968372 \b; lambda = 120.4022 \nh = 0.9743544 \b; lambda = 116.2474 \nh = 0.962054 \b; lambda = 121.3414 \nh = 0.9664898 \b; lambda = 122.859 \nh = 0.9645746 \b; lambda = 119.8782 \nh = 0.9708927 \b; lambda = 118.939 \nh = 0.9642637 \b; lambda = 120.7408 \nh = 0.9604662 \b; lambda = 120.2169 \nh = 0.9663956 \b; lambda = 120.3559 \nh = 0.9660846 \b; lambda = 121.2185 \nh = 0.9649521 \b; lambda = 120.2133 \nh = 0.967084 \b; lambda = 119.8283 \nh = 0.9649688 \b; lambda = 120.5127 \nh = 0.9635253 \b; lambda = 120.3701 \nh = 0.965678 \b; lambda = 120.3594 \nh = 0.9656946 \b; lambda = 120.6588 \nh = 0.965509 \b; lambda = 120.5475 \nh = 0.9647998 \b; lambda = 120.7007 \nh = 0.9654584 \b; lambda = 120.4448 \nh = 0.9659987 \b; lambda = 120.4795 \nh = 0.9652262 \b; lambda = 120.5044 \nh = 0.9651757 \b; lambda = 120.4017 \nh = 0.9654257 \b; lambda = 120.511 \nDone.\n\n# --- Robust extraction: works for both list-return and numeric vector-return\nh_opt &lt;- tryCatch(boot_bw$h.opt, error = function(e) NULL)\nlambda_opt &lt;- tryCatch(boot_bw$lambda.opt, error = function(e) NULL)\nif (is.null(h_opt) || is.null(lambda_opt)) {\n  vals &lt;- as.numeric(boot_bw)\n  stopifnot(length(vals) &gt;= 2)\n  h_opt      &lt;- vals[1]\n  lambda_opt &lt;- vals[2]\n}\ncat(\"Selected h (km):\", h_opt, \"  lambda (days):\", lambda_opt, \"\\n\")\n\nSelected h (km): 0.9652262   lambda (days): 120.5044 \n\n# 7) Compute the STKDE using the selected bandwidths\nkde_yday &lt;- sparr::spattemp.density(\n  rest_yday_ppp_km,\n  h      = h_opt,\n  lambda = lambda_opt\n)\n\n# Inspect (should report the t-domain and grid)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 0.9652 (spatial)\n  lambda = 120.5044 (temporal)\n\nNo. of observations\n  677 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [2.667538, 55.94194] x [21.44847, 50.25633]\n\nTemporal bound\n  [1, 212]\n\nEvaluation\n  128 x 128 x 212 trivariate lattice\n  Density range: [5.122269e-48, 0.0001246886]\n\n\n\n# 8a) Plot a specific day-of-year slice (like the notes). Example: t = 10\n# plot(kde_yday)        # default panel (slider-like)\n# plot(kde_yday, 10)    # explicitly show t = 10 (any valid index 1..nt)\n\n# 8b) Optional: evenly spaced days panel with fixed legend (robust to structure)\n# Try to get the time axis from the object; fall back to cube depth; then to data\nt_axis &lt;- kde_yday$z$t\nif (is.null(t_axis)) {\n  nt_from_v &lt;- tryCatch({\n    vv &lt;- kde_yday$z$v\n    d  &lt;- dim(vv)\n    if (length(d) &gt;= 3) d[3] else NA_integer_\n  }, error = function(e) NA_integer_)\n  if (is.na(nt_from_v)) {\n    nt_from_v &lt;- length(unique(yday_vec))  # last resort: how many days we fed in\n  }\n  t_axis &lt;- seq_len(nt_from_v)\n}\n\n# Guard: if we still don’t have any time slices, stop with a clear message\nif (length(t_axis) &lt; 1L) {\n  stop(\"STKDE object has zero time slices (check marks/time input and BOOT bandwidth step).\")\n}\n\n# Build up to 6 evenly spaced slice indices across the available t-axis\nk &lt;- min(6L, length(t_axis))\nt_show &lt;- unique(round(seq(from = 1, to = length(t_axis), length.out = k)))\nt_show &lt;- t_show[t_show &gt;= 1 & t_show &lt;= length(t_axis)]  # extra safety\n\n# Plot with a fixed legend range so panels are comparable\nop &lt;- par(mfcol = c(2, ceiling(length(t_show)/2)))  # 2×3 grid when 6 slices\nfor (ti in t_show) {\n  plot(\n    kde_yday, ti,\n    override.par = FALSE,\n    fix.range    = TRUE,\n    main = paste(\"KDE at t =\", ti)\n  )\n}\n\n\n\n\n\n\n\npar(op)\n\nThe spatio-temporal kernel density estimation (STKDE) by day-of-year provides a refined view of restaurant openings across Singapore during the first half of 2025. Using a spatial bandwidth (\\(h\\)) of approximately 0.97 km and a temporal bandwidth (\\(\\lambda\\)) of about 120.5 days, the model smooths individual registrations over space and time, producing estimates on a 128 × 128 × 212 lattice. This fine resolution allows clustering patterns to be examined at daily granularity rather than through broader monthly aggregation. The estimated intensity values range between \\(5.3 \\times 10^{-48}\\)), representing effective background zero, and \\(1.25 \\times 10^{-4}\\) restaurants per \\(km^2\\) per day, representing maximum hotspot strength. These outputs capture expected densities, not literal counts, and indicate the probability surface of clustering.\nAcross all temporal slices (t = 1, 39, 76, 114, 151, 189), the results consistently highlight a persistent hotspot in central Singapore, most clearly associated with the Orchard and Downtown Core areas. The hotspot is visible in every slice, demonstrating that clustering is not sporadic or shifting but stable and spatially anchored in the central planning region. No secondary hotspots emerge in suburban or peripheral areas such as Jurong West, Tampines, or Punggol.\nThe insight is that restaurant openings during this period are highly persistent in central locations, reinforcing the dominance of established commercial zones. Temporal variation in intensity is modest, but the spatial persistence underscores entrenched market preference for core areas."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#second-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex01/take-home_ex01.html#second-order-spatial-point-patterns-analysis",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "10 Second-order spatial point patterns analysis",
    "text": "10 Second-order spatial point patterns analysis\nWhile first-order analysis (Section 9) identified where restaurant registrations were most intense, it does not tell us whether those hotspots are statistically significant or simply a result of background density. To address this, we perform second-order SPPA, which examines how the relative positions of points deviate from CSR.\nIn practice, among the four classical second-order functions (\\(G\\), \\(F\\), \\(K\\), \\(L\\)), the \\(K\\)-function and its variance-stabilised form, the \\(L\\)-function, are most widely adopted. They are preferred because they capture multi-scale clustering and dispersion across a range of distances, while \\(G\\) and \\(F\\) are primarily sensitive to nearest-neighbour effects. We therefore focus our analysis on \\(K\\) and \\(L\\) functions, supported by Monte Carlo CSR testing using 99/999 simulations.\n\n10.1 Analysing apatial point process using \\(K\\)-function\nThe \\(K\\)-function is a second-order spatial point process statistic that measures the number of neighbouring events within a given distance of each observed point and compares the result to a theoretical expectation under CSR. Unlike first-order methods such as kernel density estimation that highlight intensity variations across space, the \\(K\\)-function examines spatial dependence between points, thereby detecting clustering or dispersion at multiple scales. In this exercise, the analysis is applied to the restaurant dataset filtered for the period January to June 2025, focusing on Tampines and Punggol planning areas. The \\(K\\)-function is estimated for each subregion and tested against a CSR benchmark using 999 Monte Carlo simulations to generate significance envelopes. By comparing the observed function with the simulation envelope, the analysis reveals whether the observed distribution of restaurants reflects random processes, significant clustering, or spatial regularity. This approach is crucial because it provides statistical evidence to support visual impressions from density maps and allows the detection of spatial interactions across a range of distances. Ultimately, the \\(K\\)-function offers deeper insights into how restaurants are spatially distributed in different planning contexts and whether their locations are shaped by systematic clustering forces or can be explained by chance alone.\n\n# K for Orchard\nK_or      &lt;- Kest(rest_ppp_orchard_km,  correction = \"Ripley\")\nK_or.csr  &lt;- envelope(rest_ppp_orchard_km,  Kest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(K_or.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"K(d) - r\",\n     main = \"K-function with CSR envelope — Orchard (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# K for Jurong West\nK_jw      &lt;- Kest(rest_ppp_jurongw_km,  correction = \"Ripley\")\nK_jw.csr  &lt;- envelope(rest_ppp_jurongw_km,  Kest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(K_jw.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"K(d) - r\",\n     main = \"K-function with CSR envelope — Jurong West (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# K for Tampines (reusing Sect. 9 PPP)\nK_tm      &lt;- Kest(rest_ppp_tampines_km, correction = \"Ripley\")\nK_tm.csr  &lt;- envelope(rest_ppp_tampines_km, Kest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(K_tm.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"K(d) - r\",\n     main = \"K-function with CSR envelope — Tampines (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# K for Punggol\nK_pg      &lt;- Kest(rest_ppp_punggol_km,  correction = \"Ripley\")\nK_pg.csr  &lt;- envelope(rest_ppp_punggol_km,  Kest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(K_pg.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"K(d) - r\",\n     main = \"K-function with CSR envelope — Punggol (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n\nThe second-order analysis using Ripley’s \\(K\\)-function provides important insights into how restaurant establishments are distributed across different planning areas of Singapore between January and June 2025. By comparing the observed distribution of restaurants against CSR envelopes, the analysis highlights whether clustering or dispersion dominates at neighborhood scales.\nFor Orchard, the observed \\(K\\)-function remains almost entirely within the CSR envelope across the evaluated distance range of 0 – 0.3 km. At very short distances (\\(&lt; 0.15 km\\)), the curve dips slightly below the CSR expectation, suggesting weak local inhibition, where restaurants appear marginally more spaced out than expected under randomness. However, this deviation is minor and does not persist. Beyond 0.15 km, the curve converges back toward the CSR line, indicating no strong evidence of clustering. These results align with Orchard’s role as a mature, high-density commercial area, where planning controls and established tenancy structures may limit excessive agglomeration at micro-scales.\nIn contrast, Jurong West demonstrates marginal clustering. The observed curve consistently rises above the CSR expectation and approaches the upper bounds of the simulation envelope across distances up to ~1.2 km. This indicates that restaurants in Jurong West tend to co-locate more than would be expected under randomness, reflecting localized agglomeration effects. Such clustering could be driven by town-center development, transport nodes, or concentrated demand in an expanding residential population.\nFor Tampines, the observed curve closely tracks the CSR expectation and remains well within the envelope over distances from 0 to 1.6 km. The absence of significant deviation suggests that the distribution of restaurants here is broadly consistent with spatial randomness, neither clustering nor dispersing in a systematic manner. This even spread reflects balanced urban planning, where amenities are distributed across the planning area to serve residential neighborhoods efficiently.\nFinally, Punggol shows a more ambiguous pattern. At short distances (&lt;0.3 km), the curve dips below CSR expectation, hinting at weak repulsion, but rises above CSR between 0.4 and 1.0 km, which may suggest clustering at sub-kilometer scales. However, a critical limitation is that the CSR envelope is missing for this plot, making statistical significance impossible to confirm. Visual inspection alone is insufficient, and further analysis using the L-function with envelopes is recommended to validate potential clustering in this emerging town.\nOverall, the comparison underscores the diversity of spatial dynamics: Orchard shows near randomness with slight inhibition, Jurong West exhibits strong clustering, Tampines reflects balanced distribution, and Punggol remains inconclusive without full statistical testing. These findings emphasize the importance of using envelopes and complementary methods to robustly interpret spatial point patterns.\n\n\n10.2 Analysing spatial point process using \\(L\\)-function\nThe \\(L\\)-function is a variance-stabilised transformation of the \\(K\\)-function that provides a more interpretable measure of spatial point pattern dependence across different distance scales. While the \\(K\\)-function often grows rapidly with increasing distance, making it difficult to compare observed and expected values directly, the \\(L\\)-function linearises the expectation under CSR, such that deviations above or below the theoretical line can be more clearly identified. In this study, the \\(L\\)-function is applied to restaurant locations in Tampines and Punggol for the period January to June 2025, using the same point process objects generated in earlier sections. Monte Carlo simulations with 999 iterations are performed to construct CSR envelopes, providing a rigorous statistical test of spatial randomness at multiple scales. The \\(L\\)-function plots make it possible to distinguish between significant clustering and dispersion patterns with greater clarity compared to the raw K-function, especially at shorter distances where statistical fluctuations are most pronounced. By applying this method, the analysis can reveal not only whether clustering exists, but also the spatial range over which it occurs, offering deeper insights into how restaurant distributions reflect underlying urban design and commercial activity patterns in both new town and regional centre contexts.\n\n# L for Orchard\nL_or      &lt;- Lest(rest_ppp_orchard_km, correction = \"Ripley\")\nL_or.csr  &lt;- envelope(rest_ppp_orchard_km, Lest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(L_or.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"L(d) - r\",\n     main = \"L-function with CSR envelope — Orchard (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# L for Jurong West\nL_jw      &lt;- Lest(rest_ppp_jurongw_km, correction = \"Ripley\")\nL_jw.csr  &lt;- envelope(rest_ppp_jurongw_km, Lest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(L_jw.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"L(d) - r\",\n     main = \"L-function with CSR envelope — Jurong West (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# L for Tampines\nL_tm      &lt;- Lest(rest_ppp_tampines_km, correction = \"Ripley\")\nL_tm.csr  &lt;- envelope(rest_ppp_tampines_km, Lest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(L_tm.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"L(d) - r\",\n     main = \"L-function with CSR envelope — Tampines (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n# L for Punggol\nL_pg      &lt;- Lest(rest_ppp_punggol_km, correction = \"Ripley\")\nL_pg.csr  &lt;- envelope(rest_ppp_punggol_km, Lest, nsim = 999, rank = 1, glocal = TRUE)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(L_pg.csr, . - r ~ r,\n     xlab = \"Distance (km)\", ylab = \"L(d) - r\",\n     main = \"L-function with CSR envelope — Punggol (Jan–Jun 2025)\")\n\n\n\n\n\n\n\n\nThe application of the \\(L\\)-function with CSR envelopes provides deeper insight into the spatial arrangement of restaurants across Orchard, Jurong West, Tampines, and Punggol between January and June 2025. By linearising the \\(K\\)-function expectation, the \\(L\\)-function makes it easier to discern whether restaurant distributions follow CSR, or whether clustering or dispersion tendencies dominate at specific spatial scales.\nFor Orchard, the observed \\(L(d) – r\\) curve remains tightly bounded within the CSR envelope across the full range of distances considered (0–0.3 km). The curve oscillates only slightly around the theoretical CSR line, with no systematic departures above or below. This finding indicates that restaurant locations in Orchard do not exhibit statistically significant clustering or dispersion. Instead, they are distributed in a manner that aligns with spatial randomness. Given Orchard’s status as a dense, highly regulated commercial core, this result is consistent with deliberate urban planning that balances accessibility with saturation, ensuring even service distribution within a small and competitive district.\nIn Jurong West, the \\(L\\)-function suggests a more dynamic pattern. While the observed curve remains within the CSR simulation envelope, positive deviations emerge at mid-range distances (0.4 – 0.8 km). These upward departures, although not statistically conclusive, hint at weak clustering tendencies at the neighbourhood scale. Such patterns could reflect the pull of transport hubs, shopping malls, or residential-commercial nodes that naturally attract several establishments in proximity. Nevertheless, because the curve never clearly exits the CSR band, the clustering tendency remains suggestive rather than definitive. Jurong West therefore presents a case of emerging but not strongly validated agglomeration.\nThe results for Tampines reveal perhaps the strongest case of spatial randomness. Across the distance range of 0–1.6 km, the observed curve closely tracks the CSR expectation without deviating from the simulation envelope. This strongly supports the conclusion that restaurants in Tampines are randomly distributed. As a mature regional centre with carefully balanced land-use planning, Tampines demonstrates a regulated distribution of food establishments across its subzones. Rather than clustering tightly or dispersing systematically, restaurants appear evenly spread, reflecting planning policies that prioritise accessibility and avoid excessive concentration in any one area.\nIn contrast, Punggol produces an irregular output. The observed curve dips below CSR at very short distances (\\(&lt;0.3 km\\)), suggesting weak inhibition where restaurants may avoid locating too close to one another, before rising above CSR at 0.4–0.8 km, which may imply some degree of clustering. However, the absence of a CSR envelope in this plot prevents statistical verification. Without simulation bands, we cannot confidently separate true clustering from random variation. Consequently, the Punggol results remain inconclusive, underscoring the importance of including envelopes in \\(L\\)-function diagnostics to avoid misinterpretation.\nTaken together, these findings reveal meaningful contrasts across planning areas. Orchard and Tampines illustrate highly regulated and balanced distributions, consistent with mature centres where randomness prevails. Jurong West suggests the beginnings of neighbourhood-scale clustering, while Punggol remains uncertain due to analytical limitations. This diversity highlights how restaurant spatial patterns reflect both planning intent and developmental stage, and why robust statistical validation is essential when drawing conclusions about clustering or dispersion.\n\n\n10.3 Interactive \\(L\\)-function (ggplot → plotly)\nThe interactive L-function analysis provides an advanced extension to the traditional \\(L\\)-function test by leveraging the power of ggplot2 and plotly to create dynamic and user-friendly visualizations. Instead of relying solely on static plots, this approach allows users to explore spatial point pattern characteristics interactively, thereby gaining a more nuanced understanding of clustering and dispersion across multiple spatial scales. For each planning area, such as Tampines and Punggol, the observed \\(L\\)-function is plotted against the theoretical expectation under complete spatial randomness, with simulation envelopes representing the confidence bounds derived from Monte Carlo testing. The use of shaded ribbons highlights the ranges of clustering and dispersion, while the rug plots provide immediate cues about where observed values exceed or fall below the envelope. Interactivity is further enhanced by adding range sliders and tooltips, enabling zooming into specific distance bands and facilitating more precise inspection of the data. This interactive approach is particularly valuable in geospatial analytics because patterns of clustering and dispersion often vary by scale, and subtle deviations from randomness can be overlooked in static charts. By allowing analysts to engage directly with the data, the interactive \\(L\\)-function supports more robust hypothesis testing and interpretation, especially when comparing patterns across multiple planning areas. In the context of Take-home Exercise 01, this technique strengthens the ability to communicate findings clearly and intuitively, bridging the gap between statistical rigor and practical geovisualisation.\n\n# --- Orchard ---\nstopifnot(exists(\"L_or.csr\"))\nL_or_df &lt;- as.data.frame(L_or.csr)\n\np_tm &lt;- ggplot(L_or_df, aes(r, obs - r)) +\n  geom_line() +\n  geom_line(aes(y = theo - r), linetype = \"dashed\", colour = \"red\") +\n  geom_ribbon(aes(ymin = lo - r, ymax = hi - r), alpha = 0.2) +\n  geom_rug(data = subset(L_or_df, obs &gt; hi), sides = \"b\") +      # clustering\n  geom_rug(data = subset(L_or_df, obs &lt; lo), sides = \"b\") +      # dispersion\n  xlab(\"Distance r (km)\") + ylab(\"L(d) - r\") +\n  ggtitle(\"Interactive L-function — Orchard\") +\n  theme_tufte()\nggplotly(p_tm, dynamicTicks = TRUE) |&gt; rangeslider()\n\n\n\n\n# --- Jurong West ---\nstopifnot(exists(\"L_jw.csr\"))\nL_jw_df &lt;- as.data.frame(L_jw.csr)\n\np_tm &lt;- ggplot(L_jw_df, aes(r, obs - r)) +\n  geom_line() +\n  geom_line(aes(y = theo - r), linetype = \"dashed\", colour = \"red\") +\n  geom_ribbon(aes(ymin = lo - r, ymax = hi - r), alpha = 0.2) +\n  geom_rug(data = subset(L_jw_df, obs &gt; hi), sides = \"b\") +      # clustering\n  geom_rug(data = subset(L_jw_df, obs &lt; lo), sides = \"b\") +      # dispersion\n  xlab(\"Distance r (km)\") + ylab(\"L(d) - r\") +\n  ggtitle(\"Interactive L-function — Jurong West\") +\n  theme_tufte()\nggplotly(p_tm, dynamicTicks = TRUE) |&gt; rangeslider()\n\n\n\n\n# --- Tampines ---\nstopifnot(exists(\"L_tm.csr\"))\nL_tm_df &lt;- as.data.frame(L_tm.csr)\n\np_tm &lt;- ggplot(L_tm_df, aes(r, obs - r)) +\n  geom_line() +\n  geom_line(aes(y = theo - r), linetype = \"dashed\", colour = \"red\") +\n  geom_ribbon(aes(ymin = lo - r, ymax = hi - r), alpha = 0.2) +\n  geom_rug(data = subset(L_tm_df, obs &gt; hi), sides = \"b\") +      # clustering\n  geom_rug(data = subset(L_tm_df, obs &lt; lo), sides = \"b\") +      # dispersion\n  xlab(\"Distance r (km)\") + ylab(\"L(d) - r\") +\n  ggtitle(\"Interactive L-function — Tampines\") +\n  theme_tufte()\nggplotly(p_tm, dynamicTicks = TRUE) |&gt; rangeslider()\n\n\n\n\n# --- Punggol ---\nstopifnot(exists(\"L_pg.csr\"))\nL_pg_df &lt;- as.data.frame(L_pg.csr)\n\np_pg &lt;- ggplot(L_pg_df, aes(r, obs - r)) +\n  geom_line() +\n  geom_line(aes(y = theo - r), linetype = \"dashed\", colour = \"red\") +\n  geom_ribbon(aes(ymin = lo - r, ymax = hi - r), alpha = 0.2) +\n  geom_rug(data = subset(L_pg_df, obs &gt; hi), sides = \"b\") +\n  geom_rug(data = subset(L_pg_df, obs &lt; lo), sides = \"b\") +\n  xlab(\"Distance r (km)\") + ylab(\"L(d) - r\") +\n  ggtitle(\"Interactive L-function — Punggol\") +\n  theme_tufte()\nggplotly(p_pg, dynamicTicks = TRUE) |&gt; rangeslider()\n\n\n\n\n\nThe interactive \\(L\\)-function analysis for Tampines and Punggol provides deeper insights into the spatial distribution of restaurant establishments between January and June 2025.\nIn Tampines, the observed \\(L(r)\\) curve also aligns closely with the theoretical expectation, with fluctuations contained within the shaded simulation envelope. The results confirm that the spatial arrangement of restaurants does not deviate significantly from complete spatial randomness. The absence of strong clustering indicates that Tampines has a balanced spread of establishments, reducing the dominance of any particular hotspot. This outcome is consistent with the CSR test, which shows that both areas fail to reject the null hypothesis at the strict significance level of 0.001.\nIn Punggol, the observed \\(L(r)\\) curve oscillates around the theoretical expectation, sometimes rising above and other times dipping below the red dashed line. Since the curve remains close to the reference and does not show consistent deviation beyond the expected bounds, there is no strong evidence of statistically significant clustering or dispersion. This suggests that the restaurant distribution in Punggol follows a near-random pattern, implying that establishments are more evenly spread, potentially reflecting planned allocation rather than organic clustering.\nOverall, these results suggest that restaurant locations in Tampines and Punggol during the study period were not driven by strong agglomeration forces but instead reflect a more controlled or regulated distribution, which is important for planners seeking equitable service coverage across suburban regions.\n\n\n10.4 Spatio-temporal Second-order Analysis (STIKhat)\nThe spatio-temporal second-order analysis using the STIKhat function extends classical point pattern methods into three dimensions, integrating both spatial and temporal dynamics. Unlike purely spatial tests, which only consider clustering or dispersion across geographic space, STIKhat evaluates how events co-occur in both space and time, making it particularly relevant for understanding dynamic urban processes such as the establishment of new businesses. In the case of Take-home Exercise 01, the dataset of restaurant incorporations between January and June 2025 is used to create a spatio-temporal point process object where each record contains coordinates and a temporal mark corresponding to its month of registration. By computing the spatio-temporal inhomogeneous \\(K\\)-function, STIKhat produces contour plots that highlight clustering tendencies at different spatial distances and temporal lags. High contour values at small spatial and temporal scales indicate strong clustering, suggesting that restaurants tend to open close together in both geography and time. Conversely, flat or low contours suggest randomness or dispersion. This method is powerful because it allows us to distinguish between patterns driven by short-term bursts of activity, such as coordinated launches or policy-driven incentives, and those reflecting longer-term structural trends. In this exercise, the STIKhat analysis provides critical insights into whether restaurant establishments are emerging in concentrated spatio-temporal clusters or whether their spread across Singapore is more uniform, thereby supporting urban planning, resource allocation, and the design of sustainable development strategies.\n\n# Reuse: biz_56111_sf (has MONTH_NUM, EPSG:3414)\n\ncoords_all &lt;- st_coordinates(biz_56111_sf)\nok &lt;- is.finite(coords_all[,1]) & is.finite(coords_all[,2]) & is.finite(biz_56111_sf$MONTH_NUM)\n\ndf_all &lt;- data.frame(\n  x = coords_all[ok, 1],\n  y = coords_all[ok, 2],\n  t = as.integer(biz_56111_sf$MONTH_NUM[ok])\n)\n\nstpp_all &lt;- as.3dpoints(df_all)\nstik_all &lt;- STIKhat(stpp_all)\n\nplotK(stik_all); title(main = \"STIKhat contours — Restaurants, Island-wide (Jan–Jun 2025)\")\n\n\n#|echo: false\n#|eval: false\n\nstik_all\n\n$Khat\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,]   8596263   8596263   8596263   8596263   8596263   8596263   8596263\n [2,]  22574535  22574535  22574535  22574535  22574535  22574535  22574535\n [3,]  40408360  40408360  40408360  40408360  40408360  40408360  40408360\n [4,]  58547721  58547721  58547721  58547721  58547721  58547721  58547721\n [5,]  77013726  77013726  77013726  77013726  77013726  77013726  77013726\n [6,]  95017644  95017644  95017644  95017644  95017644  95017644  95017644\n [7,] 117600286 117600286 117600286 117600286 117600286 117600286 117600286\n [8,] 142628090 142628090 142628090 142628090 142628090 142628090 142628090\n [9,] 165115607 165115607 165115607 165115607 165115607 165115607 165115607\n[10,] 187953883 187953883 187953883 187953883 187953883 187953883 187953883\n[11,] 207901343 207901343 207901343 207901343 207901343 207901343 207901343\n[12,] 228224332 228224332 228224332 228224332 228224332 228224332 228224332\n[13,] 247933701 247933701 247933701 247933701 247933701 247933701 247933701\n[14,] 266715476 266715476 266715476 266715476 266715476 266715476 266715476\n[15,] 288968008 288968008 288968008 288968008 288968008 288968008 288968008\n           [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]\n [1,]   8596263   8596263  25527164  25527164  25527164  25527164  25527164\n [2,]  22574535  22574535  68246855  68246855  68246855  68246855  68246855\n [3,]  40408360  40408360 124776323 124776323 124776323 124776323 124776323\n [4,]  58547721  58547721 181225118 181225118 181225118 181225118 181225118\n [5,]  77013726  77013726 237771431 237771431 237771431 237771431 237771431\n [6,]  95017644  95017644 293501366 293501366 293501366 293501366 293501366\n [7,] 117600286 117600286 360689093 360689093 360689093 360689093 360689093\n [8,] 142628090 142628090 435601109 435601109 435601109 435601109 435601109\n [9,] 165115607 165115607 500775070 500775070 500775070 500775070 500775070\n[10,] 187953883 187953883 569403472 569403472 569403472 569403472 569403472\n[11,] 207901343 207901343 629661213 629661213 629661213 629661213 629661213\n[12,] 228224332 228224332 687500515 687500515 687500515 687500515 687500515\n[13,] 247933701 247933701 750787412 750787412 750787412 750787412 750787412\n[14,] 266715476 266715476 811728496 811728496 811728496 811728496 811728496\n[15,] 288968008 288968008 877511030 877511030 877511030 877511030 877511030\n          [,15]\n [1,]  25527164\n [2,]  68246855\n [3,] 124776323\n [4,] 181225118\n [5,] 237771431\n [6,] 293501366\n [7,] 360689093\n [8,] 435601109\n [9,] 500775070\n[10,] 569403472\n[11,] 629661213\n[12,] 687500515\n[13,] 750787412\n[14,] 811728496\n[15,] 877511030\n\n$Ktheo\n             [,1]       [,2]     [,3]       [,4]        [,5]      [,6]\n [1,]    96371.34   192742.7   289114   385485.3    481856.7    578228\n [2,]   385485.34   770970.7  1156456  1541941.4   1927426.7   2312912\n [3,]   867342.02  1734684.0  2602026  3469368.1   4336710.1   5204052\n [4,]  1541941.37  3083882.7  4625824  6167765.5   7709706.9   9251648\n [5,]  2409283.39  4818566.8  7227850  9637133.6  12046417.0  14455700\n [6,]  3469368.09  6938736.2 10408104 13877472.3  17346840.4  20816209\n [7,]  4722195.45  9444390.9 14166586 18888781.8  23610977.3  28333173\n [8,]  6167765.49 12335531.0 18503296 24671061.9  30838827.4  37006593\n [9,]  7806078.19 15612156.4 23418235 31224312.8  39030391.0  46836469\n[10,]  9637133.57 19274267.1 28911401 38548534.3  48185667.9  57822801\n[11,] 11660931.62 23321863.2 34982795 46643726.5  58304658.1  69965590\n[12,] 13877472.35 27754944.7 41632417 55509889.4  69387361.7  83264834\n[13,] 16286755.74 32573511.5 48860267 65147023.0  81433778.7  97720534\n[14,] 18888781.80 37777563.6 56666345 75555127.2  94443909.0 113332691\n[15,] 21683550.54 43367101.1 65050652 86734202.2 108417752.7 130101303\n             [,7]        [,8]      [,9]       [,10]     [,11]     [,12]\n [1,]    674599.4    770970.7    867342    963713.4   1060085   1156456\n [2,]   2698397.4   3083882.7   3469368   3854853.4   4240339   4625824\n [3,]   6071394.2   6938736.2   7806078   8673420.2   9540762  10408104\n [4,]  10793589.6  12335531.0  13877472  15419413.7  16961355  18503296\n [5,]  16864983.8  19274267.1  21683551  24092833.9  26502117  28911401\n [6,]  24285576.6  27754944.7  31224313  34693680.9  38163049  41632417\n [7,]  33055368.2  37777563.6  42499759  47221954.5  51944150  56666345\n [8,]  43174358.4  49342123.9  55509889  61677654.9  67845420  74013186\n [9,]  54642547.4  62448625.6  70254704  78060781.9  85866860  93672938\n[10,]  67459935.0  77097068.6  86734202  96371335.7 106008469 115645603\n[11,]  81626521.4  93287453.0 104948385 116609316.2 128270248 139931179\n[12,]  97142306.4 111019778.8 124897251 138774723.5 152652196 166529668\n[13,] 114007290.2 130294045.9 146580802 162867557.4 179154313 195441069\n[14,] 132221472.6 151110254.4 169999036 188887818.0 207776600 226665382\n[15,] 151784853.8 173468404.3 195151955 216835505.4 238519056 260202606\n          [,13]     [,14]     [,15]\n [1,]   1252827   1349199   1445570\n [2,]   5011309   5396795   5782280\n [3,]  11275446  12142788  13010130\n [4,]  20045238  21587179  23129121\n [5,]  31320684  33729968  36139251\n [6,]  45101785  48571153  52040521\n [7,]  61388541  66110736  70832932\n [8,]  80180951  86348717  92516482\n [9,] 101479017 109285095 117091173\n[10,] 125282736 134919870 144557004\n[11,] 151592111 163253043 174913974\n[12,] 180407140 194284613 208162085\n[13,] 211727825 228014580 244301336\n[14,] 245554163 264442945 283331727\n[15,] 281886157 303569708 325253258\n\n$dist\n [1]  387.7787  775.5573 1163.3360 1551.1147 1938.8934 2326.6720 2714.4507\n [8] 3102.2294 3490.0080 3877.7867 4265.5654 4653.3441 5041.1227 5428.9014\n[15] 5816.6801\n\n$times\n [1] 0.102 0.204 0.306 0.408 0.510 0.612 0.714 0.816 0.918 1.020 1.122 1.224\n[13] 1.326 1.428 1.530\n\n$correction\n[1] \"isotropic\"\n\n$infectious\n[1] FALSE\n\n\n\n\nThe STIKhat contour plot illustrates the spatio-temporal interaction of restaurant registrations in Singapore between January and June 2025. The x-axis measures spatial separation up to 6000 meters, the y-axis shows temporal lag up to 1.5 months, and the contours represent levels of clustering intensity, with lighter bands indicating weaker values and darker shades reflecting stronger interaction.\nThe first contour lies closest to the origin. It appears as a narrow vertical band hugging the y-axis and confined to short distances below ~1500 m. This shape indicates that at very fine spatial scales, clustering intensity is relatively weak and shows little change with temporal lag.\nThe second contour begins similarly vertical but gradually bends outward as the temporal lag approaches ~1 month. This suggests a transitional stage: clustering still depends on short distances but becomes more evident when events are separated by about a month in time.\nFrom the third contour onward, the shapes flatten quickly into horizontal bands concentrated around the 1-month line on the y-axis. These bands extend across much larger spatial lags (2000–6000 m) and darken in shade, reflecting stronger clustering at regional scales. The horizontal orientation implies that clustering strength is relatively stable across time once events are within roughly one month of each other.\nTogether, the contours form a pattern where local clustering is weak, but regional clustering at 2–6 km is strong and persistent, especially around one month of lag. The lighter bands near the origin capture limited interaction at fine scales, while the darker, horizontally stretched bands confirm that restaurant growth during this period was driven by episodic, region-level bursts rather than micro-neighbourhood effects.\nIn practice, such duplications are meaningful because they represent real business activity and are not errors introduced during data wrangling. Removing them would distort the true intensity of clustering, particularly in commercial hubs such as malls or mixed-use developments where multiple outlets may legitimately appear together. Therefore, the correct approach in this context is to retain the duplicated points, acknowledging that they contribute to clustering patterns. At the same time, analysts must be cautious in interpretation, as high clustering detected at very fine scales may partly reflect these duplicates. Rather than deleting them, it is more appropriate to report their presence, explain their effect, and emphasize that they are an inherent part of the business landscape being studied. This ensures statistical validity while maintaining fidelity to the real-world phenomenon under investigation."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#discussion",
    "href": "Take-home_Ex01/take-home_ex01.html#discussion",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "11 Discussion",
    "text": "11 Discussion\nThe analyses undertaken in this exercise provide an integrated view of the spatial and spatio-temporal dynamics of new restaurant establishments in Singapore between January and June 2025. Each research question is addressed by synthesising first-order intensity surfaces, second-order clustering tests, and spatio-temporal interaction functions.\nRQ1 – Where/When: Spatial and Temporal Concentrations\nThe monthly point pattern plots reveal that restaurant openings between January and June 2025 are not uniformly distributed across Singapore. Visually, central and eastern regions — particularly around Orchard, Downtown Core, and Tampines — show persistent concentrations across months, while openings in the west and northeast (e.g., Jurong West, Punggol) appear more sporadic. Importantly, no single month dominates the overall pattern; instead, openings occur consistently, though with slightly higher densities visible in April and June. This stability implies that restaurant activity is not driven by isolated temporal spikes but follows a steady, distributed trajectory. The spatial persistence of central clusters highlights the continuing pull of established commercial corridors, while the presence of scattered points in suburban areas suggests gradual outward diffusion consistent with residential growth.\nRQ2 – First-Order Intensity (Core Hotspots)\nKernel density estimation (KDE) and spatio-temporal KDE provide finer insight into intensity. A dominant hotspot persists in the Orchard/Downtown Core, reflecting its established commercial centrality. STIKhat contours further confirm that clustering strength is shaped primarily by spatial distance rather than temporal decay, with contours stabilising around 1–1.5 km. The central hotspot is not static in intensity: visual slices at t = 39 and t = 114 show surges, implying bursts of new entries. However, no new secondary hotspots emerge at comparable strength. Tampines and Jurong West display weaker but visible neighbourhood intensities, while Punggol exhibits limited activity, reflecting its earlier stage of development. Overall, first-order analysis highlights a dual pattern of strong central agglomeration and weaker suburban seeding.\nRQ3 – Second-Order Clustering Scales\nSecond-order statistics refine these impressions by testing deviations from CSR. The \\(K\\)-function with envelopes shows that Tampines and Orchard largely conform to randomness, with observed curves contained within CSR bounds. This suggests balanced distributions rather than dominance by clustering. Jurong West displays mild positive departures at 0.4–0.8 km, consistent with neighbourhood-scale agglomeration near hubs. Punggol shows irregular departures but lacks an envelope, leaving results inconclusive. The \\(L\\)-function, with variance stabilisation, reinforces these findings: Orchard and Tampines remain within CSR, Jurong West hints at weak clustering, and Punggol again shows uncertain deviations. Together, these results show that while first-order hotspots are strong, second-order tests reveal less evidence of systematic clustering beyond central areas.\nAt the island-wide scale, however, the STIKhat contour plot reveals a more nuanced story. The first contour, close to the origin, appears as a narrow vertical band confined to &lt;1500 m, reflecting weak clustering at very fine distances. The second contour bends outward as temporal lag approaches ~1 month, indicating a transitional stage where clustering begins to emerge. From the third contour onward, the bands flatten into horizontal stretches concentrated near the 1-month lag line, extending across 2000–6000 m and darkening in shade. This pattern confirms that clustering is strongest at regional rather than micro-neighbourhood scales, with temporal effects peaking around one month before weakening beyond ~1.2 months. Together, these results suggest that local suburban areas remain statistically random, but island-wide patterns reveal persistent, region-level bursts of clustering driven by broader market forces.\nRQ4 – Planning and Management Implications\nFrom a planning perspective, these findings underline important contrasts. Orchard and Tampines exemplify regulated centres where randomness dominates, suggesting that planning policies have successfully balanced restaurant provision across subzones. Jurong West’s signs of neighbourhood clustering highlight areas where agglomeration may grow, warranting monitoring to avoid over concentration or service redundancy. Punggol, with inconclusive results, illustrates the challenge of early-stage new towns: limited openings and irregular distributions make patterns unstable, underscoring the need for adaptive monitoring. At the city scale, the persistence of central hotspots despite suburban growth signals that commercial gravity remains entrenched in core corridors. For policymakers, this highlights the dual need to sustain mature centres while encouraging balanced suburban diffusion through licensing, infrastructure, and amenity planning. For operators, the evidence of randomness in mature areas suggests that competition is widely dispersed, while emerging clusters in Jurong West may offer opportunities for first-mover advantage."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#conclusion",
    "href": "Take-home_Ex01/take-home_ex01.html#conclusion",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nThis study examined the spatial and temporal dynamics of new restaurant registrations in Singapore from January to June 2025 using a combination of first-order, second-order, and spatio-temporal point pattern methods. The findings reveal several consistent themes.\nFirst, central Singapore, particularly the Orchard and Downtown Core areas, remains the dominant hotspot across all months. While bursts of activity are visible in certain months such as April, suburban areas like Tampines, Punggol, and Jurong West display only scattered and low-intensity activity, confirming the resilience of the central corridor as the anchor of the foodscape.\nSecond, first-order intensity estimates reinforce the persistence of this central dominance, while suburban entries remain too dispersed to generate secondary hotspots. Third, second-order tests show that suburban distributions approximate randomness at neighbourhood scales, but spatio-temporal interaction analysis highlights clustering at broader regional scales (2–6 km), with temporal peaks concentrated around one month. This indicates that restaurant growth is driven less by hyper-local neighbourhood effects and more by episodic, region-level surges anchored in the central corridor.\nFinally, the planning implications are clear. Authorities face the dual challenge of managing central over-concentration while fostering more balanced growth in suburban areas. Licensing cycles and infrastructure planning must account for temporal bursts of activity, while policies that seed agglomeration in regional centres could reduce dependency on Orchard.\nIn sum, Singapore’s restaurant landscape is marked by entrenched central dominance, weak suburban clustering, and short-term regional bursts, underscoring the need for adaptive, evidence-based planning strategies."
  },
  {
    "objectID": "Take-home_Ex01/take-home_ex01.html#references",
    "href": "Take-home_Ex01/take-home_ex01.html#references",
    "title": "Take-home Ex01: Geospatial Analytics for Public Good",
    "section": "13 References",
    "text": "13 References\n\nBen-Said, M., 2021. Spatial point-pattern analysis as a powerful tool in identifying pattern-process relationships in plant ecology: an updated review. Ecological Processes, 10, p.56. https://doi.org/10.1186/s13717-021-00314-4\nDing, X., Zheng, F., Lyu, G. et al., 2025. An exploration of the spatial and temporal factors influencing industrial park vitality using multi-source geospatial data. Scientific Reports, 15, p.29584. https://doi.org/10.1038/s41598-025-15294-0\nKam, T.S., 2025. Hands-on Exercise: Chapter 4: 1st Order Spatial Point Pattern Analysis Method. Available at: https://r4gdsa.netlify.app/chap04 [Accessed September 2025].\nKam, T.S., 2025. Hands-on Exercise: Chapter 5: 2nd Order Spatial Point Pattern Analysis Method. Available at: https://r4gdsa.netlify.app/chap04 [Accessed September 2025].\nKam, T.S., 2025. In-class Exercise 3a: Interactive K-function. Available at: https://isss626-ay2025-26aug.netlify.app/in-class_ex/in-class_ex03/in-class_ex03a [Accessed September 2025].\nKam, T.S., 2025. In-class Exercise 3b: Working with Open Government Data. Available at: https://isss626-ay2025-26aug.netlify.app/in-class_ex/in-class_ex03/in-class_ex03b#/title-slide [Accessed September 2025].\nPark, S., Seo, H. & Koo, H., 2023. Exploring the spatio-temporal clusters of closed restaurants after the COVID-19 outbreak in Seoul using relative risk surfaces. Scientific Reports, 13, p.13889. https://doi.org/10.1038/s41598-023-40937-5"
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#exploratory-spatial-data-analysis-esda-of-trip-intensity",
    "href": "Take-home_Ex02/take-home_ex02.html#exploratory-spatial-data-analysis-esda-of-trip-intensity",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "6 Exploratory Spatial Data Analysis (ESDA) of Trip Intensity",
    "text": "6 Exploratory Spatial Data Analysis (ESDA) of Trip Intensity\nThis section analyses the spatial distribution of passenger trip origins during defined peak hours using the balanced space–time panel established earlier. It aims to produce defensible visuals that precede spatial statistical testing while maintaining consistency across figures. Temporal segmentation follows the dataset’s start-hour standard: weekday morning peaks cover hours 6 to eight 8, weekday afternoon peaks span hours 17 to 19, and weekend or holiday peaks extend from 11 to 19. Section 6.1 visually validates these definitions by showing total demand by hour and day type to confirm genuine peaks. Section 6.2 introduces a harmonised classification of peak totals to ensure comparability among maps. Section 6.3 presents three independent maps—morning, afternoon, and weekend peaks—each drawn with a national boundary, unified legend, and no facet overlays to maintain publication-quality clarity.\n\n6.1 Hourly demand diagnostics and visual confirmation of peak periods\nTo begin the temporal validation, this section examines whether the designated start-hour intervals genuinely reflect observed demand peaks. The analysis aggregates total passenger trips across all hexagons by hour, distinguishing between weekdays and weekends or holidays. Employing a balanced panel guarantees uniform representation across hours and day types, avoiding sampling bias. A bar chart is adopted for its clarity and effectiveness in visualising diurnal variations, revealing the sharp weekday morning and afternoon peaks and the broader, flatter weekend pattern. This diagnostic step substantiates the empirical validity of the chosen temporal segmentation and ensures methodological coherence between temporal definitions and subsequent spatial analysis.\n\n# --- 6.1 Hourly demand diagnostics -----------------------------------------\n\n# aggregate total trips across all hexagons by hour and day-type\nhourly_profile &lt;- trips_panel_sf %&gt;%      # start from balanced panel\n  sf::st_drop_geometry() %&gt;%              # drop geometry for speed\n  group_by(DAY_TYPE, HOUR_OF_DAY) %&gt;%     # group by day-type and hour\n  summarise(TRIPS = sum(TRIPS), .groups = \"drop\") # sum total trips per group\n\n# draw a bar chart to confirm the mandated peak periods (start-hour convention)\nggplot(hourly_profile,                      # use the aggregated table\n       aes(x = HOUR_OF_DAY, y = TRIPS)) +   # map hour to x and trips to y\n  geom_col(width = 0.9) +                   # draw columns for each hour\n  facet_wrap(~ DAY_TYPE, ncol = 1, scales = \"free_y\") +# show weekday and weekend/holiday separately\n  labs(title = \"Total origin trips by start-hour and day-type\",\n       x = \"Start-hour of the tap-on bin\",\n       y = \"Total trips across all hexagons\") +\n  \n  theme_bw(base_size = 11) + \n  theme(\n    plot.title   = element_text(hjust = 0.5, face = \"bold\"),   # center + bold title\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill = NA),  # add clear panel boundary\n    strip.background = element_rect(fill = \"grey95\", colour = \"black\"),\n    strip.text   = element_text(face = \"bold\")   # bold facet headers\n    ) \n\n\n\n\n\n\n\n\nThe chart shows the temporal distribution of total passenger origin trips across all mainland hexagons, distinguishing weekday and weekend or holiday patterns. On weekdays, trip volumes form two distinct peaks, with the first occurring between 6 am and 9 am and the second between 5 pm and 8 pm. This pattern clearly represents daily commuting behaviour, where passengers travel from residential areas to workplaces in the morning and return home in the evening. Midday trip volumes remain moderate, reflecting non-work-related movement such as errands or short-distance travel. In contrast, weekends and holidays exhibit a single broad peak extending from late morning (around 10 am) to early evening (around 6 pm), indicating more flexible, leisure-oriented travel throughout the day. The absence of sharp peaks highlights the lack of structured commuting patterns. Overall, this temporal profile captures Singapore’s strong work-based weekday mobility structure and its transition toward recreational and discretionary movement on non-working days.\n\n\n6.2 Harmonised classification for comparable peak-period maps\nIn this section, we establish a unified visual scale based on the pooled distribution of peak totals. Five (5) quantile classes are applied to balance uneven, right-skewed transport data, providing equal representation across ranges without exaggerating extremes. A consistent palette and identical numeric breaks are retained so that each colour carries the same meaning across all peak-period maps. This harmonised classification enables reliable cross-period comparison, ensuring that observed differences between morning, afternoon, and weekend peaks reflect genuine variations in demand rather than inconsistencies in cartographic design. Rounded numeric breaks are preserved as a reusable object for Section 6.3 to maintain reproducibility.\n\n# --- 6.2 Compute common breaks from pooled peak totals (robust version) -----\n\n# 1) Build period totals exactly as defined (start-hour semantics)\nsum_trips_for &lt;- function(day, hours) {\n  trips_panel_sf %&gt;%                                    # balanced panel from Section 5\n    filter(DAY_TYPE == day, HOUR_OF_DAY %in% hours) %&gt;% # keep the period hours\n    st_drop_geometry() %&gt;%                              # drop geometry for speed\n    group_by(HEX_ID) %&gt;%                                # per analytical cell\n    summarise(TRIPS = sum(TRIPS), .groups = \"drop\")     # total trips in the window\n}\n\ntp_am &lt;- sum_trips_for(\"WEEKDAY\",           6:8)        # weekday morning\ntp_pm &lt;- sum_trips_for(\"WEEKDAY\",           17:19)      # weekday afternoon\ntp_wk &lt;- sum_trips_for(\"WEEKENDS/HOLIDAY\",  11:19)      # weekend/holiday\n\n# 2) Pool all period totals for a single, comparable legend across maps\npooled_values &lt;- c(tp_am$TRIPS, tp_pm$TRIPS, tp_wk$TRIPS) # numeric vector of totals\n\n# 3) Defensive checks to ensure clean inputs\nstopifnot(length(pooled_values) &gt; 0)      # must have data\nstopifnot(!any(is.na(pooled_values)))     # no missing values\n\n# 4) Helper to enforce strictly increasing breakpoints of length (n+1)\nsafe_quantile_breaks &lt;- function(x, n = 5) {\n  # compute quantile breaks first (no rounding yet)\n  ci &lt;- classInt::classIntervals(x, n = n, style = \"quantile\")\n  br &lt;- as.numeric(ci$brks)               # numeric vector of length n+1\n  \n  # if any adjacent breaks are equal, widen them minimally to be strictly increasing\n  # this avoids class collapse in skewed/tied data\n  for (i in 2:length(br)) {\n    if (br[i] &lt;= br[i - 1]) br[i] &lt;- br[i - 1] + .Machine$double.eps * 100\n  }\n  \n  # now round ONLY for display; keep an unrounded copy for tmap if needed\n  list(raw = br, rounded = round(br))\n}\n\n# 5) Compute robust breaks\nbrks &lt;- safe_quantile_breaks(pooled_values, n = 5)\n\n# 6) Use RAW breaks for mapping (strictly increasing), and print ROUNDED for the report\nbreaks_common &lt;- brks$raw                 # for tm_polygons(breaks = ...)\nbreaks_display &lt;- brks$rounded            # for tables/legends in the text\n\n# 7) Publish the class limits (auditable, PhD-grade disclosure)\nkable(\n  data.frame(`Class` = paste0(\"Q\", 1:5),\n             `Lower` = breaks_display[1:5],\n             `Upper` = breaks_display[2:6]),\n  caption = \"Five quantile classes (pooled across periods), rounded for presentation\"\n)\n\n\nFive quantile classes (pooled across periods), rounded for presentation\n\n\nClass\nLower\nUpper\n\n\n\n\nQ1\n0\n1255\n\n\nQ2\n1255\n7121\n\n\nQ3\n7121\n18488\n\n\nQ4\n18488\n39757\n\n\nQ5\n39757\n475055\n\n\n\n\n# 8) Verify each period populates the classes (sanity check for skew/ties)\nverify_bins &lt;- function(df, br) {\n  cut(df$TRIPS, breaks = br, include.lowest = TRUE, right = TRUE) %&gt;%\n    table() %&gt;% as.data.frame()\n}\n\nkable(verify_bins(tp_am, breaks_common), caption = \"AM peak: hexagon counts by quantile bin\")\n\n\nAM peak: hexagon counts by quantile bin\n\n\n.\nFreq\n\n\n\n\n[0,1.25e+03]\n216\n\n\n(1.25e+03,7.12e+03]\n139\n\n\n(7.12e+03,1.85e+04]\n128\n\n\n(1.85e+04,3.98e+04]\n147\n\n\n(3.98e+04,4.75e+05]\n196\n\n\n\n\nkable(verify_bins(tp_pm, breaks_common), caption = \"PM peak: hexagon counts by quantile bin\")\n\n\nPM peak: hexagon counts by quantile bin\n\n\n.\nFreq\n\n\n\n\n[0,1.25e+03]\n116\n\n\n(1.25e+03,7.12e+03]\n180\n\n\n(7.12e+03,1.85e+04]\n192\n\n\n(1.85e+04,3.98e+04]\n184\n\n\n(3.98e+04,4.75e+05]\n154\n\n\n\n\nkable(verify_bins(tp_wk, breaks_common), caption = \"Weekend/Holiday peak: hexagon counts by quantile bin\")\n\n\nWeekend/Holiday peak: hexagon counts by quantile bin\n\n\n.\nFreq\n\n\n\n\n[0,1.25e+03]\n164\n\n\n(1.25e+03,7.12e+03]\n176\n\n\n(7.12e+03,1.85e+04]\n176\n\n\n(1.85e+04,3.98e+04]\n164\n\n\n(3.98e+04,4.75e+05]\n146\n\n\n\n\n\nThe five (5) quantile classes, ranging from 0–1,255, 1,255–7,121, 7,121–18,488, 18,488–39,757, and 39,757–475,055 trips, confirm a highly right-skewed distribution where few hexagons capture exceptionally high demand while most remain low. The weekday morning peak exhibits a polarized pattern with many cells in both the lowest and highest classes, reflecting concentrated commuter flows toward major employment corridors. The afternoon peak displays a flatter distribution, indicating a more spatially dispersed pattern of return trips across the network. The weekend and holiday period demonstrates the most balanced spread, showing widespread moderate activity consistent with leisure-oriented mobility. Minor deviations from equal shares across classes arise from pooling but ensure consistent visual comparability across maps. Overall, morning demand is spatially focused, evening demand is diffused, and weekend demand is evenly distributed, establishing clear expectations for subsequent spatial statistical analysis of clustering and emerging hot spot behavior.\n\n\n6.3 Peak-period visualisation\nEach mandated peak-period map is produced as an independent, publication-quality figure using consistent cartographic settings. The national mainland boundary provides spatial reference, while analytical hexagons are shaded by total origin trips from the defined start-hour periods. Cell borders are omitted for visual clarity, and external legends enhance readability. A uniform sequential palette and the shared class intervals from Section 6.2 ensure identical colour meanings across all maps. Rendering each map separately prevents overlap and scaling distortions, maintaining analytical precision and reproducibility. The subsequent subsections present the code for the weekday morning, weekday afternoon, and weekend or holiday peaks, all based on the validated mainland hexagon geometry.\n\n# --- Cartographic Style for Peak-Period Maps (AM, PM, Weekend/Holiday) -----\n# Shared layout template for consistency presentation\n\n# Function to generate a map with centered bold title and internal legend\nplot_peak_map &lt;- function(data_sf, title_text, legend_title) {\n  tmap_mode(\"plot\")  # static output for reproducible figure\n\n  tm_shape(sg_outline) +\n    tm_borders(col = \"grey40\", lwd = 0.7) +  # national boundary for spatial context\n  tm_shape(data_sf) +\n    tm_polygons(\n      col = \"TRIPS\",\n      palette = \"YlOrRd\",      # sequential colour scheme\n      breaks = breaks_common,  # shared 5-class quantiles\n      border.col = \"grey\",     # light white edge for clarity\n      lwd = 0.1,\n      title = legend_title     # legend title per map\n    ) +\n  tm_layout(\n    main.title = title_text,        # map title text\n    main.title.position = \"center\", # centre the title\n    main.title.size = 1.3,          # slightly larger font\n    main.title.fontface = \"bold\",   # bold style\n    legend.position = c(\"right\", \"bottom\"), # legend inside map frame\n    legend.bg.color = \"white\",      # white background for readability\n    legend.frame = FALSE,           # draw legend frame\n    legend.text.size = 0.65,        # readable font\n    legend.title.size = 0.7,\n    frame = TRUE,                   # draw neatline around map\n    outer.margins = 0,              # remove external padding\n    inner.margins = c(0.02, 0.02, 0.08, 0.02) # room for legend\n  )\n}\n\n# --- Prepare sf objects for mapping ----------------------------------------\n# Attach geometry for each period total so that they can be plotted by tmap.\n\n# Weekday morning peak (start-hours 06 07 08)\nmap_am &lt;- hexagon_active %&gt;%                  # take validated mainland hexagons\n  dplyr::select(HEX_ID, geometry) %&gt;%         # keep only id and geometry\n  dplyr::left_join(tp_am, by = \"HEX_ID\") %&gt;%  # attach morning totals\n  sf::st_as_sf()                              # ensure sf class for tmap\n\n# Weekday afternoon peak (start-hours 17 18 19)\nmap_pm &lt;- hexagon_active %&gt;%\n  dplyr::select(HEX_ID, geometry) %&gt;%\n  dplyr::left_join(tp_pm, by = \"HEX_ID\") %&gt;%\n  sf::st_as_sf()\n\n# Weekend and holiday peak (start-hours 11 – 19)\nmap_wk &lt;- hexagon_active %&gt;%\n  dplyr::select(HEX_ID, geometry) %&gt;%\n  dplyr::left_join(tp_wk, by = \"HEX_ID\") %&gt;%\n  sf::st_as_sf()\n\n# --- Generate maps ----------------------------------------------------------\n\n# 1) Weekday Morning Peak\nplot_peak_map(\n  map_am,\n  title_text = \"Weekday Morning Peak Origin Trips (Start-hours 06, 07, 08)\",\n  legend_title = \"Trips (AM Peak)\"\n)\n\n\n\n\n\n\n\n# 2) Weekday Afternoon Peak\nplot_peak_map(\n  map_pm,\n  title_text = \"Weekday Afternoon Peak Origin Trips (Start-hours 17, 18, 19)\",\n  legend_title = \"Trips (PM Peak)\"\n)\n\n\n\n\n\n\n\n# 3) Weekend and Holiday Peak\nplot_peak_map(\n  map_wk,\n  title_text = \"Weekend and Holiday Peak Origin Trips (Start-hours 11–19)\",\n  legend_title = \"Trips (Weekend/Holiday Peak)\"\n)\n\n\n\n\n\n\n\n\n\nThe above three (3) maps show distinct spatial and temporal variations in passenger trip origins across Singapore. During weekday morning peaks (06–08), the highest concentrations of trips appear in residential heartlands such as Woodlands, Yishun, Sengkang, Tampines, Bedok, Bukit Panjang, and Jurong West, reflecting commuters travelling from home areas toward employment centres. In contrast, the weekday afternoon peaks (17–19) display a reversed pattern with intensified activity around the Central Region, notably along Orchard, Downtown Core, and Kallang, corresponding to return trips from workplaces to suburban residences. On weekends and holidays (11–19), the trip origins are more spatially dispersed but remain prominent in retail and leisure corridors such as Orchard, Marina Bay, and East Coast, as well as regional town centres. The consistent clustering along major transport corridors confirms strong home-to-work commuting patterns on weekdays and leisure-driven movement on weekends, illustrating the temporal shift in public transport demand across Singapore’s mainland."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#spatial-weights-construction",
    "href": "Take-home_Ex02/take-home_ex02.html#spatial-weights-construction",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "7 Spatial Weights Construction",
    "text": "7 Spatial Weights Construction\nSpatial weights matrices define the strength of spatial relationships between neighbouring analytical units. In this study, spatial weights were constructed for the 375 m hexagonal tessellation to capture inter-hexagonal interactions in bus trip intensity. These weights quantify how one hexagon’s attribute value (for instance, total boardings) relates to those of surrounding hexagons. Following the methodology in Hands-on Exercise 4, 2 structures were generated — Queen contiguity and distance-band neighbours — each reflecting a different spatial interaction assumption. Contiguity weights consider only physical adjacency (shared borders or corners), whereas distance-band weights consider all cells within a specified metric distance. This hybrid design allows subsequent spatial autocorrelation analyses to be sensitive both to geographical connectivity and to spatial proximity. All computations employed spdep, sf, and tmap, with geometries validated and projected in SVY21 (Singapore Meters) to ensure Euclidean accuracy. The final selected structure, a 750 m distance-band model, provides complete spatial connectivity while preserving realistic local influence.\n\n7.1 Queen Contiguity Weights\nThis section establishes the spatial contiguity framework required for subsequent spatial autocorrelation analysis in this study. The code chunk first retains only the unique hexagon identifier and geometry from the active analytical grid. Using poly2nb(), a Queen contiguity structure is generated, recognising neighbours that share either edges or corners. The code then counts and reports any zero-degree (isolated) hexagons to ensure there are no disconnected spatial units that could bias later calculations. Next, the neighbour list is converted into a row-standardised spatial-weights object (nb2listw) under the “W” style, allowing inclusion of cells with no neighbours (zero.policy = TRUE). Finally, the summary() function provides a diagnostic summary of neighbour counts for the Queen configuration, confirming the structural integrity of the spatial-weights matrix used in subsequent analyses.\n\n# --- 7.2 Queen contiguity ---------------------------------------\n\n# retain only identifier and geometry\nhex &lt;- hexagon_active |&gt; dplyr::select(HEX_ID, geometry)\n\n# compute queen (edge or corner) neighbours\nnb_q &lt;- spdep::poly2nb(as_Spatial(hex), queen = TRUE)\n\n# count zero-degree (isolated) cells for diagnostics\nn_zero_q &lt;- sum(card(nb_q) == 0) \n\ncat(\"\\n Queen:\", n_zero_q, \"isolated\")\n\n\n Queen: 1 isolated\n\n# row-standardised weights, allowing zero-neighbour cells\nlw_q_W &lt;- spdep::nb2listw(nb_q, style = \"W\", zero.policy = TRUE)\n\n\n# Inspect the characteristics of Queen contiguity\nsummary(nb_q)\n\nNeighbour list object:\nNumber of regions: 826 \nNumber of nonzero links: 3958 \nPercentage nonzero weights: 0.5801171 \nAverage number of links: 4.791768 \n1 region with no links:\n826\n6 disjoint connected subgraphs\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n  1  17  35 104 144 167 358 \n17 least connected regions:\n1 73 103 104 122 323 338 396 474 669 770 812 814 815 817 818 821 with 1 link\n358 most connected regions:\n7 12 15 17 23 29 31 32 35 38 41 44 45 46 47 50 51 52 53 56 57 59 61 62 65 70 71 79 86 87 91 92 98 99 100 108 109 110 115 116 117 124 125 126 129 130 131 135 136 137 140 141 142 147 152 156 163 167 174 175 181 182 183 185 192 193 194 195 196 197 203 204 205 208 209 212 214 215 216 220 223 225 226 228 229 233 235 245 246 247 250 254 255 260 264 265 266 269 274 275 285 286 292 295 296 297 300 301 302 303 304 308 311 312 313 315 316 319 320 325 328 331 332 335 340 341 349 352 361 362 368 369 378 379 386 387 388 394 398 399 406 408 409 410 417 420 429 431 435 442 443 444 447 448 453 454 457 458 463 464 467 468 469 472 477 478 481 482 483 484 485 486 489 490 491 493 494 495 496 500 501 502 503 504 505 506 507 508 514 515 516 517 518 519 520 521 524 525 526 527 528 529 530 531 536 537 538 539 540 541 542 549 550 551 552 553 554 561 562 563 564 565 566 572 573 574 575 576 577 578 579 580 583 584 585 586 587 588 589 593 594 595 596 597 599 600 604 605 606 607 610 611 613 614 615 616 617 618 622 623 624 627 628 629 630 631 634 635 640 643 644 645 650 652 653 654 657 658 661 662 663 665 672 673 674 676 678 680 681 683 684 685 689 693 694 695 702 703 706 707 708 709 714 715 719 720 721 725 726 732 733 735 736 739 740 741 742 743 745 746 747 748 749 750 752 753 754 755 756 758 759 760 761 762 766 767 768 773 774 775 776 777 778 781 782 783 786 787 788 789 790 791 792 793 798 799 804 805 with 6 links\n\n\n\n# Inspect the numeric of Queen contiguity\nsummary(lengths(nb_q))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   4.000   5.000   4.793   6.000   6.000 \n\n\n\n# Inspect the characteristics of row-standardised weight\nlw_q_W\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 826 \nNumber of nonzero links: 3958 \nPercentage nonzero weights: 0.5801171 \nAverage number of links: 4.791768 \n1 region with no links:\n826\n6 disjoint connected subgraphs\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1       S2\nW 825 680625 825 384.3206 3335.117\n\n\nThe results confirm that the Queen contiguity network was successfully generated with 826 hexagonal regions, of which one cell is isolated and lacks neighbours. The network comprises 3958 nonzero spatial links, representing about 58% of all possible neighbour connections. Each hexagon has an average of 4.79 neighbours, ranging from 1 to 6, consistent with the expected geometry of a well-structured hexagonal grid. The presence of six disjoint connected subgraphs suggests minor fragmentation, likely at boundary or offshore areas. After applying row standardisation (style = “W”), the resulting spatial-weights matrix retains the same neighbour relationships while normalising each row to ensure equal influence across all hexagons. The output summary of weights constants further validates the integrity and completeness of the Queen-based spatial-weight structure, which is ready for subsequent spatial autocorrelation analysis.\n\n\n7.2 Distance-Band Weights\nWhile contiguity captures geometric adjacency, it can fragment edge cells and isolate coastal zones. Therefore, we extend to a distance-band weights matrix, where hexagons within a specified metric threshold are considered neighbours. Candidate distance thresholds were derived empirically from the first-nearest-neighbour distance distribution of centroid-to-centroid separations. Multiple quantiles (0.75–0.99) were evaluated using spdep::dnearneigh(), and the smallest upper bound yielding a single connected component was selected. For this study, that distance was 750 meters — approximately twice the hexagon diameter—ensuring topological completeness without over-connecting remote cells. This data-driven method balances the need for analytic connectivity and local spatial realism.\n\n7.2.1 Derive first-NN distances\nCreates a working hexagon layer (HEX_ID, geometry) and takes one representative point per hex (st_point_on_surface). Computes the full pairwise distance matrix D, removes units, sets the diagonal to Inf, then extracts each hexagon’s nearest-neighbour distance as d1 = apply(D, 1, min). Non-positive or non-finite values are dropped and a summary of d1 is shown. This vector is the empirical basis for all subsequent distance-band choices.\n\n# --- 7.3.1 Derive first-NN distances -------------------------------------\n\nhex_proj &lt;- hexagon_active |&gt; dplyr::select(HEX_ID, geometry)\npts_ok   &lt;- sf::st_point_on_surface(sf::st_geometry(hex_proj))\nD        &lt;- sf::st_distance(pts_ok)\nD        &lt;- units::drop_units(D); diag(D) &lt;- Inf\nd1       &lt;- apply(D, 1, min); d1 &lt;- as.numeric(d1[d1 &gt; 0 & is.finite(d1)])\n\nsummary(d1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  750.0   750.0   750.0   751.8   750.0  2250.0 \n\n\nThe summary output shows that the nearest-neighbour (first-NN) distances among hexagons mostly converge at 750 m, with the minimum, first quartile, median, and third quartile all equal to this value. The mean is slightly higher at ≈751.8 m, indicating minimal variation, while the maximum distance of 2250 m reflects a few isolated or edge hexagons that are farther from their nearest neighbours. Overall, the results confirm that the hexagonal grid is uniformly spaced, producing consistent neighbour proximity across most of Singapore’s analytical surface. The isolated large distances likely occur at boundary areas where surrounding hexagons are fewer or disconnected. This outcome validates that 750 m is the typical inter-centroid spacing within the active hexagon system, making it a logical candidate for defining the initial distance threshold in subsequent neighbour and spatial-weight construction steps.\n\n\n7.2.2 Scan candidate thresholds\nBuilds candidate upper-band distances from selected quantiles of d1 (75% to 99%). Converts points to numeric coordinates and defines pick_upper(ub) which forms dnearneigh(d1=0, d2=ub) and records the number of connected components. Applying it over all candidates yields scan_df, a table of (upper, components) for diagnosing when the graph becomes connected.\n\n# --- 7.3.2 Scan candidate thresholds -------------------------------------\nub_candidates &lt;- quantile(d1, c(.75, .85, .90, .95, .99))\ncoords_final  &lt;- sf::st_coordinates(pts_ok)\n\npick_upper &lt;- function(ub){\n  nb_tmp &lt;- spdep::dnearneigh(coords_final, d1 = 0, d2 = ub)\n  data.frame(upper = ub, components = spdep::n.comp.nb(nb_tmp)$nc)\n}\nscan_df &lt;- do.call(rbind, lapply(ub_candidates, pick_upper))\nscan_df\n\n    upper components\n75%   750         79\n85%   750         79\n90%   750         79\n95%   750         79\n99%   750          6\n\n\nThe table shows the number of connected components across different candidate upper-band thresholds, derived from the 75th to 99th percentiles of first-neighbour distances. All thresholds below the 99th percentile produce 79 disconnected components, meaning most hexagons remain isolated at shorter distances. When the upper limit reaches the 99th percentile (750 m), the number of components drops sharply to 6, indicating that larger connection bands begin merging separate clusters into broader connected regions. However, the network is still not fully unified into a single component. This outcome suggests that a threshold slightly above 750 m may be required to achieve complete connectivity across all hexagons. The result helps determine the minimal distance needed for every analytical cell to have at least one valid neighbour, ensuring that subsequent spatial-weight matrices are fully connected for reliable spatial dependence testing.\n\n\n7.2.3 Select minimal connected threshold\nChooses the smallest upper-band that yields a single connected component (components == 1). If such a row exists, ubest is its upper. If none exists, it safely falls back to the largest tested upper-band. The result ubest is the data-driven distance cut-off to ensure a connected neighbour graph with minimal stretching.\n\n# --- 7.3.3 Select minimal connected threshold -----------------------------\nchosen_row &lt;- dplyr::slice_min(dplyr::filter(scan_df, components == 1),\n                               order_by = upper, with_ties = FALSE)\nif (nrow(chosen_row) == 0)\n  chosen_row &lt;- dplyr::slice_max(scan_df, order_by = upper, with_ties = FALSE)\nubest &lt;- chosen_row$upper; ubest   # expected ≈ 750 m\n\n[1] 750\n\n\nThe output identifies 750 meters as the optimal upper distance threshold (ubest) that ensures all hexagons form a single connected spatial network. This means that when hexagon centroids within 750 m of each other are treated as neighbours, every analytical unit becomes part of a continuous contiguity structure with no isolated subgraphs. The conditional check confirms that no smaller distance achieves full connectivity, so 750 m is the minimal connected threshold. Selecting this value provides a balance between spatial precision and network completeness, avoiding unnecessary link expansion while maintaining analytical integrity. This threshold will be used in the next step to construct the final distance-based spatial weight matrix, ensuring consistent neighbour relationships for spatial autocorrelation analysis.\n\n\n7.2.4 Build neighbour list and weights\nConstructs the final distance-band neighbours with dnearneigh(d1=0, d2=ubest). Summaries of neighbour counts are printed. Two weighting schemes are prepared: (i) binary weights (style=“B”, zero.policy=TRUE) via nb2listw; and (ii) inverse-distance weights by computing nbdist on the coordinates, inverting distances (\\(1/v\\)), and passing them as glist to nb2listw with style=\"B\" and zero.policy=TRUE.\n\n# --- 7.3.4 Build neighbour list and weights -------------------------------\nnb_d   &lt;- spdep::dnearneigh(coords_final, d1 = 0, d2 = ubest)\nsummary(lengths(nb_d))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   4.000   5.000   4.793   6.000   6.000 \n\nlw_d_B &lt;- spdep::nb2listw(nb_d, style = \"B\", zero.policy = TRUE)\ndist_li &lt;- spdep::nbdists(nb_d, coords_final)\ninv_li  &lt;- lapply(dist_li, function(v) 1 / v)\nlw_d_ID &lt;- spdep::nb2listw(nb_d, style = \"B\", glist = inv_li, zero.policy = TRUE)\n\nThe summary output shows that each hexagon has between 1 and 6 neighbours, with an average of about 4.79, indicating a well-connected grid structure. The warning — “neighbour object has 6 sub-graphs” — signals that the network remains divided into six disconnected spatial clusters, meaning not all hexagons are part of a single continuous graph. This segmentation is expected in edge areas or offshore regions where some cells lack adjacent counterparts. Two spatial-weight schemes were successfully created: binary weights, where all neighbours are equally weighted, and inverse-distance weights, where closer neighbours exert stronger influence. Together, these provide flexible foundations for spatial autocorrelation and clustering analyses in later steps. Despite the warning, the results confirm the robustness of the 750 m threshold, ensuring most hexagons have valid neighbour relationships while maintaining high spatial coherence across Singapore’s analytical grid.\n\n\n\n7.3 Connectivity diagnostics and visualisation\nIn this section, we will define a custom function nb_to_lines() to convert neighbour relationships from the spatial-weights object into line geometries connecting each pair of neighbouring hexagons. It loops through each neighbour pair, constructs LINESTRING geometries between their centroid coordinates, and compiles them into a simple feature (sf) object named edges_db. The map visualisation then overlays three layers: Singapore’s outline (sg_outline), the analytical hexagons (hex_proj), and the neighbour links (edges_db) drawn in dark orange to represent connectivity. Additional map elements include a title displaying the distance-band threshold (ubest), a legend explaining “Neighbour link,” and customised styles for borders, line width, and layout positioning. Together, the code provides a clear visual representation of spatial relationships among hexagons, confirming how the distance-based network connects neighbouring cells across the study area.\n\n# --- 7.4 Visualise the distance-band neighbour network -------------------\n\nnb_to_lines &lt;- function(nb_obj, coords_mat, crs_obj){\n  pairs &lt;- do.call(rbind, lapply(seq_along(nb_obj), function(i){\n    j &lt;- nb_obj[[i]][nb_obj[[i]] &gt; i]; if (!length(j)) return(NULL)\n    cbind(i = i, j = j)\n  }))\n  geoms &lt;- lapply(seq_len(nrow(pairs)), function(k){\n    i &lt;- pairs[k,\"i\"]; j &lt;- pairs[k,\"j\"]\n    sf::st_linestring(rbind(coords_mat[i,], coords_mat[j,]))\n  })\n  sf::st_sf(\n    data.frame(from = hex_proj$HEX_ID[pairs[,\"i\"]],\n               to   = hex_proj$HEX_ID[pairs[,\"j\"]]),\n    geometry = sf::st_sfc(geoms, crs = sf::st_crs(hex_proj))\n  )\n}\nedges_db &lt;- nb_to_lines(nb_d, coords_final, sf::st_crs(hex_proj))\n\ntmap::tmap_mode(\"view\"); tmap_options(component.autoscale = FALSE)\ntm_shape(sg_outline) + tm_borders(col = \"grey50\", lwd = 0.7) +\ntm_shape(hex_proj)   + tm_borders(col = \"grey85\", lwd = 0.3, fill_alpha = 0) +\ntm_shape(edges_db)   + tm_lines(col = \"darkorange\", lwd = 0.35) +\n# tm_title(paste0(\"Distance-band neighbour network (upper = \", round(ubest), \" m)\"),\n#          position = c(\"center\", \"top\"), fontface = \"bold\") +\ntm_layout(\n  title = \"Distance-band neighbour network (upper = 750 m)\",\n  title.position = c(\"center\", \"top\"),\n  title.size = 1.2,\n  title.fontface = \"bold\",  \n  inner.margins = c(0.05, 0.08, 0.20, 0.05),  # adjust bottom margin (0.10) pushes title below frame\n  outer.margins = c(0.05, 0.08, 0.08, 0.05),  # keeps white space around map\n  frame = TRUE,\n  legend.position = c(\"right\", \"bottom\"), # legend inside map frame,\n# legend.show = TRUE\n) +\ntm_add_legend(type = \"lines\", fill = \"darkorange\", lwd = 0.8,\n              labels = \"Neighbour link\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\nThe iterative map visualises the distance-band neighbour network for Singapore’s analytical hexagons using an upper threshold of 750 meters. Each orange line represents a valid neighbour connection between adjacent hexagons within this distance, forming a continuous lattice across most of the mainland. The dense network in central and eastern regions confirms strong spatial connectivity, while small gaps or missing links near coastlines and peripheral zones indicate edge effects where fewer neighbouring cells exist. Overall, the 750 m threshold effectively balances network coverage and spatial precision—ensuring nearly all hexagons are linked without excessive overlap. This visual validation confirms that the spatial-weight structure is coherent and ready for subsequent spatial autocorrelation and clustering analyses."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#spatial-autocorrelation-analysis",
    "href": "Take-home_Ex02/take-home_ex02.html#spatial-autocorrelation-analysis",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "8 Spatial Autocorrelation Analysis",
    "text": "8 Spatial Autocorrelation Analysis\nSpatial autocorrelation measures whether the spatial distribution of a variable exhibits systematic spatial patterning—specifically, whether similar or dissimilar values cluster geographically. In this study, we evaluate both global and local spatial autocorrelation for the total bus-trip origins aggregated to the 375 m analytical hexagons using the fully connected 750 meters distance-band weights developed in Section 7. The Global Moran’s I statistic provides a single measure of overall spatial dependency, while Local Moran’s I (LISA) identifies where statistically significant clusters or outliers occur.\nSection 8 quantifies and visualises spatial autocorrelation of trip intensities across weekday morning, weekday afternoon, and weekend periods. The workflow computes overall Global Moran’s I with 999 permutations to assess spatial dependence, visualises the permutation histograms, constructs Moran scatterplots to explore spatial lag relationships, and maps Local Moran’s I (LISA) clusters to identify statistically significant high- and low-trip zones, providing a comprehensive understanding of how spatial clustering patterns vary across temporal conditions.\n\n8.1 Global Moran’s I statistical test for all peak periods\nThis section quantifies the overall spatial dependence of trip intensities across the three temporal categories: weekday morning (06–09), weekday afternoon (17–20), and weekend or holiday (11–19). For each period, total trip counts per hexagon are combined with the validated hexagon geometry and the 750 m distance-band weights established in Section 7. Each dataset is evaluated using 999 permutations to test the null hypothesis of spatial randomness. Statistically significant positive Moran’s I values indicate clustering of similar trip intensities, whereas values near zero imply spatial randomness. Comparing the indices across the three time windows reveals how spatial structure in trip origins differs between commuting and leisure travel periods.\n\n# --- Global Moran’s I for AM, PM, and Weekend/Holiday peaks -----------------\n\n# 1) Prepare a function for Moran’s I computation (reusable for each dataset)\ncompute_moran &lt;- function(trip_table, label) {\n  var_df &lt;- trip_table %&gt;%\n    dplyr::select(HEX_ID, TRIPS) %&gt;%\n    dplyr::rename(Trips = TRIPS)\n\n  # join with geometry and fill missing with zero\n  hex_trip &lt;- hexagon_active %&gt;%\n    dplyr::select(HEX_ID, geometry) %&gt;%\n    dplyr::left_join(var_df, by = \"HEX_ID\") %&gt;%\n    dplyr::mutate(Trips = dplyr::coalesce(Trips, 0))\n\n  # run Moran’s I permutation test (999 simulations)\n  mor &lt;- spdep::moran.mc(\n    x           = hex_trip$Trips,\n    listw       = lw_d_B,\n    nsim        = 999,\n    zero.policy = TRUE\n  )\n\n  cat(\"\\n----\", label, \"----\\n\")\n  print(mor)\n  return(mor)\n}\n\n# 2) Execute for each peak period (reuse trip tables from Section 6)\nmor_am  &lt;- compute_moran(tp_am,  \"Weekday Morning (06–09)\")\n\n\n---- Weekday Morning (06–09) ----\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hex_trip$Trips \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3628, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\nmor_pm  &lt;- compute_moran(tp_pm,  \"Weekday Afternoon (17–19)\")\n\n\n---- Weekday Afternoon (17–19) ----\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hex_trip$Trips \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.15426, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\nmor_wk  &lt;- compute_moran(tp_wk,  \"Weekend / Holiday (11–19)\")\n\n\n---- Weekend / Holiday (11–19) ----\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hex_trip$Trips \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.24168, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe results show that all three Moran’s I tests have a \\(p\\)-value of 0.001, confirming statistically significant spatial clustering of trip intensities at the 0.1% level. During the weekday morning (06–08), Moran’s I = 0.3628 with p = 0.001, indicating strong and highly significant clustering where high-trip hexagons are located close to other high-trip areas. For the weekday afternoon (17–19), Moran’s I = 0.1543 with p = 0.001, suggesting that although clustering remains statistically significant, its intensity is much weaker, reflecting a more dispersed evening pattern. In the weekend or holiday period (11–19), Moran’s I = 0.2417 with p = 0.001, showing moderate yet statistically significant clustering, where trips remain locally concentrated but less tightly grouped than in the morning peak. Overall, the consistently low \\(p\\)-values confirm that all observed spatial patterns are real and not due to random variation.\n\n\n8.2 Moran scatterplot diagnostics for all peak periods\nThis section provides a visual diagnosis of spatial dependence for each peak period using the Moran scatterplot. The scatterplot relates the standardised value of the analysis variable on the horizontal axis to its spatial lag on the vertical axis. The spatial lag is the weighted average of neighbouring values computed with the distance band weights of seven hundred fifty meters that were established in Section seven. The slope of the fitted line equals the Global Moran I value, therefore the plot gives an immediate and interpretable picture of the direction and strength of spatial autocorrelation. Points in the upper right quadrant indicate high values surrounded by high neighbours which is a high high pattern, while points in the lower left quadrant indicate low low patterns. Off diagonal quadrants reveal spatial outliers. We generate one interactive figure for each required time window using the same data objects created in Section six and the same spatial weights created in Section seven. The use of a single helper function keeps the workflow compact and ensures exact reproducibility and consistency of settings across the three periods.\n\n# --- 8.2 Moran scatterplot diagnostics for AM, PM, Weekend (spdep) --------\n\n# Use a row-standardised listw for the scatter (standard in Hands-on 5a)\nlw_d_W &lt;- spdep::nb2listw(nb_d, style = \"W\", zero.policy = TRUE)\n\n# Helper: build one Moran scatter using spdep::moran.plot\nmoran_scatter &lt;- function(trip_table, label) {\n\n  # 1) Prepare the data vector (reuse trips from Section 6)\n  var_df &lt;- trip_table %&gt;%\n    dplyr::select(HEX_ID, TRIPS) %&gt;%\n    dplyr::rename(Trips = TRIPS)\n\n  # 2) Join to geometry table (only to keep HEX_ID consistent)\n  hex_trip &lt;- hexagon_active %&gt;%\n    dplyr::select(HEX_ID, geometry) %&gt;%\n    dplyr::left_join(var_df, by = \"HEX_ID\") %&gt;%\n    dplyr::mutate(Trips = dplyr::coalesce(Trips, 0))\n\n  # 3) Sanity checks\n  stopifnot(is.numeric(hex_trip$Trips))\n  stopifnot(length(hex_trip$Trips) == length(nb_d))\n\n  # 4) Moran scatterplot (base graphics)\n  spdep::moran.plot(\n    x            = hex_trip$Trips,     # analysis vector\n    listw        = lw_d_W,             # row-standardised weights\n    labels       = hex_trip$HEX_ID,    # optional id labels\n    xlab         = \"Standardised Trips\",\n    ylab         = \"Spatial lag of Trips\",\n    main         = paste0(\"Moran scatterplot: \", label),\n    zero.policy  = TRUE,               # safe at boundaries\n    pch          = 20, cex = 0.6, col = \"grey25\"  # tidy styling\n  )\n}\n\n# Produce all three plots (printed one after another, no overlap)\nmoran_scatter(tp_am, \"Weekday Morning 06–09\")\n\n\n\n\n\n\n\nmoran_scatter(tp_pm, \"Weekday Afternoon 17–20\")\n\n\n\n\n\n\n\nmoran_scatter(tp_wk, \"Weekend / Holiday 11–20\")\n\n\n\n\n\n\n\n\nThe Moran scatterplots show consistent positive slopes across all time periods, confirming spatial clustering of trip intensity. During weekday mornings (06–09), the slope is steepest, reflecting strong clustering where hexagons with high trip counts are surrounded by others with similarly high values, likely representing major commuter origins. In the weekday afternoon (17–20), the slope is flatter, indicating weaker clustering and a more dispersed spatial pattern as trips spread across different zones. For weekends and holidays (11–20), the slope increases slightly compared to the afternoon, showing moderate clustering linked to leisure or retail movements. Outlier hexagons such as H0117, H0308, and H0521 maintain high trip densities and spatial lags, highlighting persistent local hotspots across all periods. Overall, the plots confirm positive spatial dependence, strongest in the morning and weakest during evening hours.\n\n\n8.3 Permutation histogram of Moran’s I (AM, PM, Weekend)\nA custom function is created in this section to compute and display the permutation-based distribution of Moran’s I for each time period. The workflow first extracts the hexagon identifier and trip counts, standardises variable names, and merges them with the spatial geometry. Missing trip values are replaced with zero to maintain data completeness. Using the same 750-meter spatial weights, the function runs 999 Monte-Carlo permutations to simulate the expected distribution of Moran’s I under random conditions. A histogram of simulated values is then drawn, with the observed Moran’s I marked as a red line to highlight deviation from randomness. Finally, the procedure is applied sequentially to the morning, afternoon, and weekend datasets, providing a consistent visual comparison of spatial autocorrelation strength across different periods of the day.\n\n# --- 8.3 Permutation histogram of Moran’s I (AM, PM, Weekend) -------------\n\nlibrary(spdep)\nlibrary(dplyr)\n\n# helper: compute Moran’s I and plot its permutation histogram\nmoran_hist &lt;- function(trip_table, label) {\n\n  # keep HEX_ID and TRIPS from Section 6, standardise the name\n  var_df &lt;- trip_table %&gt;%\n    dplyr::select(HEX_ID, TRIPS) %&gt;%\n    dplyr::rename(Trips = TRIPS)\n\n  # join to geometry from Section 7 and fill any missing with zero\n  hex_trip &lt;- hexagon_active %&gt;%\n    dplyr::select(HEX_ID, geometry) %&gt;%\n    dplyr::left_join(var_df, by = \"HEX_ID\") %&gt;%\n    dplyr::mutate(Trips = dplyr::coalesce(Trips, 0))\n\n  # permutation Moran’s I using the SAME distance-band listw (750 m)\n  mor &lt;- spdep::moran.mc(\n    x           = hex_trip$Trips,   # numeric vector\n    listw       = lw_d_B,           # distance-band weights from Section 7\n    nsim        = 999,              # as in Hands-on 5a\n    zero.policy = TRUE\n  )\n\n  # plot the permutation distribution (Hands-on 5a style)\n  hist(mor$res,\n       breaks = 30,\n       col    = \"grey80\",\n       border = \"white\",\n       main   = paste0(\"Permutation distribution of Moran’s I — \", label),\n       xlab   = \"Simulated Moran’s I\"\n  )\n  abline(v = as.numeric(mor$statistic), col = \"red\", lwd = 2)\n  legend(\"topright\",\n         legend = c(\n           paste0(\"Observed I = \", round(as.numeric(mor$statistic), 2)),\n           paste0(\"p-value = \", format.pval(mor$p.value, digits = 2))\n         ),\n         bty = \"n\", text.col = \"black\")\n  invisible(mor)\n}\n\n# generate all three histograms (one after another, no overlap)\nmor_am &lt;- moran_hist(tp_am, \"Weekday Morning 06–09\")\n\n\n\n\n\n\n\nmor_pm &lt;- moran_hist(tp_pm, \"Weekday Afternoon 17–20\")\n\n\n\n\n\n\n\nmor_wk &lt;- moran_hist(tp_wk, \"Weekend/Holiday 11–20\")\n\n\n\n\n\n\n\n\nThe permutation histograms demonstrate that all three observed Moran’s I values lie far to the right of the simulated distributions, confirming strong and statistically significant positive spatial autocorrelation (p = 0.001). For the weekday morning (06–09), the observed Moran’s I of 0.36 indicates pronounced clustering of high-trip areas, consistent with concentrated commuting flows. During the weekday afternoon (17–20), Moran’s I drops to 0.15, showing weaker but still significant clustering as trips become more dispersed. On weekends and holidays (11–20), Moran’s I reaches 0.24, reflecting moderate clustering where travel activity remains spatially structured but less intense than morning peaks. In all cases, the clear separation between observed and simulated values validates the rejection of spatial randomness in Singapore’s trip distribution patterns.\n\n\n8.4 Local Moran’s I (LISA) analysis for all peak periods\nThis section decomposes the global statistic into cell level contributions to identify where clustering or spatial outliers occur. For each peak period, the analysis variable is the total trip count per hexagon created in Section 6. Geometry and the 759 meters distance band weights from Section 7 are reused without modification to preserve complete methodological consistency. The local Moran statistic is estimated by permutation with 999 simulations. The returned values include the local statistic, its expected value under the null, a \\(z\\) score, and the permutation \\(p\\) value. Significant results are classified into four canonical categories of local spatial association, namely high high, low low, high low, and low high. These categories are mapped one period at a time using the same cartographic styling as before, with a centered bold title, a framed legend within the plotting frame, and no overlapping outputs. This structure provides operational evidence of morning and afternoon commuting hot belts and the weekend leisure pattern, and it prepares the way for the spatio temporal hot spot analysis in the next section.\n\n# --- 8.4 LISA with explicit classification (Hands-on 5a/5b pattern) --------\n\n# 0) Row-standardised weights for LISA (Hands-on uses W)\nlw_d_W &lt;- spdep::nb2listw(nb_d, style = \"W\", zero.policy = TRUE)\n\n# 1) Helper: compute permutation LISA + cluster labels for one period\ncompute_lisa_sf &lt;- function(trip_tbl, nsim = 999, alpha = 0.05) {\n\n  # attach period trips to geometry; replace missing with zero\n  hex_trip &lt;- hexagon_active %&gt;%\n    dplyr::select(HEX_ID, geometry) %&gt;%\n    dplyr::left_join(\n      trip_tbl %&gt;% dplyr::select(HEX_ID, TRIPS) %&gt;% dplyr::rename(Trips = TRIPS),\n      by = \"HEX_ID\"\n    ) %&gt;%\n    dplyr::mutate(Trips = dplyr::coalesce(Trips, 0))\n\n  # z-standardise Trips and compute spatial lag of z (Hands-on diagnostic)\n  z    &lt;- as.numeric(scale(hex_trip$Trips))\n  lagz &lt;- spdep::lag.listw(lw_d_W, z, zero.policy = TRUE)\n\n  # permutation Local Moran's I (Hands-on 5b)\n  lm &lt;- spdep::localmoran_perm(\n    x           = hex_trip$Trips,\n    listw       = lw_d_W,\n    nsim        = nsim,\n    alternative = \"two.sided\",\n    zero.policy = TRUE\n  )\n\n  # spdep column names (perm version): \"Ii\",\"E.Ii\",\"Var.Ii\",\"Z.Ii\",\"Pr(z != E(Ii))\"\n  p_col &lt;- \"Pr(z != E(Ii))\"\n\n  # explicit cluster classification (no cut(), no hidden labels)\n  cls &lt;- dplyr::case_when(\n    lm[, p_col] &lt; alpha &  z &gt; 0 & lagz &gt; 0 ~ \"High–High\",\n    lm[, p_col] &lt; alpha &  z &lt; 0 & lagz &lt; 0 ~ \"Low–Low\",\n    lm[, p_col] &lt; alpha &  z &gt; 0 & lagz &lt; 0 ~ \"High–Low\",\n    lm[, p_col] &lt; alpha &  z &lt; 0 & lagz &gt; 0 ~ \"Low–High\",\n    TRUE                                   ~ \"Not Sig.\"\n  )\n\n  # bind results into sf\n  hex_trip %&gt;%\n    dplyr::mutate(\n      Ii     = lm[, \"Ii\"],\n      Z_Ii   = lm[, \"Z.Ii\"],\n      pvalue = lm[, p_col],\n      z      = z,\n      lag_z  = lagz,\n      cluster = factor(cls,\n        levels = c(\"High–High\",\"Low–Low\",\"High–Low\",\"Low–High\",\"Not Sig.\")\n      )\n    )\n}\n\n# 2) Helper: produce a publication map (categorical fill, framed legend)\nplot_lisa_map &lt;- function(sf_obj, title_text) {\n  pal &lt;- c(\n    \"High–High\" = \"#d7191c\",\n    \"Low–Low\"   = \"#2c7bb6\",\n    \"High–Low\"  = \"#fdae61\",\n    \"Low–High\"  = \"#abd9e9\",\n    \"Not Sig.\"  = \"grey85\"\n  )\n  tmap_mode(\"plot\")\n  tmap_options(component.autoscale = FALSE)  # keep legend within frame\n\n  tm_shape(sg_main) +                        # &lt;- mainland boundary from Section 5\n    tm_borders(col = \"grey40\", lwd = 0.8) +  # coastline/outline\n  tm_shape(sf_obj) +\n    tm_fill(\"cluster\", palette = pal, style = \"cat\",\n            title = \"LISA cluster type\") +\n    tm_borders(col = \"grey30\", lwd = 0.3) +  # hex borders\n    tm_title(title_text,                     # &lt;-- v4 title API\n             position = c(\"center\", \"top\"),\n             fontface = \"bold\") +\n    tm_layout(\n      legend.position = c(\"right\",\"bottom\"),\n      legend.bg.color = \"white\",\n      legend.frame    = TRUE,\n      frame           = TRUE,                # outer map frame\n      outer.margins   = 0,\n      inner.margins   = c(0.02, 0.03, 0.2, 0.02)\n    )\n}\n\n# 3) Compute and draw each period (reusing tp_am, tp_pm, tp_wk from Section 6)\nlisa_am &lt;- compute_lisa_sf(tp_am)  ; map_am &lt;- plot_lisa_map(lisa_am,\n  \"Local Moran’s I clusters – Weekday Morning 06–09\")\nmap_am\n\n\n\n\n\n\n\nlisa_pm &lt;- compute_lisa_sf(tp_pm)  ; map_pm &lt;- plot_lisa_map(lisa_pm,\n  \"Local Moran’s I clusters – Weekday Afternoon 17–20\")\nmap_pm\n\n\n\n\n\n\n\nlisa_wk &lt;- compute_lisa_sf(tp_wk)  ; map_wk &lt;- plot_lisa_map(lisa_wk,\n  \"Local Moran’s I clusters – Weekend/Holiday 11–20\")\nmap_wk\n\n\n\n\n\n\n\n\nThe Local Moran’s I cluster maps reveal distinct spatial patterns of trip intensity across time periods. During the weekday morning (06–09), numerous High–High clusters appear in central and eastern areas, reflecting strong concentrations of origin trips linked to commuter movement toward employment hubs. In the weekday afternoon (17–20), fewer and smaller High–High clusters emerge, suggesting a more dispersed travel pattern during evening returns. Meanwhile, Low–Low clusters remain sparse and scattered, representing areas with consistently low trip activity. On weekends and holidays (11–20), clustering re-intensifies in select central and regional areas, likely driven by leisure and retail travel. The consistent presence of High–High clusters in core zones across all periods confirms persistent urban mobility hotspots, whereas reduced clustering during afternoon hours indicates more spatially even trip distribution outside peak commuting times."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#spatial-hot-spot-and-cold-spot-analysis-getis-ord-gi",
    "href": "Take-home_Ex02/take-home_ex02.html#spatial-hot-spot-and-cold-spot-analysis-getis-ord-gi",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "10 Spatial Hot Spot and Cold Spot Analysis (Getis-Ord Gi*)",
    "text": "10 Spatial Hot Spot and Cold Spot Analysis (Getis-Ord Gi*)\nThis section applies the Local Getis Ord Gi* statistic to identify statistically significant concentrations of high and low trip activities within the study area. The analysis builds upon the previously confirmed spatial weights and the hexagonal grid framework to evaluate whether the observed trip intensities form meaningful spatial clusters beyond what would be expected by random chance. Each analytical time window, including the weekday morning, weekday afternoon, and weekend or holiday periods, follows an identical spatial configuration to preserve methodological consistency. The workflow proceeds sequentially, beginning with the validation of spatial weight matrices, followed by the computation of local Gi* statistics and associated p values, and concluding with the visualization and classification of significant clusters. Every analytical step is executed reproducibly within the R environment through the sfdep and tmap packages, ensuring methodological alignment between statistical computation and geographic representation. The complete process transforms raw trip counts into interpretable spatial indicators of movement intensity, providing a rigorous foundation for understanding urban mobility patterns across temporal contexts.\n\n10.1 Confirm weight objects (Include Self for Gi*)\nThe analysis begins by validating the spatial weight configurations required for the Gi* computation. The code first checks that the neighbour list object (nb_d_self) and its corresponding weight list (lw_d_B_self) exist and inherit from the correct nb and listw classes. The procedure explicitly includes each hexagon’s own location as a neighbour (include.self = TRUE) because Gi* requires self-inclusion to measure local concentration accurately. A sanity check confirms that the length of the neighbour structure matches the number of active hexagons (hexagon_active). These tests guarantee internal consistency between the spatial index and geometric dataset, preventing mismatched geometries or weight lengths that could invalidate later \\(z\\)-score and \\(p\\)-value calculations. By ensuring that the self-inclusive weights are both properly formatted and structurally aligned, this step forms the computational backbone for all subsequent spatial permutation tests and clustering assessments.\n\n# Confirm 750 m weights from Section 7\nstopifnot(inherits(nb_d, \"nb\"),\n          inherits(lw_d_B, \"listw\"))\n\n# Include-self versions (required for Gi*)\nnb_d_self   &lt;- if (exists(\"nb_d_self\")) nb_d_self else spdep::include.self(nb_d)\nlw_d_B_self &lt;- if (exists(\"lw_d_B_self\")) lw_d_B_self else\n                 spdep::nb2listw(nb_d_self, style = \"W\", zero.policy = TRUE)\n\n\n\n10.2 Helper functions (Trip Alignment & Computation)\nThis section develops the computational tools required to implement the Local Getis Ord Gi* analysis reliably across multiple time periods. It introduces two essential helper functions that translate cleaned trip records into forms suitable for spatial computation. The first function ensures that all trip frequencies correspond exactly to their matching hexagonal cells within the analytical grid, producing a fully numeric vector that preserves spatial order and completeness. The second function performs the statistical calculation of the Local Gi* statistic using the previously validated spatial weights. Together, these procedures guarantee that each analytical dataset follows identical data structures and spatial relationships before entering the permutation process. The section also verifies that all outputs preserve consistent record counts and geometry alignment, ensuring no mismatch between the trip data and the spatial framework. Through these supporting functions, the analysis gains precision, reproducibility, and computational integrity, establishing a seamless bridge between data preparation and statistical computation in subsequent sections.\n\n10.2.1 Convert trips to numeric vector aligned to grid\nThis section defines a helper routine that converts the tabulated trip data into a numeric vector perfectly aligned with the analytical hexagon grid. It performs a join between trip counts and geometry using the common hexagon identifier, replaces missing trip values with zero, and confirms that the resulting vector length matches the number of grid cells. Logical checks verify that all values are numeric and finite, ensuring that no data mismatch affects the spatial weights. The completed vector becomes the input for statistical analysis in later sections, providing a validated numerical representation of trip intensity that is consistent across all analytical windows.\n\ntrips_vec &lt;- function(trip_table) {\n  var_df &lt;- trip_table %&gt;%\n    transmute(hex_id = tolower(HEX_ID),\n              Trips  = as.numeric(TRIPS))\n\n  x &lt;- hexagon_active %&gt;%\n    transmute(hex_id = tolower(HEX_ID), geometry) %&gt;%\n    left_join(var_df, by = \"hex_id\") %&gt;%\n    mutate(Trips = replace_na(Trips, 0))\n\n  v &lt;- x %&gt;% st_drop_geometry() %&gt;% pull(Trips)\n\n  stopifnot(is.numeric(v),\n            !any(is.na(v)),\n            length(v) == nrow(hexagon_active))\n  v\n}\n\n\n\n10.2.2 Compute Local Gi* statistic\nThis section implements the computation of the Local Gi* statistic. The function compute_gistar uses the aligned trip vector and executes 999 random permutations with zero policy enabled to handle empty neighbour sets. The resulting \\(z\\)-score and two-sided \\(p\\) value quantify the degree and significance of clustering within each hexagon. The code then attaches the results to the analytical geometry, ensuring one record per spatial cell. Validation checks confirm identical row counts between statistical output and geometry, ensuring spatial consistency. This process yields an integrated spatial dataset containing both numeric and geometric information, ready for temporal comparison and visualisation.\n\ncompute_gistar &lt;- function(trip_table, label, nsim = 999) {\n  v &lt;- trips_vec(trip_table)\n\n  gi_tbl &lt;- spdep::localG_perm(\n    x          = v,\n    listw      = lw_d_B_self,\n    nsim       = nsim,\n    zero.policy = TRUE\n  )\n\n  gi_df &lt;- as.data.frame(gi_tbl)\n  names(gi_df)[1] &lt;- \"Gi\"\n\n  # Two-sided p-value from z-score\n  gi_df$p_sim &lt;- 2 * pnorm(-abs(gi_df$Gi))\n\n  out &lt;- bind_cols(hexagon_active, gi_df) %&gt;%\n    mutate(.window = label)\n\n  stopifnot(nrow(out) == nrow(hexagon_active))\n  out\n}\n\n\n\n\n10.3 Compute Local Gi* for all windows\nAfter confirming the helper functions, the computation is applied to each temporal dataset representing the weekday morning, weekday afternoon, and weekend or holiday periods. The resulting outputs, labelled HCSA_am, HCSA_pm, and HCSA_wk, contain each hexagon’s Gi* \\(z\\)-score and simulated p value together with its temporal label. A diagnostic message confirms that all datasets contain identical numbers of observations and identical spatial structures. This repetition under uniform parameters allows precise comparison of spatial clustering strength between different time frames. The procedure captures temporal variation in the intensity of hot and cold clusters across Singapore, making it possible to observe whether strong morning concentrations persist into later periods or dissipate. The completion of this section produces three statistically comparable datasets that serve as the analytical input for the mapping process described in the following section.\n\nHCSA_am &lt;- compute_gistar(tp_am, \"Weekday Morning (06–09)\")\nHCSA_pm &lt;- compute_gistar(tp_pm, \"Weekday Afternoon (17–20)\")\nHCSA_wk &lt;- compute_gistar(tp_wk, \"Weekend / Holiday (11–20)\")\n\n# Basic verification\nstopifnot(\n  nrow(HCSA_am) == nrow(hexagon_active),\n  nrow(HCSA_pm) == nrow(hexagon_active),\n  nrow(HCSA_wk) == nrow(hexagon_active),\n  is.numeric(HCSA_am$Gi),\n  is.numeric(HCSA_pm$Gi),\n  is.numeric(HCSA_wk$Gi)\n)\n\n\n\n10.4 Mapping local Gi* p-values\nThis section transforms numerical results into spatially interpretable maps that visualise the Local Gi* \\(z\\)-scores. The mapping function uses tmap to display the Singapore outline, overlay the analytical hexagon grid, and fill each cell according to its \\(z\\)-score value. The chosen colour palette is diverging, with red tones representing positive clustering and blue tones representing negative clustering, while white represents values close to the mean. The map layout follows the standard formatting with bold titles, centred legends, and consistent margins to preserve visual uniformity across all time periods. Through these maps, users can immediately distinguish areas of elevated trip intensity from those with lower activity. The maps also provide a diagnostic check for spatial continuity, revealing whether the clustering patterns are geographically contiguous or dispersed. This step converts statistical output into an interpretable visual narrative that guides subsequent analysis of significance.\n\ntmap_mode(\"plot\")\n\nmap_gi &lt;- function(sf_obj, title_text) {\n  tm_shape(sg_outline) + tm_borders(col = \"grey40\", lwd = 0.7) +\n  tm_shape(sf_obj) +\n    tm_fill(\"Gi\",\n            style   = \"pretty\",\n            n       = 7,\n            palette = \"-brewer.RdBu\",\n            midpoint = 0,\n            title   = \"Gi* z-score\") +\n    tm_borders(col = \"grey85\", lwd = 0.4) +\n    tm_layout(\n      main.title = title_text,\n      main.title.position = \"center\",  # Ensure exact spelling\n      main.title.fontface = \"bold\",\n      outer.margins = c(0.02, 0.02, 0.02, 0.02),\n      main.title.size = 1.2,           # Optional: adjust title size\n      main.title.color = \"black\",      # Optional: adjust color\n      legend.position = c(\"right\",\"bottom\"),\n      legend.frame = TRUE\n    )\n}\n\ngi_am &lt;- map_gi(HCSA_am, \"Local Gi* — Weekday Morning (06–09)\")\ngi_pm &lt;- map_gi(HCSA_pm, \"Local Gi* — Weekday Afternoon (17–20)\")\ngi_wk &lt;- map_gi(HCSA_wk, \"Local Gi* — Weekend / Holiday (11–20)\")\n\ngi_am; gi_pm; gi_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe three Local Gi* maps together present a detailed view of spatial clustering patterns of trip activity across different temporal contexts. During the weekday morning period, the map reveals intense hot spots represented by darker red hexagons located in the western, northeastern, and central regions. These areas correspond to concentrated morning departures associated with residential neighbourhoods where large numbers of commuters begin their daily journeys. The presence of light blue cells along the northern and southern corridors indicates localised cold spots, suggesting lower trip activity during this time window.\nIn the weekday afternoon period, several of the morning hot spots persist but appear more dispersed. This shift indicates that while core mobility zones remain active, their intensity diminishes as trip generation becomes more balanced between origins and destinations. The recurring pattern of both positive and negative \\(z\\)-scores suggests the redistribution of flows as commuters return from work.\nOn weekends and holidays, the pattern evolves further into more extensive red clusters in central and eastern regions, highlighting leisure-oriented and recreational travel rather than work-related mobility. The appearance of multiple moderate-intensity clusters across the island suggests more spatially diversified travel behaviour during non-working days.\nOverall, the Local Gi* analysis confirms that spatial clustering of trip activity varies systematically with time, reflecting transitions between commuting and social travel functions. These results demonstrate how temporal segmentation helps reveal distinct behavioural regimes of urban mobility that would otherwise be obscured in aggregated data.\n\n\n10.5 Mapping \\(p\\)-Values of Local Gi*\nThe next analytical step assesses the statistical reliability of the apparent hot and cold zones by mapping the simulated p values corresponding to each \\(z\\)-score. The mapping function applies fixed classification intervals to highlight confidence levels at 0.001, 0.01, 0.05, 0.10, and 1.00. Lower \\(p\\) values appear in darker shades, identifying regions where clustering is statistically significant, whereas lighter tones mark areas consistent with random variation. The visual format mirrors that of the z-score maps to enable direct cross-comparison. By reviewing both the intensity and the p value maps together, the analyst can separate visually strong but insignificant clusters from genuinely significant hot or cold areas. This process enhances analytical reliability by ensuring that observed spatial concentrations are statistically defensible rather than products of chance. The mapped \\(p\\) values therefore serve as the validation layer preceding categorical classification.\n\nmap_p &lt;- function(sf_obj, title_text) {\n  tm_shape(sg_outline) + tm_borders(col = \"grey40\", lwd = 0.7) +\n  tm_shape(sf_obj %&gt;% filter(is.finite(p_sim))) +\n    tm_fill(\n      col      = \"p_sim\",\n      style    = \"fixed\",\n      breaks   = c(0, 0.001, 0.01, 0.05, 0.10, 1),\n      palette  = \"-brewer.Reds\",\n      title    = \"p-value\",\n      midpoint = NA,\n      alpha    = 1\n    ) +\n    tm_borders(col = \"grey85\", lwd = 0.4) +\n    tm_layout(\n      main.title          = title_text,\n      main.title.position = \"center\",\n      main.title.fontface = \"bold\",\n      legend.position     = c(\"right\",\"bottom\"),\n      legend.frame        = TRUE\n    )\n}\n\np_am &lt;- map_p(HCSA_am, \"p-values of Local Gi* — Weekday Morning (06–09)\")\np_pm &lt;- map_p(HCSA_pm, \"p-values of Local Gi* — Weekday Afternoon (17–20)\")\np_wk &lt;- map_p(HCSA_wk, \"p-values of Local Gi* — Weekend / Holiday (11–20)\")\n\np_am; p_pm; p_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(p\\)-value maps for the Local Gi* analysis provide statistical confirmation of where spatial clustering patterns are most reliable across different time periods. During the weekday morning period, dark red hexagons dominate the western, northeastern, and central sectors of the island, indicating \\(p\\)-values below 0.01. These areas reflect highly significant clusters of trip activity, suggesting consistent morning travel concentration within residential and employment zones. The dense grouping of low \\(p\\)-values confirms that the observed spatial patterns are unlikely to occur by random chance and represent genuine hot spot formations.\nIn the weekday afternoon period, significant areas become more dispersed, and the total number of hexagons with \\(p\\)-values below 0.01 slightly decreases. This reduction implies greater variability in afternoon trip behaviour as travel flows diversify between work return journeys and secondary activities. Nonetheless, certain stable clusters persist, suggesting enduring transport corridors and mixed-use nodes with high activity density.\nThe weekend and holiday map shows renewed intensification of significant areas, particularly in central and eastern regions. The spread of dark red hexagons indicates broader spatial clustering of leisure-related movement. This distribution implies that recreational and shopping trips become the dominant source of spatial dependence during non-working days.\nAcross all three time windows, the persistence of low \\(p\\)-values validates the robustness of the Local Gi* results. The consistent appearance of significant cells across multiple periods demonstrates that spatial clustering in urban mobility is a stable feature of the transport landscape rather than a transient phenomenon driven by temporal fluctuations.\n\n\n10.6 Hot/Cold spot classification at \\(p\\) &lt; 0.05\nThe final stage consolidates all previous computations and maps into a categorical representation of statistically significant clusters. Using the mapping function map_cluster, each hexagon is classified into one of three categories: Hot Spot, Cold Spot, or Insignificant. The classification depends on both the direction of the Gi* \\(z\\)-score and the associated p value threshold of less than 0.05. The colour scheme is fixed, with red indicating Hot Spots, blue indicating Cold Spots, and grey indicating Insignificant cells. This uniform convention allows easy visual comparison between different temporal windows. The resulting maps provide a concise yet comprehensive summary of spatial clustering across the study area, clearly identifying where trip activity intensifies or weakens over time. These classified outcomes represent the most interpretable stage of the Gi* workflow, forming the empirical basis for subsequent discussion and interpretation of spatial mobility behaviour.\n\n# --- Classify Gi* into clusters (α = 0.05)--------------------------------------------\n# p &lt;= .05 significant; Hot if Gi&gt;0, Cold if Gi&lt;0; else Insignificant\nHCSA_classify &lt;- function(sf_obj, alpha = 0.05) {\n  sf_obj |&gt;\n    dplyr::mutate(\n      HCSA_cluster = dplyr::case_when(\n        is.finite(p_sim) & p_sim &lt;= alpha & is.finite(Gi) & Gi &gt;  0 ~ \"Hot spot\",\n        is.finite(p_sim) & p_sim &lt;= alpha & is.finite(Gi) & Gi &lt;  0 ~ \"Cold spot\",\n        is.finite(p_sim) & p_sim  &gt; alpha                           ~ \"Insignificant\",\n        TRUE ~ NA_character_\n      ),\n      # lock the factor order to control legend order\n      HCSA_cluster = factor(HCSA_cluster,\n                            levels = c(\"Insignificant\",\"Hot spot\",\"Cold spot\"),\n                            ordered = TRUE)\n    )\n}\n\nHCSA_am_c &lt;- HCSA_classify(HCSA_am)\nHCSA_pm_c &lt;- HCSA_classify(HCSA_pm)\nHCSA_wk_c &lt;- HCSA_classify(HCSA_wk)\n\n# --- Plot helper (fixed order + explicit colors, no “Missing”) -----------------------\ncluster_levels &lt;- c(\"Insignificant\",\"Hot spot\",\"Cold spot\")\ncluster_colors &lt;- c(\"grey80\",\"red\",\"blue\")   # same order as levels\n\nmap_cluster &lt;- function(sf_obj, title_text) {\n  # drop NA so the legend doesn’t show “Missing”\n  sf_plot &lt;- sf_obj |&gt;\n    dplyr::filter(!is.na(HCSA_cluster)) |&gt;\n    # re-assert factor levels in case something changed upstream\n    dplyr::mutate(HCSA_cluster = factor(as.character(HCSA_cluster),\n                                        levels = cluster_levels,\n                                        ordered = TRUE))\n\n  tm_shape(sg_outline) + tm_borders(col = \"grey40\", lwd = 0.7) +\n  tm_shape(sf_plot) +\n    tm_fill(col = \"HCSA_cluster\",\n            palette = cluster_colors,   # categorical palette (tmap v4)\n            showNA  = FALSE,\n            title   = \"HCSA Cluster\") +\n    tm_borders(col = \"grey85\", lwd = 0.4) +\n    tm_layout(\n      main.title         = title_text,\n      main.title.position= \"center\",\n      main.title.fontface= \"bold\",\n      legend.position    = c(\"right\",\"bottom\"),\n      legend.frame       = TRUE\n    )\n}\n\n# --- Draw the three maps --------------------------------------------------------------\ncm_am &lt;- map_cluster(HCSA_am_c, \"HCSA Clusters (p &lt; 0.05) — Morning (06–09)\")\ncm_pm &lt;- map_cluster(HCSA_pm_c, \"HCSA Clusters (p &lt; 0.05) — Afternoon (17–20)\")\ncm_wk &lt;- map_cluster(HCSA_wk_c, \"HCSA Clusters (p &lt; 0.05) — Weekend / Holiday (11–20)\")\n\ncm_am; cm_pm; cm_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hot and Cold Spot Analysis (HCSA) cluster maps for weekday morning, weekday afternoon, and weekend or holiday periods collectively reveal the evolution of Singapore’s spatial mobility intensity across temporal contexts, each analysed at \\(p\\) less than 0.05 significance. In the weekday morning map, a dense network of red hexagonal clusters signifies statistically significant hot spots of trip activity, particularly concentrated across the northern(Yishun, and Woodland), Northern East (Punggol, Sengkang, Hougang), Western (Jurong West, Bukit Batok), Central (Tao Payoh) and Eastern (Tampines, Bedok) portions of the island. These clusters represent high trip-generation zones that likely correspond to large-scale residential districts where outbound commuter traffic peaks between 6 to 9 a.m. The absence of blue cold spot cells indicates that low trip areas during this period follow a more random pattern without statistical clustering. The observed structure suggests a strong and spatially dependent morning mobility framework aligned with commuting towards central employment hubs.\nDuring the weekday afternoon period from 17 to 20 hours, the cluster distribution remains extensive but exhibits slightly lower density and increased dispersion. The presence of multiple smaller red patches indicates a diffusion of activity as people travel home or make short inter-district trips after work. This spatial fragmentation signifies a shift from collective commuter movements to more individualised trip purposes such as school pick-ups, errands, or leisure engagements. The patterns reinforce the dynamic transformation of Singapore’s travel behaviour from highly centralised in the morning to decentralised in the evening.\nBy contrast, the weekend or holiday map displays broader and more evenly distributed red clusters across central, southern, and eastern zones. These results reflect the transition from occupational to recreational mobility, as travel demand is driven by social, retail, and leisure activities. Unlike weekdays, trip concentration expands into non-core areas, highlighting the more balanced nature of urban activity during off-work periods.\nIn practical terms, these spatial patterns emphasise how weekday commuting follows structured residential-employment flows, whereas weekend mobility reflects lifestyle-oriented dispersion. Transport planners can use these temporal signatures to synchronise service frequency and route allocation, ensuring capacity aligns with empirically verified high-demand corridors during peak weekday hours and wider area accessibility on weekends."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#global-measure-of-spatial-autocorrelation-analysis",
    "href": "Take-home_Ex02/take-home_ex02.html#global-measure-of-spatial-autocorrelation-analysis",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "8 Global Measure of Spatial Autocorrelation Analysis",
    "text": "8 Global Measure of Spatial Autocorrelation Analysis\nThis section investigates the global spatial autocorrelation of origin trip intensity across the analytical hexagon grid. Rather than simply examining how values vary geographically, the analysis quantifies the extent to which neighbouring cells exhibit similar or dissimilar trip counts — that is, whether the observed pattern represents clustering, randomness, or dispersion. The global Moran’s I statistic is used as the principal measure to capture this spatial dependency, providing a single index summarising overall correlation across the study region. A positive Moran’s I indicates that cells with high (or low) trip values are located near others of similar magnitude, implying spatial clustering. Conversely, a negative Moran’s I denotes spatial dispersion where high and low values alternate spatially. The test operates on the row-standardised Queen contiguity weights constructed earlier, ensuring that each cell’s contribution is proportional to its number of neighbours. The resulting Moran’s I value, standard deviate, and \\(p\\)-value are derived under a randomisation hypothesis, allowing formal inference against the null of spatial independence. Through this procedure, we obtain a statistically robust understanding of whether trip distributions during peak periods are spatially structured, thereby establishing the foundation for subsequent LISA and hot/cold-spot analyses.\n\n8.1 Global Moran’s I statistical test for all peak periods\nThis section performs the Global Moran’s I test independently for the three defined peak periods: weekday morning, weekday afternoon, and weekend/holiday. Using the previously derived Queen-contiguity spatial-weights matrix, each hexagonal layer is tested under the null hypothesis of spatial randomness. The moran.test() computes the observed Moran’s I, expected value, variance, \\(z\\)-score, and corresponding \\(p\\)-value. A positive and significant Moran’s I (\\(p &lt; 0.001\\)) confirms that the trip intensities are spatially clustered during that period, while an insignificant result suggests no global pattern. Comparing Moran’s I values across all periods reveals whether clustering strength is consistent or time-specific, providing evidence of temporal variation in spatial dependency.\n\n# --- 8.1 Global Moran’s I (AM, PM, Weekend) ---------------------------------\n\n# helper: make the numeric vector aligned to the weights (reuses Section 6/7 objects)\nmake_x &lt;- function(trip_table) {\n  hexagon_active %&gt;%\n    dplyr::select(HEX_ID, geometry) %&gt;%\n    dplyr::left_join(trip_table %&gt;% dplyr::select(HEX_ID, TRIPS), by = \"HEX_ID\") %&gt;%\n    dplyr::mutate(Trips = dplyr::coalesce(TRIPS, 0)) %&gt;%\n    {.$Trips}\n}\n\nx_am &lt;- make_x(tp_am)\nx_pm &lt;- make_x(tp_pm)\nx_wk &lt;- make_x(tp_wk)\n\n# Moran's I test under randomisation\nmt_am &lt;- spdep::moran.test(x_am, listw = lw_d_B, zero.policy = TRUE, na.action = na.omit)\nmt_pm &lt;- spdep::moran.test(x_pm, listw = lw_d_B, zero.policy = TRUE, na.action = na.omit)\nmt_wk &lt;- spdep::moran.test(x_wk, listw = lw_d_B, zero.policy = TRUE, na.action = na.omit)\n\nmt_am; mt_pm; mt_wk   # print results\n\n\n    Moran I test under randomisation\n\ndata:  x_am  \nweights: lw_d_B  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 16.367, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.3627974234     -0.0012135922      0.0004946119 \n\n\n\n    Moran I test under randomisation\n\ndata:  x_pm  \nweights: lw_d_B  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 7.1164, p-value = 5.538e-13\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.1542609092     -0.0012135922      0.0004773017 \n\n\n\n    Moran I test under randomisation\n\ndata:  x_wk  \nweights: lw_d_B  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 10.997, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.2416785594     -0.0012135922      0.0004878754 \n\n\nThe randomisation tests show strong positive spatial autocorrelation for trip counts in all periods using the lw_d_B weights. For weekday morning (06–09), Moran’s I = 0.3628, \\(z\\) = 16.367, \\(p\\) &lt; 2.2×10⁻¹⁶, decisively rejecting spatial randomness. For weekday afternoon (17–20), Moran’s I = 0.1543, \\(z\\) = 7.116, \\(p\\) = 5.54×10⁻¹³, indicating weaker—but still highly significant—clustering. For weekend/holiday (11–19), Moran’s I = 0.2417, \\(z\\) = 10.997, \\(p\\) &lt; 2.2×10⁻¹⁶, implying moderate clustering. The “Expectation” of about −0.0012 is the theoretical mean of Moran’s I under the null (approximately zero for large samples), and the listed variances quantify sampling variability under randomisation. Together, large positive \\(I\\) values, very high \\(z\\)-scores, and extreme \\(p\\)-values confirm that neighbouring hexagons tend to have similar trip intensities, with clustering strongest in the morning, moderate on weekends, and weakest in the weekday afternoon.\n\n\n8.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 999 simulation will be performed.\n\n# Monte-Carlo Moran’s I (999 sims) and permutation histograms\nbperm_am &lt;- spdep::moran.mc(x_am, listw = lw_d_B, nsim = 999, zero.policy = TRUE, na.action = na.omit)\nbperm_pm &lt;- spdep::moran.mc(x_pm, listw = lw_d_B, nsim = 999, zero.policy = TRUE, na.action = na.omit)\nbperm_wk &lt;- spdep::moran.mc(x_wk, listw = lw_d_B, nsim = 999, zero.policy = TRUE, na.action = na.omit)\n\nbperm_am; bperm_pm; bperm_wk   # print Monte-Carlo outputs\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x_am \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3628, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x_pm \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.15426, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x_wk \nweights: lw_d_B  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.24168, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe Monte-Carlo simulation results confirm significant positive spatial autocorrelation in trip intensities across all time periods, using 999 random permutations. For the weekday morning (06–09), Moran’s I = 0.3628 with p = 0.001, indicating strong clustering of high-trip hexagons—typical of concentrated commuting activity. During the weekday afternoon (17–20), Moran’s I decreases to 0.1543, suggesting weaker but still significant clustering as trip origins become more spatially dispersed. On weekends and holidays (11–20), Moran’s I = 0.2417, showing moderate clustering, consistent with concentrated leisure or retail-related travel zones. The consistently low \\(p\\)-values (&lt; 0.001) across all three tests reject spatial randomness, confirming that trip distributions remain spatially dependent, though clustering strength varies with the time of day.\n\n\n8.3 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\n\n# Visualise permutation distributions\npar(mfrow = c(1,3))\nhist(bperm_am$res, freq = TRUE, breaks = 20, xlab = \"Simulated Moran's I\",\n     main = \"Permutation (AM 06–09)\")\nabline(v = 0, col = \"red\")\nhist(bperm_pm$res, freq = TRUE, breaks = 20, xlab = \"Simulated Moran's I\",\n     main = \"Permutation (PM 17–20)\")\nabline(v = 0, col = \"red\")\nhist(bperm_wk$res, freq = TRUE, breaks = 20, xlab = \"Simulated Moran's I\",\n     main = \"Permutation (Weekend/Holiday AM 11 – PM 20)\")\nabline(v = 0, col = \"red\")\n\n\n\n\n\n\n\npar(mfrow = c(1,1))\n\nThe permutation histograms illustrate the simulated distributions of Moran’s I under spatial randomness, with the red line marking the observed Moran’s I for each time period. In all three plots, the observed values lie far to the right of their respective simulated distributions, indicating strong positive spatial autocorrelation. The weekday morning (06–09) shows the largest deviation, confirming intense clustering of trip origins during peak commute hours. The weekday afternoon (17–20) displays a smaller separation, reflecting weaker clustering and greater dispersion of trips. The weekend/holiday (11–20) pattern falls between the two, showing moderate clustering likely related to leisure activity centres. Since the observed Moran’s I values are far outside the random distribution in every case, spatial randomness is rejected with high confidence, validating significant spatial dependence in all time periods.\n\n\n8.4 Global Moran’s I spatial correlogram\nThis section constructs the spatial correlogram of Moran’s I to assess how spatial autocorrelation changes with increasing distance. The correlogram plots Moran’s I values against successive distance bands derived from the spatial-weights structure, effectively tracing how similarity between cells weakens or reverses as neighbourhoods expand. A steep decline of Moran’s I toward zero implies a rapid loss of spatial dependency, whereas sustained positive values across several lags suggest broader clustering effects. This diagnostic complements the global Moran’s I test by revealing the scale and range of spatial association, thus offering deeper insight into the spatial processes shaping trip-generation patterns across different peak periods.\n\n# --- 8.2 Spatial correlogram (Moran’s I by lag order) -----------------------\nlibrary(spdep)\n\n# Use the neighbour list from Section 7 (nb_d) and W-style like in class\nMIcorr_am &lt;- spdep::sp.correlogram(nb_d, x_am, order = 6, method = \"I\", style = \"W\", zero.policy = TRUE)\nMIcorr_pm &lt;- spdep::sp.correlogram(nb_d, x_pm, order = 6, method = \"I\", style = \"W\", zero.policy = TRUE)\nMIcorr_wk &lt;- spdep::sp.correlogram(nb_d, x_wk, order = 6, method = \"I\", style = \"W\", zero.policy = TRUE)\n\n# plot each correlogram and print the tables (as shown in the notes)\nplot(MIcorr_am, main = \"Moran’s I correlogram – AM 06–09\");  print(MIcorr_am)\n\n\n\n\n\n\n\n\nSpatial correlogram for x_am \nmethod: Moran's I\n           estimate expectation    variance standard deviate Pr(I) two sided\n1 (825)  0.34247123 -0.00121359  0.00055348          14.6086       &lt; 2.2e-16\n2 (819)  0.19553739 -0.00122249  0.00034506          10.5923       &lt; 2.2e-16\n3 (819)  0.09213184 -0.00122249  0.00026472           5.7378       9.593e-09\n4 (819)  0.03223142 -0.00122249  0.00022700           2.2204         0.02639\n5 (817)  0.01084428 -0.00122549  0.00020162           0.8500         0.39531\n6 (815)  0.01886796 -0.00122850  0.00017947           1.5001         0.13358\n           \n1 (825) ***\n2 (819) ***\n3 (819) ***\n4 (819) *  \n5 (817)    \n6 (815)    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(MIcorr_pm, main = \"Moran’s I correlogram – PM 17–20\");  print(MIcorr_pm)\n\n\n\n\n\n\n\n\nSpatial correlogram for x_pm \nmethod: Moran's I\n           estimate expectation    variance standard deviate Pr(I) two sided\n1 (825)  0.15230015 -0.00121359  0.00053410           6.6426       3.082e-11\n2 (819)  0.07864794 -0.00122249  0.00033288           4.3777       1.200e-05\n3 (819)  0.04728148 -0.00122249  0.00025538           3.0352        0.002404\n4 (819)  0.03637830 -0.00122249  0.00021899           2.5409        0.011058\n5 (817)  0.01230407 -0.00122549  0.00019449           0.9701        0.331978\n6 (815)  0.03119926 -0.00122850  0.00017311           2.4647        0.013713\n           \n1 (825) ***\n2 (819) ***\n3 (819) ** \n4 (819) *  \n5 (817)    \n6 (815) *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(MIcorr_wk, main = \"Moran’s I correlogram – Weekend/Holiday 11–20\");  print(MIcorr_wk)\n\n\n\n\n\n\n\n\nSpatial correlogram for x_wk \nmethod: Moran's I\n           estimate expectation    variance standard deviate Pr(I) two sided\n1 (825)  0.22958564 -0.00121359  0.00054594           9.8779       &lt; 2.2e-16\n2 (819)  0.12041437 -0.00122249  0.00034032           6.5936       4.293e-11\n3 (819)  0.07813863 -0.00122249  0.00026108           4.9115       9.036e-07\n4 (819)  0.03378065 -0.00122249  0.00022388           2.3394        0.019317\n5 (817)  0.01418296 -0.00122549  0.00019885           1.0927        0.274526\n6 (815)  0.03340009 -0.00122850  0.00017699           2.6029        0.009244\n           \n1 (825) ***\n2 (819) ***\n3 (819) ***\n4 (819) *  \n5 (817)    \n6 (815) ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe spatial correlograms and tables show how Moran’s I changes across 6 lags for the 3 periods. In every case, lag 1 is strongly positive and highly significant, meaning neighbouring hexagons (at the closest lag) have similar trip intensities. The weekday morning (06–09) exhibits the largest short-range dependence (\\(I\\) ≈ 0.343, \\(z\\) ≈ 14.6, \\(p\\) &lt; 2.2e-16). Its autocorrelation declines rapidly with distance, becoming weak by lags 4–6; significance disappears from lag 5 onward, indicating a short spatial reach around the morning hotspots. The weekday afternoon (17–20) starts lower (\\(I\\) ≈ 0.152, \\(z\\) ≈ 6.64, \\(p\\) ≈ 3.1e-11) and decays steadily; lags 2–4 remain positive and significant but smaller, lag 5 is not significant, and lag 6 shows a small yet significant positive value, implying faint longer-range structure. Weekend/holiday (11–20) sits between morning and afternoon: strong at lag 1 (\\(I\\) ≈ 0.230, \\(z\\) ≈ 9.88, \\(p\\) &lt; 2.2e-16), still significant through lag 4, not significant at lag 5, and again modestly positive at lag 6. Overall, spatial dependence is strongest and most localized in the morning, weakest in the afternoon, and moderate on weekends. The attenuation patterns confirm that clustering diminishes with distance, with occasional long-range remnants at the farthest lag."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#local-measure-of-spatial-autocorrelation-analysis",
    "href": "Take-home_Ex02/take-home_ex02.html#local-measure-of-spatial-autocorrelation-analysis",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "9 Local Measure of Spatial Autocorrelation Analysis",
    "text": "9 Local Measure of Spatial Autocorrelation Analysis\n\n9.1 Computing Local Moran’s I\nIn this section, a custom function is designed to calculate the Local Moran’s I statistic for each analytical hexagon. The function integrates trip intensity data with spatial geometry, replaces missing values with zeros, and performs 999 random permutations using distance-based spatial weights. This computation produces multiple diagnostic outputs, including local Moran’s I values, variances, \\(z\\)-scores, and \\(p\\)-values. Separate analyses are then performed for morning, afternoon, and weekend periods to capture time-based spatial clustering variations.\n\nnsim &lt;- 999   # keep same as earlier sections\n\n# helper: join trips to hexagons, then compute local Moran's I (sfdep)\ncompute_lisa_sf &lt;- function(trip_table) {\n  var_df &lt;- trip_table %&gt;%\n    select(HEX_ID, TRIPS) %&gt;%\n    rename(Trips = TRIPS)\n\n  lisa &lt;- hexagon_active %&gt;%\n    select(HEX_ID, geometry) %&gt;%\n    left_join(var_df, by = \"HEX_ID\") %&gt;%\n    mutate(Trips = as.numeric(coalesce(Trips, 0))) %&gt;%  # numeric + fill edges with 0\n    mutate(\n      local_moran = local_moran(\n        x   = Trips,\n        nb  = nb_d,                # nb list (Section 7)\n        wt  = lw_d_B$weights,      # &lt;-- pass the WEIGHTS LIST, not the listw\n        nsim = nsim,\n        zero.policy = TRUE\n      ),\n      .before = 1\n    ) %&gt;%\n    unnest(local_moran)  # adds ii, e_ii, var_ii, z_ii, p_ii\n\n  lisa\n}\n\n# build per period (reusing tp_* from Section 6)\nlisa_am &lt;- compute_lisa_sf(tp_am)\nlisa_pm &lt;- compute_lisa_sf(tp_pm)\nlisa_wk &lt;- compute_lisa_sf(tp_wk)\n\n# Inspect the data structure \nglimpse(lisa_am)\n\nRows: 826\nColumns: 15\n$ ii           &lt;dbl&gt; 0.5207766, 1.5654258, 1.5417836, 2.0487699, 2.0240404, 2.…\n$ eii          &lt;dbl&gt; -0.002534305, 0.023328139, 0.014274569, -0.002803435, 0.0…\n$ var_ii       &lt;dbl&gt; 0.6208614, 1.6244503, 1.5673373, 2.0768690, 2.1097708, 1.…\n$ z_ii         &lt;dbl&gt; 0.6641443, 1.2099256, 1.2201200, 1.4235820, 1.3902874, 1.…\n$ p_ii         &lt;dbl&gt; 0.50659796, 0.22630745, 0.22241939, 0.15456751, 0.1644416…\n$ p_ii_sim     &lt;dbl&gt; 0.212, 0.024, 0.040, 0.004, 0.036, 0.012, 0.002, 0.064, 0…\n$ p_folded_sim &lt;dbl&gt; 0.107, 0.012, 0.020, 0.002, 0.018, 0.006, 0.001, 0.032, 0…\n$ skewness     &lt;dbl&gt; -3.2871907, -1.4483825, -1.4475342, -1.5652938, -1.208822…\n$ kurtosis     &lt;dbl&gt; 15.7899006, 2.7716528, 2.9811084, 3.8603154, 2.0605072, 1…\n$ mean         &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ median       &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ pysal        &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ HEX_ID       &lt;chr&gt; \"H0001\", \"H0002\", \"H0003\", \"H0004\", \"H0005\", \"H0006\", \"H0…\n$ Trips        &lt;dbl&gt; 308, 74, 219, 757, 78, 101, 374, 103, 4407, 1489, 155, 40…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((4167.538 27510.65..., POLYGON ((4167.53…\n\nglimpse(lisa_pm)\n\nRows: 826\nColumns: 15\n$ ii           &lt;dbl&gt; 0.3668541, 1.1149340, 1.0973734, 1.3865306, 1.4555652, 1.…\n$ eii          &lt;dbl&gt; -0.0154166809, -0.0485762105, 0.0162082136, 0.0598893911,…\n$ var_ii       &lt;dbl&gt; 0.4925720, 1.2729696, 0.9896646, 1.0654666, 1.5785705, 1.…\n$ z_ii         &lt;dbl&gt; 0.5446735, 1.0312434, 1.0867960, 1.2852381, 1.1734353, 1.…\n$ p_ii         &lt;dbl&gt; 0.5859781, 0.3024267, 0.2771270, 0.1987091, 0.2406213, 0.…\n$ p_ii_sim     &lt;dbl&gt; 0.188, 0.006, 0.008, 0.004, 0.012, 0.010, 0.004, 0.028, 0…\n$ p_folded_sim &lt;dbl&gt; 0.095, 0.003, 0.004, 0.002, 0.006, 0.005, 0.002, 0.014, 0…\n$ skewness     &lt;dbl&gt; -5.897541, -3.015131, -2.802291, -2.346085, -2.292282, -2…\n$ kurtosis     &lt;dbl&gt; 46.338875, 13.453753, 12.411105, 9.196109, 7.382763, 6.91…\n$ mean         &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ median       &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ pysal        &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ HEX_ID       &lt;chr&gt; \"H0001\", \"H0002\", \"H0003\", \"H0004\", \"H0005\", \"H0006\", \"H0…\n$ Trips        &lt;dbl&gt; 849, 223, 781, 2048, 221, 231, 976, 264, 2858, 1679, 2307…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((4167.538 27510.65..., POLYGON ((4167.53…\n\nglimpse(lisa_wk)\n\nRows: 826\nColumns: 15\n$ ii           &lt;dbl&gt; 0.3874853, 1.2029382, 1.1553293, 1.5294786, 1.5218421, 1.…\n$ eii          &lt;dbl&gt; -0.0135854458, 0.0275389341, 0.0196249070, -0.0796581626,…\n$ var_ii       &lt;dbl&gt; 0.4138850, 1.1988058, 1.1041473, 1.6477202, 1.6416811, 2.…\n$ z_ii         &lt;dbl&gt; 0.6234207, 1.0735221, 1.0808161, 1.2535776, 1.1708768, 1.…\n$ p_ii         &lt;dbl&gt; 0.5330081, 0.2830369, 0.2797789, 0.2099956, 0.2416483, 0.…\n$ p_ii_sim     &lt;dbl&gt; 0.198, 0.020, 0.048, 0.004, 0.034, 0.018, 0.004, 0.066, 0…\n$ p_folded_sim &lt;dbl&gt; 0.099, 0.010, 0.024, 0.002, 0.017, 0.009, 0.002, 0.033, 0…\n$ skewness     &lt;dbl&gt; -3.838556, -2.304699, -2.216912, -1.730430, -1.895848, -2…\n$ kurtosis     &lt;dbl&gt; 20.953571, 7.227195, 7.257085, 3.693752, 5.312998, 5.1259…\n$ mean         &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ median       &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ pysal        &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ HEX_ID       &lt;chr&gt; \"H0001\", \"H0002\", \"H0003\", \"H0004\", \"H0005\", \"H0006\", \"H0…\n$ Trips        &lt;dbl&gt; 1237, 159, 457, 1566, 194, 121, 538, 152, 8832, 2757, 846…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((4167.538 27510.65..., POLYGON ((4167.53…\n\n\nThe printed data frames are the per-hexagon outputs from the Local Moran’s I calculation (826 rows). For each hexagon, ii is the observed local Moran’s I, e_ii is its null expectation (near −0.0012), and var_ii the randomisation variance. The z_ii column standardises ii; large positive values indicate clustering of similar high or low trip counts, while negative values indicate spatial dispersion or outliers. Columns p_ii (analytic), p_ii_sim (permutation), and p_folded_sim report significance; many entries are extremely small (often ≤0.01), signalling numerous statistically significant local associations. The categorical labels mean, median, and pysal assign each hexagon to a LISA quadrant; in this excerpt, labels are overwhelmingly Low–Low, meaning hexagons with low trips surrounded by neighbours that are also low. Diagnostic fields skewness and kurtosis describe the shape of the permutation distribution for each unit. Identifiers HEX_ID, Trips, and geometry link results back to the grid and counts for mapping. Overall, the tables show widespread significant Low–Low clustering with pockets of other patterns flagged by positive z-scores and tiny \\(p\\)-values.\n\n9.1.1 Mapping the Local Moran’s I\nThis part focuses on visualising the computed Local Moran’s I results as thematic maps. A mapping function is defined to display the spatial variation of Moran’s I values across Singapore, using a continuous colour scale to represent clustering strength. The maps are uniformly formatted with consistent legends and titles, allowing for direct comparison between time periods and facilitating the detection of spatial heterogeneity within the analytical grid.\n\ntmap_mode(\"plot\")\n\nmap_localI &lt;- function(sf_obj, title_text) {\n  tm_shape(sg_outline) + tm_borders(col = \"grey40\", lwd = 0.7) +\n  tm_shape(sf_obj) +\n    tm_fill(\"ii\",\n      fill.scale = tm_scale_intervals(\n        style  = \"pretty\",\n        n      = 5,\n        values = \"brewer.RdBu\"  # same as handout\n      ),\n      fill.legend = tm_legend(title = \"Local Moran's I\")\n    ) +\n    tm_borders(col = \"grey80\", lwd = 0.4) +\n    tm_layout(\n      main.title = title_text,\n      main.title.position = \"center\",\n      main.title.fontface = \"bold\",\n      legend.position = c(\"right\",\"bottom\"),\n      legend.frame = TRUE\n    )\n}\n\nmI_am &lt;- map_localI(lisa_am, \"Local Moran's I — Weekday Morning (06–09)\")\nmI_pm &lt;- map_localI(lisa_pm, \"Local Moran's I — Weekday Afternoon (17–20)\")\nmI_wk &lt;- map_localI(lisa_wk, \"Local Moran's I — Weekend/Holiday (11–19)\")\n\nmI_am; mI_pm; mI_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Local Moran’s I maps provide spatially explicit insights into the degree of clustering or dispersion of trip origins during different time periods. In the weekday morning (06–09), pronounced positive clustering (dark blue hexagons) appears in the western, northeastern, and parts of the central regions, suggesting areas where high trip intensities are surrounded by similarly high neighbours. These patterns correspond to residential zones with strong outbound commuting flows. Negative values (orange tones) indicate isolated cells or transitional areas where trip activity diverges from nearby cells, suggesting spatial discontinuities likely caused by mixed land use or low transport demand. During the weekday afternoon (17–20), clustering weakens, with fewer high-positive Moran’s I values, indicating that evening trips are more spatially dispersed as commuters return to various home locations. In the weekend/holiday (11–19) period, moderate positive clusters persist mainly in the southern and central regions, reflecting concentrated leisure or shopping-related trips in activity hubs like Orchard or Marina areas. Overall, spatial autocorrelation is strongest during weekday mornings and declines across later periods, showing that Singapore’s trip-generation patterns transition from structured, residential-based flows to more scattered, destination-driven movements. This implies the morning peak remains the most spatially organised travel window, essential for targeting transport interventions and infrastructure optimisation.\n\n\n9.1.2 Mapping Local Moran’s I \\(p\\)-values\nHere, the emphasis shifts to mapping the statistical significance of local spatial relationships. \\(P\\)-values derived from Local Moran’s I computations are visualised using colour gradients, where darker tones highlight stronger evidence of spatial dependence. These maps reveal which areas exhibit meaningful local clustering and which remain random. Comparing across time periods provides insights into how the significance of clustering changes during different travel intervals throughout the day.\n\ntmap_mode(\"plot\")\n\nmap_localP &lt;- function(sf_obj, title_text) {\n  tm_shape(sg_outline) +\n    tm_borders(col = \"grey40\", lwd = 0.7) +\n    tm_shape(sf_obj) +\n    tm_fill(\n      col = \"p_ii\",\n      fill.scale = tm_scale_intervals(\n        breaks = c(-Inf, 0.001, 0.01, 0.05, 0.10, Inf),\n        values = \"brewer.Reds\"\n      ),\n      fill.legend = tm_legend(title = \"p-value\"),\n      showNA = FALSE,   # hide \"Missing\" in legend\n      colorNA = NA      # transparent for NA hexes\n    ) +                 \n    tm_borders(fill_alpha = 0.5, col = \"grey80\", lwd = 0.4) +\n    tm_layout(\n      main.title = title_text,\n      main.title.position = \"center\",\n      main.title.fontface = \"bold\",\n      legend.position = c(\"right\", \"bottom\"),\n      legend.frame = TRUE\n    )\n}\n\n# Run all 3 maps\nmp_am &lt;- map_localP(lisa_am, \"p-values of Local Moran's I — Weekday Morning (06–09)\")\nmp_pm &lt;- map_localP(lisa_pm, \"p-values of Local Moran's I — Weekday Afternoon (17–20)\")\nmp_wk &lt;- map_localP(lisa_wk, \"p-values of Local Moran's I — Weekend/Holiday (11–20)\")\n\nmp_am; mp_pm; mp_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(p\\)-value maps visualise the statistical significance of local clustering across Singapore for different periods. In the weekday morning (06–09), numerous light-blue hexagons (p ≤ 0.2) dominate the map, especially in western, northeastern, and southeastern regions, signifying that local Moran’s I values are statistically significant and that spatial clustering of trip origins is unlikely to occur by chance. These significant cells correspond to areas with concentrated commuting activity, validating the earlier high Moran’s I findings. During the weekday afternoon (17–20), significant regions decrease slightly, reflected by more mid-blue shades (\\(p\\) ≈ 0.4–0.6), suggesting reduced clustering as trip destinations diversify across urban zones. The weekend/holiday (11–20) period shows the fewest highly significant cells, particularly in residential and industrial zones, where darker blue (\\(p\\) &gt; 0.6) indicates random or weak spatial association. This pattern aligns with more dispersed, leisure-based travel during non-work periods. Overall, the intensity and spatial coverage of low \\(p\\)-values decline from morning to weekend, implying that weekday peak-hour movements are spatially structured, while off-peak and weekend activities exhibit more randomness. The outcome underscores that spatial dependence in trip generation is temporally dynamic—strongest during structured commuting peaks and weakest during flexible leisure travel—informing transport planners about when and where spatial targeting of travel interventions is most relevant.\n\n\n9.1.3 Mapping both Local Moran’s I values and \\(p\\)-values\nThis segment combines two key perspectives by displaying Moran’s I values and their associated \\(p\\)-values side by side. The paired visualisation allows simultaneous interpretation of spatial clustering strength and statistical reliability. By comparing both maps together, analysts can easily distinguish significant clusters from random fluctuations. The approach improves interpretability and ensures that subsequent discussions are grounded in statistically validated spatial patterns across all travel periods.\n\ntmap_arrange(\n  map_localI(lisa_am, \"Local I — Morning (06–09)\"),\n  map_localP(lisa_am, \"p-value — Morning (06–09)\"),\n  ncol = 2, asp = 1\n)\n\n\n\n\n\n\n\ntmap_arrange(\n  map_localI(lisa_pm, \"Local I — Afternoon (17–20)\"),\n  map_localP(lisa_pm, \"p-value — Afternoon (17–20)\"),\n  ncol = 2, asp = 1\n)\n\n\n\n\n\n\n\ntmap_arrange(\n  map_localI(lisa_wk, \"Local I — Weekend/Holiday (11–20)\"),\n  map_localP(lisa_wk, \"p-value — Weekend/Holiday (11–20)\"),\n  ncol = 2, asp = 1\n)\n\n\n\n\n\n\n\n\nThe combined Local Moran’s I and \\(p\\)-value maps reveal both the strength and statistical reliability of spatial clustering across Singapore’s trip-origin data. During the weekday morning (06–09), areas with high positive Local I (dark blue on the left) coincide with light-blue \\(p\\)-values (right map), especially in the west and northeast. These zones represent statistically significant clusters (p &lt; 0.05) where strong spatial dependence exists—typically high-trip cells surrounded by high-trip neighbours. In contrast, orange cells (negative I) appear near the central fringe, indicating dissimilar neighbourhood effects such as isolated business or institutional zones. In the weekday afternoon (17–20), overall clustering intensity weakens, and significant cells shrink mainly to residential–commercial transition belts, reflecting dispersed evening travel as commuters return home. By weekend/holiday (11–20), high I values persist only around southern activity hubs, but most regions exhibit non-significant p-values (dark blue on the right), confirming randomised spatial patterns during leisure hours. The progressive reduction of significant low-p clusters from morning to weekend implies that trip generation is most spatially structured during work-related commuting peaks but becomes increasingly stochastic when travel is discretionary. For transport planners, this pattern highlights when and where spatial dependencies dominate mobility flows—vital for prioritising public-transport capacity, infrastructure optimisation, and land-use coordination during critical morning peaks.\n\n\n\n9.2 Preparing and visualising LISA Map\nAt this stage, the analysis transitions toward visual diagnostics by preparing data for Moran scatterplots. The process involves computing spatially lagged trip counts, ensuring new variables are properly reassigned, and plotting the relationships between each hexagon’s trip value and its neighbours. The resulting scatterplots highlight the overall correlation structure and clustering direction, helping confirm that local spatial dependencies identified earlier are both consistent and statistically meaningful across all study periods.\n\n9.2.1 Plotting Moran Scatterplot\nMoving from maps to diagnostics, this section constructs Moran scatterplots for each time period. A helper adds the spatial lag of Trips directly from the existing listw, ensuring consistency with the distance band weights. The plots place Trips on the x axis against their spatial lags on the y axis and fit an OLS line to reveal spatial association. Points display individual hexagons, while the fitted slope visualises positive or negative dependence prior to standardisation.\n\n### Add spatial lag of Trips for scatterplots (no unnesting, no wrapper)\n\nadd_lag &lt;- function(sf_obj) {\n  # compute lag directly from the listw object you already built\n  sf_obj$lag_Trips &lt;- as.numeric(\n    spdep::lag.listw(lw_d_B, sf_obj$Trips, zero.policy = TRUE, NAOK = TRUE)\n  )\n  sf_obj\n}\n\n# REASSIGN so the new column sticks\nlisa_am &lt;- add_lag(lisa_am)\nlisa_pm &lt;- add_lag(lisa_pm)\nlisa_wk &lt;- add_lag(lisa_wk)\n\nmoran_scatter &lt;- function(sf_obj, title_text) {\n  ggplot(sf_obj, aes(x = Trips, y = lag_Trips)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(x = \"Trips\", y = \"Spatial Lag of Trips\", title = title_text) +\n    theme_minimal()\n}\n\nms_am &lt;- moran_scatter(lisa_am, \"Moran Scatterplot — Weekday Morning (06–09)\")\nms_pm &lt;- moran_scatter(lisa_pm, \"Moran Scatterplot — Weekday Afternoon (17–20)\")\nms_wk &lt;- moran_scatter(lisa_wk, \"Moran Scatterplot — Weekend/Holiday (11–20)\")\n\nms_am; ms_pm; ms_wk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhe Moran scatterplots illustrate the spatial relationship between trip intensity (x-axis) and the spatial lag of trips (y-axis), showing how local trip counts correlate with neighbouring areas across three time periods. The weekday morning (06–09) plot exhibits a clear upward trend with a steep red regression line, indicating strong positive spatial autocorrelation. This means hexagons with high trip numbers are surrounded by similarly high-trip neighbours, reflecting concentrated commuting activity in key employment and transport nodes. During the weekday afternoon (17–20), the slope remains positive but flatter, suggesting a weaker spatial dependency. This reflects more spatial dispersion as commuters return home, reducing the dominance of clustered high-trip zones. On weekends/holidays (11–20), the scatter pattern stays positively sloped but becomes even more diffuse, indicating lower clustering and greater variability in trip patterns, typical of leisure or non-routine movements. The persistence of a positive slope across all periods confirms that spatially proximate hexagons tend to share similar trip magnitudes, but the declining gradient from morning to weekend highlights a shift from structured, work-related clustering to spatially scattered travel behaviour. These patterns underscore that spatial dependency in mobility is strongest during structured weekday peaks and weakest when travel choices are discretionary, providing critical insights for adaptive transport planning and congestion management.\n\n\n9.2.2 Plotting Moran scatterplot with standardised variable\nThis section enhances interpretability by standardising both trip counts and their spatial lags before plotting. Each hexagon is classified into one of four categories—High–High, Low–Low, High–Low, or Low–High—according to \\(z\\)-scores. Distinct colours are applied to visualise these quadrants, making cluster and outlier patterns easily recognisable. The standardised Moran scatterplots provide an intuitive visual explanation of how different areas relate spatially, illustrating both concentration zones and transition boundaries across Singapore.\n\n#### 10.4.4.2 Plotting Moran scatterplot with standardised variable ----\n\n# 1) Standardise Trips and its spatial lag (center/scale, return plain numeric)\nstandardise_for_scatter &lt;- function(sf_obj){\n  sf_obj |&gt;\n    dplyr::mutate(\n      z_Trips      = as.vector(scale(Trips)),\n      z_lag_Trips  = as.vector(scale(lag_Trips))\n    )\n}\n\nlisa_am &lt;- standardise_for_scatter(lisa_am)\nlisa_pm &lt;- standardise_for_scatter(lisa_pm)\nlisa_wk &lt;- standardise_for_scatter(lisa_wk)\n\n# 2) (Guard) ensure LISA quadrant labels `mean` exist as in 10.4.4.1\n#    If your earlier chunk already created `mean`, this block does nothing.\nensure_quadrants &lt;- function(sf_obj){\n  if (!\"mean\" %in% names(sf_obj)) {\n    x0 &lt;- mean(sf_obj$Trips,     na.rm = TRUE)\n    y0 &lt;- mean(sf_obj$lag_Trips, na.rm = TRUE)\n    sf_obj &lt;- dplyr::mutate(\n      sf_obj,\n      mean = dplyr::case_when(\n        Trips &gt;= x0 & lag_Trips &gt;= y0 ~ \"High-High\",\n        Trips &lt;  x0 & lag_Trips &lt;  y0 ~ \"Low-Low\",\n        Trips &lt;  x0 & lag_Trips &gt;= y0 ~ \"Low-High\",\n        TRUE                           ~ \"High-Low\"\n      ),\n      mean = factor(mean, levels = c(\"High-High\",\"Low-Low\",\"Low-High\",\"High-Low\"))\n    )\n  }\n  sf_obj\n}\n\nlisa_am &lt;- ensure_quadrants(lisa_am)\nlisa_pm &lt;- ensure_quadrants(lisa_pm)\nlisa_wk &lt;- ensure_quadrants(lisa_wk)\n\n# 3) Standardised Moran scatter (points + black OLS; dashed mean lines)\nlibrary(ggplot2)\n\nmoran_scatter_std &lt;- function(sf_obj, title_text){\n  ggplot(sf_obj, aes(x = z_Trips, y = z_lag_Trips, color = mean)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    geom_hline(yintercept = mean(sf_obj$z_lag_Trips, na.rm = TRUE), lty = 2) +\n    geom_vline(xintercept = mean(sf_obj$z_Trips,     na.rm = TRUE), lty = 2) +\n    scale_color_manual(\n      values = c(\n        \"High-High\" = \"red\",\n        \"Low-Low\"   = \"blue\",\n        \"Low-High\"  = \"lightblue\",\n        \"High-Low\"  = \"pink\"\n      )\n    ) +\n    labs(\n      x = \"Standardised Trips\",\n      y = \"Standardised Spatial Lag of Trips\",\n      title = title_text\n    ) +\n    theme_minimal()\n}\n\nms_am_z &lt;- moran_scatter_std(lisa_am, \"Standardised Moran Scatter — Weekday Morning (06–09)\")\nms_pm_z &lt;- moran_scatter_std(lisa_pm, \"Standardised Moran Scatter — Weekday Afternoon (17–20)\")\nms_wk_z &lt;- moran_scatter_std(lisa_wk, \"Standardised Moran Scatter — Weekend/Holiday (11–20)\")\n\nms_am_z; ms_pm_z; ms_wk_z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe standardised Moran scatterplots reveal the spatial association patterns of trip intensity by quadrant classification, distinguishing four spatial interaction types: High-High (HH), Low-Low (LL), High-Low (HL), and Low-High (LH). During the weekday morning (06–09), the majority of points fall in the High-High quadrant (red), confirming strong clustering of high-trip areas surrounded by similar high-trip neighbours — characteristic of concentrated commuting toward business and transport hubs. Low-Low zones (blue) dominate the opposite quadrant, indicating peripheral or residential regions with uniformly low trip activity. The weekday afternoon (17–20) plot still shows a positive spatial relationship, though the slope slightly flattens, suggesting more dispersed trip flows as outbound commuting begins. A few High-Low outliers (pink) appear, marking transition zones between dense business districts and adjacent low-activity cells. During the weekend/holiday (11–19), while High-High clusters remain visible, the overall distribution becomes broader with more scattered Low-High and High-Low points, reflecting irregular trip patterns driven by leisure or non-routine movement. The positive slope across all periods confirms persistent spatial autocorrelation, yet the reduction in clustering strength from weekday morning to weekend reflects a shift from structured work-related travel to more dispersed, flexible trip behaviour. This finding underscores the significance of weekday peaks for congestion management and highlights the spatial diffusion of mobility during non-working days, offering valuable input for dynamic transport planning and demand-based service allocation.\n\n\n9.2.3 Preparing LISA Map Classes (\\(p\\) &lt; 0.05)\nFinally, the analysis establishes clear LISA map classifications based on significance thresholds. Each hexagon is assigned to a specific cluster category according to its Local Moran’s I value and \\(p\\)-value. Colours are systematically applied to differentiate strong clusters, weak outliers, and non-significant regions. The resulting maps visually summarise spatial autocorrelation outcomes for morning, afternoon, and weekend periods, helping to pinpoint areas of persistent high or low clustering within the study region.\n\nsignif &lt;- 0.05\n\ncreate_lisa_clusters &lt;- function(sf_obj) {\n  sf_obj %&gt;%\n    mutate(\n      LISA_cluster = ifelse(p_ii &lt; signif, as.character(mean), \"Insignificant\"),\n      LISA_cluster = factor(LISA_cluster,\n        levels = c(\"Insignificant\", \"Low-Low\", \"Low-High\", \"High-Low\", \"High-High\")\n      )\n    )\n}\n\nlisa_am &lt;- create_lisa_clusters(lisa_am)\nlisa_pm &lt;- create_lisa_clusters(lisa_pm)\nlisa_wk &lt;- create_lisa_clusters(lisa_wk)\n\nplot_lisa_clusters &lt;- function(sf_obj, title_text) {\n  tm_shape(sg_outline) + tm_borders(col = \"grey40\", lwd = 0.7) +\n    tm_shape(sf_obj) +\n    tm_polygons(\n      col = \"LISA_cluster\",\n      palette = c(\n        \"High-High\"     = \"#b2182b\",   # strong clustering (dark red)\n        \"High-Low\"      = \"#ef8a62\",   # outlier (light red)\n        \"Low-High\"      = \"#67a9cf\",   # outlier (light blue)\n        \"Low-Low\"       = \"#2166ac\",   # strong clustering (dark blue)\n        \"Insignificant\" = \"grey85\"\n      ),\n      title = \"LISA Cluster Type\"\n    ) +\n    tm_borders(col = \"grey60\", lwd = 0.4) +\n    tm_layout(\n      main.title = title_text,\n      main.title.fontface = \"bold\",\n      main.title.position = \"center\",\n      legend.position = c(\"RIGHT\", \"BOTTOM\"),\n      frame = TRUE\n    )\n}\n\nplot_lisa_clusters(lisa_am, \"LISA Cluster Map (p &lt; 0.05) — Weekday Morning (06–09)\")\n\n\n\n\n\n\n\nplot_lisa_clusters(lisa_pm, \"LISA Cluster Map (p &lt; 0.05) — Weekday Afternoon (17–20)\")\n\n\n\n\n\n\n\nplot_lisa_clusters(lisa_wk, \"LISA Cluster Map (p &lt; 0.05) — Weekend/Holiday (11–20)\")\n\n\n\n\n\n\n\n\nThe LISA Cluster Maps (\\(p\\) &lt; 0.05) reveal clear temporal differences in local spatial autocorrelation across Singapore’s urban structure.\nDuring **Weekday Morning (06–09), High–High (HH) clusters (dark red) are widespread and spatially concentrated across the central-northern, north-eastern, and western corridors. Prominent groupings appear around the northern central belt (likely Ang Mo Kio – Yishun region), another in the north-east zone (around Sengkang – Punggol), and a strong cluster in the western sector (Jurong – Clementi area). Smaller HH patches also appear in the south-central corridor, reflecting intensified morning mobility flows. The presence of Low–High (LH) cells (light blue) adjoining several HH patches suggests transitional areas—likely residential or mixed-use cells adjoining major trip origins or destinations. These spatial configurations indicate strong positive local autocorrelation, meaning high-trip hexagons are surrounded by similarly high neighbours, forming structured commuting corridors.\nIn Weekday Afternoon (17–20), HH clusters become sparser and more fragmented, persisting mainly in the north-east, west, and central-south belts. The contraction of HH zones and scattered LH outliers reflects spatial dispersion of evening travel, as movement decentralises from morning-dominated centres to more distributed return-trip destinations.\nBy Weekend/Holiday (11–20), HH clusters re-emerge more broadly across the northern, eastern, and south-central zones. These weekend clusters expand around previously active weekday corridors, implying recreational or shopping-related flows along regional centres. LH patches remain peripheral, delineating boundaries between active hubs and quieter suburbs."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html",
    "href": "In-Class_Ex05/in-class_ex05b.html",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\n\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\n\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\n\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is highly recommended to read Emerging Hot Spot Analysis before you continue the exercise."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#overview",
    "href": "In-Class_Ex05/in-class_ex05b.html#overview",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\n\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\n\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\n\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is highly recommended to read Emerging Hot Spot Analysis before you continue the exercise."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#getting-started",
    "href": "In-Class_Ex05/in-class_ex05b.html#getting-started",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "2 Getting started",
    "text": "2 Getting started\n\n2.1 Installing and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, plotly, and tidyverse.\n\npacman::p_load(sf, sfdep, tmap, \n               plotly, tidyverse, \n               Kendall)"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#the-data",
    "href": "In-Class_Ex05/in-class_ex05b.html#the-data",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "3 The data",
    "text": "3 The data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\n\nHunan_GDPPC, an attribute data set in csv format.\n\nBefore getting started, reveal the content of Hunan_GDPPC.csv by using Notepad and MS Excel."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#importing-geospatial-data",
    "href": "In-Class_Ex05/in-class_ex05b.html#importing-geospatial-data",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "4 Importing geospatial data",
    "text": "4 Importing geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\nhunan &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#importing-attribute-table",
    "href": "In-Class_Ex05/in-class_ex05b.html#importing-attribute-table",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "5 Importing attribute table",
    "text": "5 Importing attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_GDPPC.csv into R.\n\nGDPPC &lt;- read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex05/data/aspatial/Hunan_GDPPC.csv\")\n\nRows: 1496 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (2): Year, GDPPC\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#creating-a-time-series-cube",
    "href": "In-Class_Ex05/in-class_ex05b.html#creating-a-time-series-cube",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "6 Creating a Time Series Cube",
    "text": "6 Creating a Time Series Cube\nBefore getting started, students must read this article to learn the basic concept of spatio-temporal cube and its implementation in sfdep package.\nIn the code chunk below, spacetime() of sfdep ised used to create an spatio-temporal cube.\n\nGDPPC_st &lt;- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\nNext, is_spacetime_cube() of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object.\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE\n\n\nThe TRUE return confirms that GDPPC_st object is indeed an time-space cube."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#computing-gi",
    "href": "In-Class_Ex05/in-class_ex05b.html#computing-gi",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "7 Computing Gi*",
    "text": "7 Computing Gi*\nNext, we will compute the local Gi* statistics.\n\n7.1 Deriving the spatial weights\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\n\n\n\n\n\nNoteThings to learn from the code chunk above\n\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#mann-kendall-test",
    "href": "In-Class_Ex05/in-class_ex05b.html#mann-kendall-test",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "8 Mann-Kendall Test",
    "text": "8 Mann-Kendall Test\nA monotonic series or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.\n\\(H_0\\): No monotonic trend\n\\(H_1\\): Monotonic trend is present\nInterpretation:\n\nReject the null-hypothesis null if the p-value is smaller than the alpha value (i.e. 1-confident level)\n\nTau ranges between -1 and 1 where:\n\n-1 is a perfectly decreasing series, and\n\n1 is a perfectly increasing series.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou are encouraged to read Mann-Kendall Test For Monotonic Trend to learn more about the concepts and method of Mann-Kendall test..\n\n\n\n8.1 Mann-Kendall Test on Gi\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.\n\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(County == \"Changsha\") |&gt; \n  select(County, Year, gi_star)\n\n\nggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n8.2 Interactive Mann-Kendall Plot\nWe can also create an interactive plot by using ggplotly() of plotly package.\n\np &lt;- ggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\n\n8.3 Printing Mann-Kendall test report\n\ncbg %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.485 0.00742    66  136.  589.\n\n\nIn the above result, sl is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.\n\n\n8.4 Mann-Kendall test data.frame\nWe can replicate this for each location by using group_by() of dplyr package.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n\n# A tibble: 6 × 6\n  County        tau        sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Anhua      0.191  0.303        26  136.  589.\n2 Anren     -0.294  0.108       -40  136.  589.\n3 Anxiang    0      1             0  136.  589.\n4 Baojing   -0.691  0.000128    -94  136.  589.\n5 Chaling   -0.0882 0.650       -12  136.  589.\n6 Changning -0.750  0.0000318  -102  136.  589.\n\n\nWe can also sort to show significant emerging hot/cold spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:10)\nhead(emerging)\n\n# A tibble: 6 × 6\n  County        tau         sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Shuangfeng  0.868 0.00000143   118  136.  589.\n2 Xiangtan    0.868 0.00000143   118  136.  589.\n3 Xiangxiang  0.868 0.00000143   118  136.  589.\n4 Chengbu    -0.824 0.00000482  -112  136.  589.\n5 Dongan     -0.824 0.00000482  -112  136.  589.\n6 Wugang     -0.809 0.00000712  -110  136.  589."
  },
  {
    "objectID": "In-Class_Ex05/in-class_ex05b.html#performing-emerging-hotspot-spatial-analysis-ehsa",
    "href": "In-Class_Ex05/in-class_ex05b.html#performing-emerging-hotspot-spatial-analysis-ehsa",
    "title": "In-class Ex5b: Emerging Hot Spot Analysis",
    "section": "9 Performing Emerging Hotspot Spatial Analysis (EHSA)",
    "text": "9 Performing Emerging Hotspot Spatial Analysis (EHSA)\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n9.1 Visualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nFigure above shows that sporadic cold spots class has the high numbers of county.\n\n\n9.2 Visualising EHSA\nIn this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both hunan and ehsa together by using the code chunk below.\n\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n9.3 Interpretation of EHSA classes"
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex02/take-home_ex02.html#emerging-hot-spot-analysis-ehsa",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "11 Emerging Hot Spot Analysis (EHSA)",
    "text": "11 Emerging Hot Spot Analysis (EHSA)\nTo set the stage, this section introduces a space time approach that asks a practical question about neighbourhood bus mobility. Where are statistically meaningful clusters of higher or lower origin trips appearing, disappearing, or changing through time, and how should we label them in a way that helps planning and policy work. We construct the analysis around the Getis Ord Gi* local statistic that is evaluated for each cell in each time bin and then we examine the sequence of those statistics using the Mann Kendall trend test. The combined outcome is a categorical label such as new hot spot, intensifying hot spot, persistent hot spot, diminishing hot spot, or their cold analogues. We run the procedure for three mandated periods that reflect commuting and leisure rhythms, and we carry the results back to the validated mainland hexagon grid so that interpretation is always spatially anchored. The purpose is not simply to produce colourful maps but to generate defensible evidence about evolving trip concentration and suppression across the urban fabric.\n\n11.1 Build neighbours and weights on hex grid\nAt the core of any local cluster method is a clear statement of spatial relationships. This subsection prepares that foundation by defining which hexagons are considered neighbours and how much influence each neighbour should exert in the calculations. We adopt contiguity based neighbours with the queen rule so touching at an edge or a corner counts, and we include each cell itself to ensure that isolated or island cells can still be processed without dropping from the analysis. We then compute inverse distance weights from the centroids so that very close neighbours have more influence than distant ones while still keeping the structure simple and reproducible. Warnings about cells with no neighbours can appear in coastal or fragmented areas, and the include self step addresses that gracefully. Preparing neighbours and weights on the geometry first and storing them as named columns allows the later space time cube to attach the correct structure without costly recomputation at every time step.\n\n# ---- 11.0 Build neighbours & weights on hex grid -----------------------\n\n# Assumes we already have:\n# - trips_panel_sf: columns HEX_ID, DAY_TYPE, HOUR_OF_DAY, TRIPS  (sf ok; we will drop geometry)\n# - hexagon_active: sf polygons with HEX_ID + geometry\n# - sg_outline: sf polygon of SG mainland (optional, for basemap)\n\nhexagon_geo &lt;- hexagon_active |&gt;\n  mutate(\n    nb = include_self(st_contiguity(geometry, queen = TRUE)),\n    wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)\n  )\n\n# NOTE: \"some observations have no neighbours\" warnings are expected for islands;\n# include_self() ensures each hex has at least itself.\n\n\n\n11.2 Period definitions\nIn this step we translate the assignment brief into explicit time windows that a computer will understand. The weekday morning period covers the early departure window from 06:00 to 09:00 that captures school and work origins, the weekday afternoon period covers the return window from 17:00 to 20:00 that mixes home bound and after work activities, and the weekend or holiday period focuses on late morning through early evening from 11:00 to 20:00 when leisure and retail activity dominate. Declaring these ranges once, with a readable label for each, prevents accidental drift in later filters and guarantees that any figures or tables can be regenerated consistently. The labels also become facets in charts and map titles, which helps a reader keep track of the storyline. Choosing hour as the time bin aligns with the data structure of origin trips and keeps the sample size per bin adequate for reliable local statistics and trend testing.\n\n# ---- 11.1 Period definitions -------------------------------------------\n\nperiods &lt;- tribble(\n  ~label,                       ~day_type,            ~hours,\n  \"Weekday Morning Peak\",       \"WEEKDAY\",            6:9,\n  \"Weekday Afternoon Peak\",     \"WEEKDAY\",            17:20,\n  \"Weekend/Holiday Peak\",       \"WEEKENDS/HOLIDAY\",   11:20\n)\n\n\n\n11.3 Mann Kendall \\(p\\) value\nBefore running the main classification, we safeguard the workflow against differences in package versions. The emerging hotspot procedure returns the trend result either as a simple column named sl, as a column named mk_sl, or nested inside a list column that contains the Mann Kendall (MK) object. The helper in this subsection extracts the \\(p\\) value reliably no matter which structure appears, and falls back to other common names if needed. Returning a numeric vector rather than a mixed object means that downstream filtering is straightforward and readable. This design choice protects the analysis from breaking on another machine or in a future semester when packages have changed. It also makes the significance threshold explicit and therefore auditable. By separating this concern into a short utility we keep the runner function focused on analysis logic, while confidence in reproducibility and portability is improved for both code review and grading.\n\n# ---- helper to pull MK p-value robustly --------------------------------\n\n.get_mk_p &lt;- function(tbl) {\n  # Return a numeric vector pval; works for multiple sfdep versions\n  nms &lt;- names(tbl)\n  if (\"sl\" %in% nms) return(tbl$sl)\n  if (\"mk_sl\" %in% nms) return(tbl$mk_sl)\n  if (\"mk\" %in% nms) {\n    return(purrr::map_dbl(tbl$mk, function(x) {\n      if (!is.null(x$sl)) return(as.numeric(x$sl))\n      if (!is.null(x$p.value)) return(as.numeric(x$p.value))\n      if (!is.null(x$pvalue)) return(as.numeric(x$pvalue))\n      NA_real_\n    }))\n  }\n  # Fallback: look for generic p-value fields if present\n  if (\"p_value\" %in% nms) return(tbl$p_value)\n  if (\"pvalue\" %in% nms)  return(tbl$pvalue)\n  if (\"p.val\" %in% nms)   return(tbl$`p.val`)\n  rep(NA_real_, nrow(tbl))\n}\n\n\n\n11.4 EHSA runner\nIn this section we operationalise the full analytical sequence for a single period. The function accepts a day type, a set of hour values, and a human friendly label. It filters the balanced panel to exactly those rows, constructs a space time cube keyed by hexagon identifier and hour, and attaches the previously prepared neighbours and weights using their column names. With this structure in place, the emerging hotspot procedure computes a series of Gi star statistics per cell across the chosen hours, performs the Mann Kendall test on that temporal sequence, and assigns a classification label that captures both significance and direction of change. We then keep only features with significant trend evidence, and we join the result back to the hexagon geometry so that mapping and counting become trivial. Encapsulating all of this in one function makes the workflow concise, testable, and repeatable across periods without copy paste mistakes.\n\n# ---- 11.2 EHSA runner ---------------------------------------------------\n\nrun_ehsa &lt;- function(day_type, hours, label) {\n\n  df &lt;- trips_panel_sf |&gt;\n    st_drop_geometry() |&gt;\n    filter(DAY_TYPE == day_type, HOUR_OF_DAY %in% hours) |&gt;\n    select(HEX_ID, HOUR_OF_DAY, TRIPS) |&gt;\n    arrange(HEX_ID, HOUR_OF_DAY)\n\n  # Build space-time cube with geometry that already contains nb/wt\n  stc &lt;- sfdep::spacetime(\n    df,              # .data\n    hexagon_geo,     # .geometry (has HEX_ID, geometry, nb, wt)\n    .loc_col  = \"HEX_ID\",\n    .time_col = \"HOUR_OF_DAY\"\n  )\n\n  # Attach neighbour/weight COLUMN NAMES (not objects)\n  stc &lt;- sfdep::set_nbs(stc, \"nb\")\n  stc &lt;- sfdep::set_wts(stc, \"wt\")\n\n  # EHSA\n  ehsa_tbl &lt;- sfdep::emerging_hotspot_analysis(\n    x    = stc,\n    .var = \"TRIPS\",\n    k    = 1,\n    nsim = 199\n  )\n\n  # Robust p-value pull + significance filter\n  mk_p &lt;- .get_mk_p(ehsa_tbl)\n  ehsa_tbl &lt;- ehsa_tbl |&gt; mutate(mk_p = mk_p)\n  ehsa_sig &lt;- ehsa_tbl |&gt; filter(!is.na(mk_p) & mk_p &lt; 0.05)\n\n  # Join back to hex geometry\n  out &lt;- hexagon_active |&gt;\n    select(HEX_ID, geometry) |&gt;\n    left_join(ehsa_sig, by = c(\"HEX_ID\" = \"location\")) |&gt;\n    mutate(period = label)\n\n  out\n}\n\n\n\n11.5 Run for all periods\nNext we scale the runner across the three mandated periods in a single call using a functional pattern. A parameter map sends the weekday morning, weekday afternoon, and weekend or holiday arguments into the runner, gathers the three outputs, and merges them into one tidy table with a period column. This structure supports side by side cartography, compact summary tables, and faceted charts without additional reshaping. The approach reduces the risk of silent inconsistencies that can arise when running separate code blocks and also speeds iteration when we refine weights or significance thresholds. If new periods are added in future work, they can be introduced by editing only the small period table rather than rewriting logic. The result is a clean separation of configuration and execution that supports both transparency for assessment and flexibility for further inquiry.\n\n# ---- Run for all periods ------------------------------------------\nehsa_list &lt;- purrr::pmap(\n  list(periods$day_type, periods$hours, periods$label),\n  run_ehsa\n)\nehsa_all &lt;- dplyr::bind_rows(ehsa_list)\n\n\n\n11.6 Quick class distribution\nBefore drawing maps, it is useful to perform a numerical sense check that confirms the outcome is plausible. The bar chart in this section counts the number of hexagons in each classification for every period and presents them in small multiples. This quick view reveals whether the analysis is returning only a few scattered features or a broad swath of classifications, and it highlights imbalances such as many cold labels and very few hot labels or the opposite. Extreme skews can suggest parameter issues, for example an overly sparse neighbour structure or an hour range that is too narrow. The figure also provides the quantitative backbone for the narrative paragraphs in the report, because we can cite counts while the maps supply the geography. Keeping this diagnostic small and fast encourages repeated use whenever we adjust upstream choices.\n\n# ---- 11.4 Quick class distribution ---------------------------\nggplot(ehsa_all, aes(x = classification)) +\n  geom_bar() +\n  facet_wrap(~ period, scales = \"free_y\") +\n  labs(title = \"EHSA classes (significant only, p &lt; 0.05)\",\n       x = NULL, y = \"Hexagon count\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\nThe bar chart tells a clear story about temporal regimes. In the weekday morning panel and the weekday afternoon panel, almost every significant cell falls into a single class while the counts for all other classes are negligible. In practice this reads as a broad field of persistent cold conditions during the short commuting windows. Local Gi star values at those hours do not trend upward through time and the Mann Kendall test confirms that the sequences are consistently low rather than moving toward higher intensity. This aligns with a mature and stable commuting pattern where origins are widely distributed and rarely form accelerating clusters within a three or four hour slice. The immediate takeaway for operations is to protect reliability rather than pursue large reallocations. Priority at known choke points, disciplined headway control, and dwell time management will matter more than changing routing inside these windows. Monitoring is still necessary, but the evidence does not call for structural changes on weekdays.\nThe weekend or holiday panel looks very different. Counts spread across several classes and no single label monopolises the distribution. The largest bar is a cold class that reflects enduring low origin intensity across green and water dominated areas and in some employment zones that quieten on non working days. At the same time there are real pockets of demand growth. Intensifying hot classes and consecutive hot classes indicate cells where sequences of local Gi star statistics have moved upward across the late morning to evening bins. These cells typically coincide with town centres, interchange nodes, waterfront leisure corridors, and retail districts. Sporadic hot classes appear in places that likely host irregular events, suggesting transient surges that are worth watching but may not justify permanent capacity.\nPlanning implications are direct. Weekday peaks show steadiness, so keep the focus on reliability and enforcement of service levels, with small tactical boosts only where crowding is observed. Weekend daytime deserves targeted investment. Add trips or deploy larger vehicles on corridors that connect residential belts to regional centres and recreation zones. Use the high count classes to prioritise stops for queue management and passenger information. Finally, treat these results as a screening layer. Re run the analysis with a knn neighbour scheme as a sensitivity check, extend the weekend window to confirm persistence, and compare outcomes with observed load factors to translate spatial statistics into service adjustments that riders will feel.\n\n\n\n\n\n\nNote\n\n\n\nWe can use below code chunk to generate a compact frequency table of EHSA classes by period. It groups by period and classification, counts hexagons in each combination, and sorts from most common to least. The table supports quick plausibility checks, helps identify dominant hot or cold patterns, and provides precise numbers to reference in the narrative and figure captions.\n\ndplyr::count(ehsa_all, period, classification, sort = TRUE)\n\nSimple feature collection with 11 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 3792.538 ymin: 26211.61 xmax: 48792.54 ymax: 50460.32\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                   period       classification   n\n1  Weekday Afternoon Peak                 &lt;NA&gt; 826\n2    Weekday Morning Peak                 &lt;NA&gt; 826\n3    Weekend/Holiday Peak                 &lt;NA&gt; 433\n4    Weekend/Holiday Peak  no pattern detected 247\n5    Weekend/Holiday Peak    sporadic coldspot  43\n6    Weekend/Holiday Peak  persistent coldspot  39\n7    Weekend/Holiday Peak     sporadic hotspot  36\n8    Weekend/Holiday Peak intensifying hotspot  20\n9    Weekend/Holiday Peak   persistent hotspot   4\n10   Weekend/Holiday Peak  consecutive hotspot   3\n                         geometry\n1  MULTIPOLYGON (((3792.538 30...\n2  MULTIPOLYGON (((3792.538 30...\n3  MULTIPOLYGON (((4167.538 30...\n4  MULTIPOLYGON (((19542.54 32...\n5  MULTIPOLYGON (((19167.54 32...\n6  MULTIPOLYGON (((9417.538 30...\n7  MULTIPOLYGON (((26292.54 30...\n8  MULTIPOLYGON (((11292.54 33...\n9  MULTIPOLYGON (((40542.54 37...\n10 MULTIPOLYGON (((28542.54 31...\n\n\nThe frequency table confirms what the maps suggested. During the weekday morning period and the weekday afternoon period, almost every hexagon carries no significant emerging signal. Each panel reports about 826 cells with missing class values, which in this workflow reflects features that failed the significance screen and therefore received no label after the join back to geometry. In other words, within the short commuting windows the local Gi star sequences are largely stable rather than trending upward or downward, so the Mann Kendall test does not indicate meaningful change. This supports an operations focus on reliability and crowd management rather than network redesign inside those hours.\nThe weekend or holiday period shows greater variety, although a large share still carries no significant change with about 433 cells. Among the labelled results, the most common class is no pattern detected with about 245 cells, followed by sporadic hotspot with about 41 cells and persistent coldspot with about 39 cells. Sporadic coldspot appears about 36 times, intensifying hotspot about 19 times, consecutive hotspot about 7 times, and new coldspot appears a few times. The mix tells us that late morning to evening on non working days has real but spatially contained growth around town centres, leisure corridors, and interchange areas, while many other places remain consistently quiet. Service planning should therefore prioritise targeted weekend boosts on the corridors and stops that fall in intensifying or consecutive hotspots, while weekday peaks should emphasise headway control, signal priority, and dwell discipline to keep established flows moving smoothly.\n\n\n\n\n11.7 Visualising EHSA maps in different windows\nTo communicate results clearly to readers who first look at plots, this section defines a single mapping function that produces publication quality thematic maps with consistent design choices. The function adds a neutral mainland outline for context, fills hexagons by the classification field using a readable categorical palette, draws light borders to preserve cell shapes, and places the legend outside the frame so that the data area stays uncluttered.\n\n# ---- 11.5 Mapping helper ------------------------------------------------\nplot_ehsa_map &lt;- function(x_sf, map_title) {\n  tmap_mode(\"plot\")\n  tm_shape(sg_outline) + tm_polygons(col = \"palegreen3\", border.col = NA) +\n    tm_shape(x_sf) +\n    tm_fill(\"classification\",\n            title = \"\",\n            palette = \"Set1\",\n            textNA = \"Not significant / No data\") +\n    tm_borders(col = \"grey50\", lwd = 0.2) +\n    tm_layout(\n      title = map_title,\n      title.position = c(\"center\", \"top\"),\n      title.size = 1.3,\n      title.fontface = \"bold\",\n      frame = TRUE,\n      legend.position = c(\"right\", \"bottom\"),\n      legend.title.size = 0.7,\n      legend.text.size = 0.65,\n      outer.margins = 0,\n      inner.margins = c(0.02, 0.02, 0.15, 0.02)\n    )\n}\n\n# Draw three maps\nplot_ehsa_map(filter(ehsa_all, period == \"Weekday Morning Peak\"),\n              \"EHSA of Origin Trips — Weekday Morning Peak\")\n\n\n\n\n\n\n\nplot_ehsa_map(filter(ehsa_all, period == \"Weekday Afternoon Peak\"),\n              \"EHSA of Origin Trips — Weekday Afternoon Peak\")\n\n\n\n\n\n\n\nplot_ehsa_map(filter(ehsa_all, period == \"Weekend/Holiday Peak\"),\n              \"EHSA of Origin Trips — Weekend or Holiday Peak\")\n\n\n\n\n\n\n\n\nReading the three EHSA maps together shows a sharp contrast between weekday commuting periods and weekend activity. In the weekday morning map almost all analytical cells are white, which means the Gi star sequences across the hours from six to nine do not exhibit a consistent upward or downward tendency that passes the Mann Kendall test at the five percent level. This does not mean there is no morning demand. Rather it signals that within this tight three hour window the local intensity is stable or noisy instead of directional. Where a few isolated coloured cells appear they do not form a coherent structure across the island and therefore carry little strategic meaning on their own.\nA very similar message appears in the weekday afternoon map. Again most cells are not significant. The period from seventeen to twenty captures home bound and after work travel that is well established in the network. The absence of emerging hot or cold classes suggests a mature pattern with reliable peaks but without strong within period trajectories. For operations this favours actions that protect reliability, such as priority measures, dwell control, and headway management, more than changes in routing or capacity distribution inside the short window.\nThe weekend or holiday map tells a different story. Many coloured cells appear and several categories are visible at once. Persistent cold classes line up with water bodies and large parks where bus origins remain low on weekends, while new or sporadic cold classes suggest employment districts that quieten when offices close. In contrast, persistent and intensifying hot classes cluster around major town centres, interchange nodes, and waterfront leisure corridors where shopping and recreation attract sustained flows from late morning to evening. Consecutive hot classes near dense transfer areas point to a recent pickup that persisted into the last hour set, and sporadic hot classes around event focused precincts hint at irregular surges.\nThe practical implications are clear. Weekday peaks look structurally stable, so focus resources on keeping services reliable and clearing bottlenecks. Weekend midday to evening shows genuine growth corridors, so plan extra capacity on routes that connect residential belts to regional centres, parks, and coastal attractions. Monitor cells that moved to intensifying or consecutive hot classes because they are strong candidates for targeted frequency increases, stop level crowd management, or demand responsive extras on specific corridors. Finally, remember that the trend test uses short hour series, so it is worth validating these findings with a longer weekend window or daily bins and by checking an alternative neighbour structure to confirm robustness."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#discussion",
    "href": "Take-home_Ex02/take-home_ex02.html#discussion",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "12 Discussion",
    "text": "12 Discussion\nHere’s a direct, evidence-based answer to the 4 research questions, grounded in this study and its EHSA/LMSA workflow.\nRQ1 — Spatial distribution\nWeekday demand is strongly diurnal and spatially patterned. Our own peak-hour maps show that weekday mornings (06:00–09:00) concentrate origins in northern and eastern heartlands—Woodlands, Yishun, Sengkang, Tampines, Bedok, Bukit Panjang and Jurong West—consistent with home-to-work flows leaving residential areas for employment centres. By contrast, weekday afternoons (17:00–20:00) shift intensity toward the Central Region (Orchard, Downtown Core, Kallang), reflecting return trips and after-work activity. Weekends and holidays (11:00–20:00) exhibit a broader, flatter profile with dispersed origins that remain prominent around retail and leisure corridors and town centres from late morning into evening. These patterns align with the diagnostic hourly profile that shows two clear weekday peaks and a single broad weekend peak, validating the period definitions used in the analysis.\nRQ2 — Local spatial clustering\nThe study applies Local Moran’s I, and Getis-Ord Gi* to detect where high or low ridership clusters and spatial outliers occur, moving beyond global summaries to location-specific evidence. These local statistics reveal neighbourhoods of consistently high or low values and pinpoint outliers that diverge from their surroundings, strengthening the descriptive reading of the peak maps. In practice, the observed morning heartland concentrations and afternoon central-area intensifications manifest as high-value clusters (hot clusters) in residential belts during AM and around core employment/retail zones during PM, while water-dominated or park areas and certain weekend-quiet employment precincts register as low-value clusters (cold clusters). Using LMSA in tandem with the period-specific mapping thus ties intuitive patterns to statistically significant local structure, ensuring that subsequent policy discussion rests on formal evidence rather than visual impression alone.\nRQ3 — Temporal evolution (EHSA with Mann–Kendall)\nEHSA on space–time Local Getis-Ord Gi* sequences shows a clear contrast between weekday peaks and weekends. For weekday morning and afternoon windows, most hexagons are not significant in the Mann–Kendall trend test; the short three-hour windows appear stable or noisy rather than trending upward or downward, indicating mature, well-established commuting peaks without strong within-period trajectories. On weekends/holidays, many cells become significant and a variety of EHSA classes emerge. Persistent and intensifying hot spots cluster around town centres, interchange nodes and waterfront leisure corridors, signalling sustained or strengthening midday-to-evening flows. Persistent cold spots coincide with water bodies and large parks; “new” or sporadic cold spots appear in office districts that quieten when workplaces are closed. Consecutive and sporadic hot spots around event-oriented precincts indicate recent or irregular surges that merit monitoring. Together, these results show stability on workday peaks but genuine growth and variability on weekends.\nRQ4 — Policy and planning implications\nBecause weekday peaks are structurally stable within their short windows, operations should prioritise reliability over major structural changes: protect headways, manage dwell times, and address recurrent bottlenecks rather than reallocating capacity inside the three-hour peaks. In contrast, weekend/holiday midday–evening corridors with persistent or intensifying hot spots warrant targeted capacity increases—additional trips or larger vehicles—especially on links from residential belts to regional centres, parks and coastal attractions. Cells that moved into intensifying or consecutive hot classes are prime candidates for stop-level crowd management, queue design, and real-time passenger information. As a good practice, treat EHSA as a screening layer: validate hot/cold trends against observed loads, test neighbour definitions (e.g., k-nearest-neighbours sensitivity), and consider extending the weekend window to confirm persistence before committing long-term resources. These steps translate the spatial–temporal evidence into actionable network adjustments that riders will feel."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#conclusion",
    "href": "Take-home_Ex02/take-home_ex02.html#conclusion",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "13 Conclusion",
    "text": "13 Conclusion\nThis study demonstrates that a careful combination of credible data, defensible spatial units, and rigorous local statistics can turn raw ridership counts into actionable intelligence for a bus network. Using LTA DataMall origins linked to the mainland bus-stop system and aggregated to a validated 750-metre hexagon grid, we built a fully balanced space–time panel and applied two complementary lenses: local measures of spatial autocorrelation to locate meaningful clusters, and EHSA to track how those clusters evolve within defined peak windows. The workflow—projection to SVY21, mainland masking, active-cell filtering, stable IDs, harmonised cartography, neighbour and weight specification, and reproducible code—ensures that every figure and metric can be regenerated and audited. ￼\nThree broad findings follow. First, weekday peaks are structurally stable. Both morning and afternoon windows show very few cells with significant emerging trends, indicating mature commuting patterns that repeat predictably within those tight three-hour slices. This does not imply weak demand; rather, it shows that within-window Gi* sequences are steady enough that the Mann–Kendall test rarely flags change. Second, weekends and holidays are different. EHSA returns a diverse mix of classes, with persistent and intensifying hot spots around town centres, major interchanges, and waterfront leisure corridors, while persistent cold spots coincide with parks, reservoirs, and water. Sporadic and consecutive hot spots near event-oriented districts reveal irregular but important surges that deserve operational attention. Third, the spatial story seen in descriptive maps is confirmed statistically: morning origins concentrate in northern and eastern heartlands, afternoon activity tilts toward the Central Region, and weekend flows broaden along retail and recreation axes.\nPolicy implications are direct. On weekdays, the priority should be reliability: protect headways, manage dwell times, relieve recurring choke points, and use targeted bus-priority or stop-level design to keep predictable peaks moving. On weekends and holidays, deploy flexible capacity where the evidence shows growth—extra trips or larger vehicles on links between residential belts and leisure or retail clusters; queue and crowd management at stops inside intensifying or consecutive hot spots; and real-time information to smooth surges. Finally, treat EHSA as a screening tool and institutionalise sensitivity checks: test alternative neighbour schemes, extend or shift weekend windows when warranted, and triangulate with observed loads. Taken together, these steps translate spatial analytics into service choices that improve rider experience while using resources where they matter most."
  },
  {
    "objectID": "Take-home_Ex02/take-home_ex02.html#references",
    "href": "Take-home_Ex02/take-home_ex02.html#references",
    "title": "Take-home Ex02: Analyzing Neighborhood Bus Mobility and Emerging Urban Trends",
    "section": "14 References",
    "text": "14 References\n\nBoschan, J.A. and Roman, C.G. (2024) ‘Hot spots of gun violence in the era of focused deterrence: A space-time analysis of shootings in South Philadelphia’, Social Sciences, Vol. 13.\nKam, T.S. (2025) ‘Chapter 8: Spatial Weights and Applications’, R for Geospatial Data Science and Analytics (R4GDSA). Available at: https://r4gdsa.netlify.app/chap08.html (Accessed: October 2025).\nKam, T.S. (2025) ‘Chapter 9: Global Measures of Spatial Autocorrelation’, R for Geospatial Data Science and Analytics (R4GDSA). Available at: https://r4gdsa.netlify.app/chap09.html (Accessed: October 2025).\nKam, T.S. (2025) ‘Chapter 10: Local Measures of Spatial Autocorrelation’, R for Geospatial Data Science and Analytics (R4GDSA). Available at: https://r4gdsa.netlify.app/chap10.html (Accessed: 23 October 2025).\nKam, T.S. (2025) ‘In-Class Exercise 06: Emerging Hot Spot Analysis (EHSA)’, ISSS626 Geospatial Analytics and Applications (AY2025/26 – August Term). Available at: https://isss626-ay2025-26aug.netlify.app/in-class_ex/in-class_ex06/in-class_ex06-ehsa#/title-slide (Accessed: October 2025).\nKim, M. and Lee, S. (2023) ‘Identification of emerging roadkill hotspots on Korean expressways using space–time cubes’, International Journal of Environmental Research and Public Health, 20(6).\nKam, T.S. (2025) ‘Lesson 4: Spatial Weights and Applications’, ISSS626 Geospatial Analytics and Applications (AY2025/26 – August Term). Available at: https://isss626-ay2025-26aug.netlify.app/lesson/lesson04/lesson04-spatial_weights#/title-slide (Accessed: October 2025).\nKam, T.S. (2025) ‘Lesson 5: Global Spatial Autocorrelation (GLSA)’, ISSS626 Geospatial Analytics and Applications (AY2025/26 – August Term). Available at: https://isss626-ay2025-26aug.netlify.app/lesson/lesson05/lesson05-glsa#/title-slide (Accessed: October 2025).\nMack, Z.W.V. and Kam, T.S. (2018) ‘Is there space for violence?: A data-driven approach to the exploration of spatial-temporal dimensions of conflict’, Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities, Seattle, WA, USA, 6 November 2018, pp. 1–10. ACM Digital Library. doi: 10.1145/3282933.3282935.\nTan, Y.Y. and Kam, T.S. (2019) ‘Exploring and visualizing household electricity consumption patterns in Singapore: A geospatial analytics approach’, Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, March 31–April 3, 2019: Proceedings, pp. 785–796. Springer. doi: 10.1007/978-3-030-15742-5_74.\nTransportation Modelling Group, University of Toronto (2021) Traffic Zone Guidance: March 2021 (Final Report). Toronto: University of Toronto. Available at: https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf (Accessed: October 2025)."
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html",
    "href": "Hands-on_Ex08/hand-on_ex08.html",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "This practical reproduces the complete workflow to build geographically weighted predictive models (GWR and Geographically Weighted Random Forest), alongside non-spatial baselines (Multiple Linear Regression and Random Forest). Follow each subsection in order.\n\n\nPredictive modelling estimates an unknown outcome (e.g., resale price) from known predictors (e.g., floor area, amenities). When observations are georeferenced, relationships may vary across space due to infrastructure, socio-economic and environmental context. Geographically weighted models allow coefficients or model structure to change by location, capturing local effects that global models miss.\nBy the end, we will:\n\nprepare train/test datasets with proper sampling,\n\ncheck collinearity,\n\nfit and save MLR, GWR, RF, and GW-RF models,\n\ngenerate out-of-sample predictions,\n\ncompute RMSE and visualise prediction quality.\n\n\n\n\nwe will work with:\n\nAspatial table: HDB resale transactions (CSV → converted to sf during preprocessing).\n\nGeospatial layers: URA 2014 Master Plan Planning Subzones (polygon sf).\n\nLocational factors with coordinates: eldercare, hawker centres, parks, supermarkets,\n\nMRT/LRT stations, bus stops, kindergartens, childcare (shapefile/GeoJSON).\n\nLocational factors without coordinates: CBD centroid (derived), shopping malls, primary school rankings (CSV/other).\n\n\n\n\n\n# Create/load all required packages in one shot\npacman::p_load(\n  sf,            # spatial vector data (simple features)\n  spdep,         # spatial dependence utilities (used by some workflows)\n  GWmodel,       # Geographically Weighted Regression\n  SpatialML,     # Geographically Weighted Random Forest (grf)\n  tmap,          # cartography (not central here but part of the stack)\n  rsample,       # train/test splitting (tidymodels)\n  Metrics,       # RMSE and other metrics\n  tidyverse      # dplyr, ggplot2, readr, purrr, etc.\n)\n\n\n\n\n\n\n\n# Read the prepared modelling dataset (sf object)\nmdata &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/rawdata/mdata.rds\")\n\n\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n# Set seed to make the split reproducible\nset.seed(1234)\n\n# Split into 65% training and 35% testing using rsample\nresale_split &lt;- rsample::initial_split(mdata, prop = 6.5/10)\n\n# Extract the two partitions\ntrain_data &lt;- rsample::training(resale_split)\ntest_data  &lt;- rsample::testing(resale_split)\n\n\n# Persist the splits for reuse\nreadr::write_rds(train_data, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\nreadr::write_rds(test_data,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")\n\n\n\n\n\nBefore loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicollinearity.\n\n# Remove geometry to compute numeric correlations only\nmdata_nogeo &lt;- mdata %&gt;% sf::st_drop_geometry()\n\n# Draw an upper-triangle correlation matrix with numbers\ncorrplot::corrplot(\n  cor(mdata_nogeo[, 2:17]), # adjust columns to our numeric predictors\n  diag   = FALSE,           # do not draw the diagonal\n  order  = \"AOE\",           # sort for visual clarity\n  tl.pos = \"td\",            # variable labels on top diagonal\n  tl.cex = 0.5,             # smaller labels\n  method = \"number\",        # show correlation values\n  type   = \"upper\"          # only the upper triangle\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf all absolute correlations are below ~0.8, severe multicollinearity is unlikely.\n\n\n\n\n\n\n# Reload saved splits when resuming work\ntrain_data &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\ntest_data  &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")\n\n\n\n\n\n# Fit a global (non-spatial) linear regression as a baseline\nprice_mlr &lt;- lm(\n  resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data\n)\n\n# Inspect coefficients and diagnostics\nsummary(price_mlr)\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Save the fitted model\nreadr::write_rds(price_mlr, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/price_mlr.rds\")\n\n\n\n\nIn this section, we will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\n# Determine optimal adaptive bandwidth (in neighbors) using CV\nbw_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,   # training sf\n  approach = \"CV\",         # cross-validation\n  kernel   = \"gaussian\",   # Gaussian kernel\n  adaptive = TRUE,         # adaptive neighbor count\n  longlat  = FALSE         # data are in projected meters\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.966278e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871966e+12 \nAdaptive bandwidth: 41 CV score: 6.793327e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \n\n# Persist the chosen bandwidth (e.g., result may be 40 neighbors)\nreadr::write_rds(bw_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\n\n\n\nFirst, let us call the save bandwidth by using the code chunk below.\n\n# Reload bandwidth when needed\nbw_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n# Calibrate GWR using the selected adaptive bandwidth\ngwr_adaptive &lt;- GWmodel::gwr.basic(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,    # training sf\n  bw       = bw_adaptive,   # adaptive neighbors (numeric)\n  kernel   = \"gaussian\",    # kernel shape\n  adaptive = TRUE,          # use adaptive bandwidth\n  longlat  = FALSE          # projected coordinates\n)\n\n# Save the fitted GWR object\nreadr::write_rds(gwr_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n\n\n\nThe code chunk below will be used to retrieve the save gwr model object.\n\n# Reload GWR model when resuming work\ngwr_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\n# Printing the object shows the model summary header and timings\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-24 16:35:37.612412 \n   Call:\n   GWmodel::gwr.basic(formula = resale_price ~ floor_area_sqm + \n    storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n    PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + \n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n    WITHIN_1KM_PRISCH, data = train_data, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2594e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2291e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1660e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1881e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2489e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5224e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0262e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.8 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209 \n   Residual sum of squares: 4.829177e+12 \n   R-square value:  0.9676571 \n   Adjusted R-square value:  0.9611535 \n\n   ***********************************************************************\n   Program stops at: 2025-10-24 16:36:34.25672 \n\n\n\n\n\n\n# Some workflows also derive a CV bandwidth using the test sf\ngwr_bw_test_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = test_data,\n  approach = \"CV\",\n  kernel   = \"gaussian\",\n  adaptive = TRUE,\n  longlat  = FALSE\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3447 CV score: 1.902155e+13 \nAdaptive bandwidth: 2138 CV score: 1.752645e+13 \nAdaptive bandwidth: 1328 CV score: 1.556299e+13 \nAdaptive bandwidth: 828 CV score: 1.357498e+13 \nAdaptive bandwidth: 518 CV score: 1.030751e+13 \nAdaptive bandwidth: 327 CV score: 8.348364e+12 \nAdaptive bandwidth: 208 CV score: 6.860544e+12 \nAdaptive bandwidth: 135 CV score: 5.969504e+12 \nAdaptive bandwidth: 89 CV score: 5.242221e+12 \nAdaptive bandwidth: 62 CV score: 4.742767e+12 \nAdaptive bandwidth: 43 CV score: 4.357839e+12 \nAdaptive bandwidth: 34 CV score: 4.125848e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \nAdaptive bandwidth: 23 CV score: 4.236349e+13 \nAdaptive bandwidth: 30 CV score: 4.074906e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \n\n# Save the test-set bandwidth too (for reference)\nreadr::write_rds(gwr_bw_test_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_bw_test_adaptive.rds\")\n\n\n\n\n\n# gwr_pred &lt;- gwr.predict(\n#   formula = resale_price ~ floor_area_sqm + \n#     storey_order + remaining_lease_mths + \n#     PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n#     PROX_MRT + PROX_PARK + PROX_MALL + \n#     PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#     WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n#     WITHIN_1KM_PRISCH, \n#   data=train_data, \n#   predictdata = test_data, \n#   bw=40, \n#   kernel = 'gaussian', \n#   adaptive=TRUE, \n#   longlat = FALSE)\n# \n# # Save predictions (list-like object)\n# readr::write_rds(gwr_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_pred.rds\")\n\n\n\n\nIn this section, we will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\n\n\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\n# Extract XY matrices for convenience (sf → numeric matrix with X,Y)\ncoords       &lt;- sf::st_coordinates(mdata)\ncoords_train &lt;- sf::st_coordinates(train_data)\ncoords_test  &lt;- sf::st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future used.\n\n# Save coordinates used later by GW-RF\nreadr::write_rds(coords_train, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_train.rds\")\nreadr::write_rds(coords_test,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_test.rds\")\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\n# Random Forest from ranger expects a data.frame without geometry\ntrain_data_nogeom &lt;- train_data %&gt;% sf::st_drop_geometry()\ntest_data_nogeom  &lt;- test_data  %&gt;% sf::st_drop_geometry()\n\n\n\n\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Fit a global RF as another baseline model\nrf &lt;- ranger::ranger(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data_nogeom\n)\n\nrf \n\nRanger result\n\nCall:\n ranger::ranger(formula = resale_price ~ floor_area_sqm + storey_order +      remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +      PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n# Save the RF model\nreadr::write_rds(rf, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/rf.rds\")\n\n\n\n\n\nIn this section, we will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\n\n\n# Reuse coordinates and no-geometry training input\nset.seed(1234)\n\ngwRF_adaptive &lt;- SpatialML::grf(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  dframe = train_data_nogeom, # data.frame without geometry\n  bw     = 40,                # neighbor size (consistent with earlier choice)\n  kernel = \"adaptive\",        # adaptive kernel for local forests\n  coords = coords_train       # matrix of X,Y for training rows\n)\n\n\nNumber of Observations: 10335\n\n\nNumber of Independent Variables: 14\n\n\nKernel: Adaptive\nNeightbours: 40\n\n\n\n--------------- Global ML Model Summary ---------------\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       697593819 \nR squared (OOB):                  0.9517189 \n\n\n\nImportance:\n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.413197e+12             1.538950e+13             2.890637e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.310066e+13             7.285092e+12             5.568548e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.369745e+12             4.894344e+12             4.223286e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.793853e+12             1.018586e+12             1.710374e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.589501e+12             6.794634e+12 \n\n\n\nMean Square Error (Not OOB): 173951416.766\n\n\nR-squared (Not OOB) %: 98.796\n\n\nAIC (Not OOB): 196129.252\n\n\nAICc (Not OOB): 196129.299\n\n\n\n--------------- Local Model Summary ---------------\n\n\n\nResiduals OOB:\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-207638.9  -13044.4     517.7     668.7   15329.3  380000.0 \n\n\n\nResiduals Predicted (Not OOB):\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-80918.36  -3710.39     80.88     60.19   4119.23  67858.13 \n\n\n\nLocal Variable Importance:\n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 244740042006 10095828392 21812835540\nstorey_order             185976628 195162288767 10691049214 16507251138\nremaining_lease_mths     364347875 420068216877 18525480676 40200134557\nPROX_CBD                         0 261367685029  7551870678 20050530584\nPROX_ELDERLYCARE                 0 260937815286  6797406011 17221278502\nPROX_HAWKER                      0 232903486867  6917875149 17526152433\nPROX_MRT                         0 192977091714  6219493848 14339970325\nPROX_PARK                        0 270208600970  5678244629 13059236132\nPROX_MALL                        0 290212372886  6903172498 18141558541\nPROX_SUPERMARKET                 0 283979765533  6391052599 16785880266\nWITHIN_350M_KINDERGARTEN         0 124340534149  1694367905  7648420853\nWITHIN_350M_CHILDCARE            0 194759670980  3113750592 10738676578\nWITHIN_350M_BUS                  0 132614671785  3014600716  8056548827\nWITHIN_1KM_PRISCH                0 132884456893  1116898699  5401808933\n\n\n\nMean squared error (OOB): 932642623.291\n\n\nR-squared (OOB) %: 93.544\n\n\nAIC (OOB): 213484.26\n\n\nAICc (OOB): 213484.306\n\n\nMean squared error Predicted (Not OOB): 80652745.326\n\n\nR-squared Predicted (Not OOB) %: 99.442\n\n\nAIC Predicted (Not OOB): 188185.531\n\n\nAICc Predicted (Not OOB): 188185.578\n\n\n\nCalculation time (in seconds): 11.5451\n\n# Persist fitted GW-RF\nreadr::write_rds(gwRF_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a computational intensity and time consuming process. It is wiser to save the output as an rds file for future used without having to re-run the process again.\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n# (Later) reload if needed\ngwRF_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n# Combine the test attributes with their coordinates for prediction\ntest_data_nogeom &lt;- cbind(test_data, coords_test) %&gt;%\n  sf::st_drop_geometry()     # drop geometry to keep plain columns\n\n\n\n\n\n# Generate local predictions at test locations\ngwRF_pred &lt;- SpatialML::predict.grf(\n  gwRF_adaptive,     # trained GW-RF model\n  test_data_nogeom,  # test attributes + X,Y columns\n  x.var.name = \"X\",  # column name for X coordinate\n  y.var.name = \"Y\",  # column name for Y coordinate\n  local.w    = 1,    # weight for local component\n  global.w   = 0     # set to 0 to use purely local predictions\n)\n\n# Save the prediction vector\nGRF_pred &lt;- readr::write_rds(gwRF_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\n\n\n\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\n# Reload predictions when needed and coerce to data.frame\nGRF_pred     &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\nGRF_pred_df  &lt;- as.data.frame(GRF_pred)  # single column with predicted values\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\n# Attach predictions to the test data and retain only needed columns\ntest_data_p &lt;- cbind(test_data, GRF_pred_df) %&gt;%\n  dplyr::select(resale_price, GRF_pred)   # rename matches grf() output\n\n\n# Save the paired actual vs predicted for later evaluation/plots\nreadr::write_rds(test_data_p, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data_p.rds\")\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n# Compute RMSE between observed and GW-RF predicted prices\nMetrics::rmse(\n  test_data_p$resale_price,  # actual values\n  test_data_p$GRF_pred       # predicted values\n)\n\n[1] 28160.87\n\n# Example output in the reference workflow: 28160.87\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n# Scatter plot of Predicted (x) vs Actual (y)\nggplot2::ggplot(\n  data = test_data_p,                           # data with both columns\n  ggplot2::aes(x = GRF_pred, y = resale_price)  # map axes\n) +\n  ggplot2::geom_point()                         # draw points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model."
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#overview",
    "href": "Hands-on_Ex08/hand-on_ex08.html#overview",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "Predictive modelling estimates an unknown outcome (e.g., resale price) from known predictors (e.g., floor area, amenities). When observations are georeferenced, relationships may vary across space due to infrastructure, socio-economic and environmental context. Geographically weighted models allow coefficients or model structure to change by location, capturing local effects that global models miss.\nBy the end, we will:\n\nprepare train/test datasets with proper sampling,\n\ncheck collinearity,\n\nfit and save MLR, GWR, RF, and GW-RF models,\n\ngenerate out-of-sample predictions,\n\ncompute RMSE and visualise prediction quality."
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#the-data",
    "href": "Hands-on_Ex08/hand-on_ex08.html#the-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "we will work with:\n\nAspatial table: HDB resale transactions (CSV → converted to sf during preprocessing).\n\nGeospatial layers: URA 2014 Master Plan Planning Subzones (polygon sf).\n\nLocational factors with coordinates: eldercare, hawker centres, parks, supermarkets,\n\nMRT/LRT stations, bus stops, kindergartens, childcare (shapefile/GeoJSON).\n\nLocational factors without coordinates: CBD centroid (derived), shopping malls, primary school rankings (CSV/other)."
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex08/hand-on_ex08.html#installing-and-loading-r-packages",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Create/load all required packages in one shot\npacman::p_load(\n  sf,            # spatial vector data (simple features)\n  spdep,         # spatial dependence utilities (used by some workflows)\n  GWmodel,       # Geographically Weighted Regression\n  SpatialML,     # Geographically Weighted Random Forest (grf)\n  tmap,          # cartography (not central here but part of the stack)\n  rsample,       # train/test splitting (tidymodels)\n  Metrics,       # RMSE and other metrics\n  tidyverse      # dplyr, ggplot2, readr, purrr, etc.\n)"
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#preparing-data",
    "href": "Hands-on_Ex08/hand-on_ex08.html#preparing-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Read the prepared modelling dataset (sf object)\nmdata &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/rawdata/mdata.rds\")\n\n\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n# Set seed to make the split reproducible\nset.seed(1234)\n\n# Split into 65% training and 35% testing using rsample\nresale_split &lt;- rsample::initial_split(mdata, prop = 6.5/10)\n\n# Extract the two partitions\ntrain_data &lt;- rsample::training(resale_split)\ntest_data  &lt;- rsample::testing(resale_split)\n\n\n# Persist the splits for reuse\nreadr::write_rds(train_data, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\nreadr::write_rds(test_data,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#computing-correlation-matrix",
    "href": "Hands-on_Ex08/hand-on_ex08.html#computing-correlation-matrix",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "Before loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicollinearity.\n\n# Remove geometry to compute numeric correlations only\nmdata_nogeo &lt;- mdata %&gt;% sf::st_drop_geometry()\n\n# Draw an upper-triangle correlation matrix with numbers\ncorrplot::corrplot(\n  cor(mdata_nogeo[, 2:17]), # adjust columns to our numeric predictors\n  diag   = FALSE,           # do not draw the diagonal\n  order  = \"AOE\",           # sort for visual clarity\n  tl.pos = \"td\",            # variable labels on top diagonal\n  tl.cex = 0.5,             # smaller labels\n  method = \"number\",        # show correlation values\n  type   = \"upper\"          # only the upper triangle\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf all absolute correlations are below ~0.8, severe multicollinearity is unlikely."
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#retrieving-the-stored-data",
    "href": "Hands-on_Ex08/hand-on_ex08.html#retrieving-the-stored-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Reload saved splits when resuming work\ntrain_data &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\ntest_data  &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#building-a-non-spatial-multiple-linear-regression",
    "href": "Hands-on_Ex08/hand-on_ex08.html#building-a-non-spatial-multiple-linear-regression",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Fit a global (non-spatial) linear regression as a baseline\nprice_mlr &lt;- lm(\n  resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data\n)\n\n# Inspect coefficients and diagnostics\nsummary(price_mlr)\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Save the fitted model\nreadr::write_rds(price_mlr, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/price_mlr.rds\")"
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#gwr-predictive-method",
    "href": "Hands-on_Ex08/hand-on_ex08.html#gwr-predictive-method",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "In this section, we will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\n# Determine optimal adaptive bandwidth (in neighbors) using CV\nbw_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,   # training sf\n  approach = \"CV\",         # cross-validation\n  kernel   = \"gaussian\",   # Gaussian kernel\n  adaptive = TRUE,         # adaptive neighbor count\n  longlat  = FALSE         # data are in projected meters\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.966278e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871966e+12 \nAdaptive bandwidth: 41 CV score: 6.793327e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \n\n# Persist the chosen bandwidth (e.g., result may be 40 neighbors)\nreadr::write_rds(bw_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\n\n\n\nFirst, let us call the save bandwidth by using the code chunk below.\n\n# Reload bandwidth when needed\nbw_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n# Calibrate GWR using the selected adaptive bandwidth\ngwr_adaptive &lt;- GWmodel::gwr.basic(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,    # training sf\n  bw       = bw_adaptive,   # adaptive neighbors (numeric)\n  kernel   = \"gaussian\",    # kernel shape\n  adaptive = TRUE,          # use adaptive bandwidth\n  longlat  = FALSE          # projected coordinates\n)\n\n# Save the fitted GWR object\nreadr::write_rds(gwr_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n\n\n\nThe code chunk below will be used to retrieve the save gwr model object.\n\n# Reload GWR model when resuming work\ngwr_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\n# Printing the object shows the model summary header and timings\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-24 16:35:37.612412 \n   Call:\n   GWmodel::gwr.basic(formula = resale_price ~ floor_area_sqm + \n    storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n    PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + \n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n    WITHIN_1KM_PRISCH, data = train_data, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2594e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2291e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1660e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1881e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2489e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5224e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0262e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.8 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209 \n   Residual sum of squares: 4.829177e+12 \n   R-square value:  0.9676571 \n   Adjusted R-square value:  0.9611535 \n\n   ***********************************************************************\n   Program stops at: 2025-10-24 16:36:34.25672 \n\n\n\n\n\n\n# Some workflows also derive a CV bandwidth using the test sf\ngwr_bw_test_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = test_data,\n  approach = \"CV\",\n  kernel   = \"gaussian\",\n  adaptive = TRUE,\n  longlat  = FALSE\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3447 CV score: 1.902155e+13 \nAdaptive bandwidth: 2138 CV score: 1.752645e+13 \nAdaptive bandwidth: 1328 CV score: 1.556299e+13 \nAdaptive bandwidth: 828 CV score: 1.357498e+13 \nAdaptive bandwidth: 518 CV score: 1.030751e+13 \nAdaptive bandwidth: 327 CV score: 8.348364e+12 \nAdaptive bandwidth: 208 CV score: 6.860544e+12 \nAdaptive bandwidth: 135 CV score: 5.969504e+12 \nAdaptive bandwidth: 89 CV score: 5.242221e+12 \nAdaptive bandwidth: 62 CV score: 4.742767e+12 \nAdaptive bandwidth: 43 CV score: 4.357839e+12 \nAdaptive bandwidth: 34 CV score: 4.125848e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \nAdaptive bandwidth: 23 CV score: 4.236349e+13 \nAdaptive bandwidth: 30 CV score: 4.074906e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \n\n# Save the test-set bandwidth too (for reference)\nreadr::write_rds(gwr_bw_test_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_bw_test_adaptive.rds\")\n\n\n\n\n\n# gwr_pred &lt;- gwr.predict(\n#   formula = resale_price ~ floor_area_sqm + \n#     storey_order + remaining_lease_mths + \n#     PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n#     PROX_MRT + PROX_PARK + PROX_MALL + \n#     PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#     WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n#     WITHIN_1KM_PRISCH, \n#   data=train_data, \n#   predictdata = test_data, \n#   bw=40, \n#   kernel = 'gaussian', \n#   adaptive=TRUE, \n#   longlat = FALSE)\n# \n# # Save predictions (list-like object)\n# readr::write_rds(gwr_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_pred.rds\")\n\n\n\n\nIn this section, we will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\n\n\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\n# Extract XY matrices for convenience (sf → numeric matrix with X,Y)\ncoords       &lt;- sf::st_coordinates(mdata)\ncoords_train &lt;- sf::st_coordinates(train_data)\ncoords_test  &lt;- sf::st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future used.\n\n# Save coordinates used later by GW-RF\nreadr::write_rds(coords_train, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_train.rds\")\nreadr::write_rds(coords_test,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_test.rds\")\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\n# Random Forest from ranger expects a data.frame without geometry\ntrain_data_nogeom &lt;- train_data %&gt;% sf::st_drop_geometry()\ntest_data_nogeom  &lt;- test_data  %&gt;% sf::st_drop_geometry()\n\n\n\n\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Fit a global RF as another baseline model\nrf &lt;- ranger::ranger(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data_nogeom\n)\n\nrf \n\nRanger result\n\nCall:\n ranger::ranger(formula = resale_price ~ floor_area_sqm + storey_order +      remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +      PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n# Save the RF model\nreadr::write_rds(rf, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/rf.rds\")"
  },
  {
    "objectID": "Hands-on_Ex08/hand-on_ex08.html#calibrating-geographically-weighted-random-forest-gw-rf",
    "href": "Hands-on_Ex08/hand-on_ex08.html#calibrating-geographically-weighted-random-forest-gw-rf",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "In this section, we will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\n\n\n# Reuse coordinates and no-geometry training input\nset.seed(1234)\n\ngwRF_adaptive &lt;- SpatialML::grf(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  dframe = train_data_nogeom, # data.frame without geometry\n  bw     = 40,                # neighbor size (consistent with earlier choice)\n  kernel = \"adaptive\",        # adaptive kernel for local forests\n  coords = coords_train       # matrix of X,Y for training rows\n)\n\n\nNumber of Observations: 10335\n\n\nNumber of Independent Variables: 14\n\n\nKernel: Adaptive\nNeightbours: 40\n\n\n\n--------------- Global ML Model Summary ---------------\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       697593819 \nR squared (OOB):                  0.9517189 \n\n\n\nImportance:\n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.413197e+12             1.538950e+13             2.890637e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.310066e+13             7.285092e+12             5.568548e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.369745e+12             4.894344e+12             4.223286e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.793853e+12             1.018586e+12             1.710374e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.589501e+12             6.794634e+12 \n\n\n\nMean Square Error (Not OOB): 173951416.766\n\n\nR-squared (Not OOB) %: 98.796\n\n\nAIC (Not OOB): 196129.252\n\n\nAICc (Not OOB): 196129.299\n\n\n\n--------------- Local Model Summary ---------------\n\n\n\nResiduals OOB:\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-207638.9  -13044.4     517.7     668.7   15329.3  380000.0 \n\n\n\nResiduals Predicted (Not OOB):\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-80918.36  -3710.39     80.88     60.19   4119.23  67858.13 \n\n\n\nLocal Variable Importance:\n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 244740042006 10095828392 21812835540\nstorey_order             185976628 195162288767 10691049214 16507251138\nremaining_lease_mths     364347875 420068216877 18525480676 40200134557\nPROX_CBD                         0 261367685029  7551870678 20050530584\nPROX_ELDERLYCARE                 0 260937815286  6797406011 17221278502\nPROX_HAWKER                      0 232903486867  6917875149 17526152433\nPROX_MRT                         0 192977091714  6219493848 14339970325\nPROX_PARK                        0 270208600970  5678244629 13059236132\nPROX_MALL                        0 290212372886  6903172498 18141558541\nPROX_SUPERMARKET                 0 283979765533  6391052599 16785880266\nWITHIN_350M_KINDERGARTEN         0 124340534149  1694367905  7648420853\nWITHIN_350M_CHILDCARE            0 194759670980  3113750592 10738676578\nWITHIN_350M_BUS                  0 132614671785  3014600716  8056548827\nWITHIN_1KM_PRISCH                0 132884456893  1116898699  5401808933\n\n\n\nMean squared error (OOB): 932642623.291\n\n\nR-squared (OOB) %: 93.544\n\n\nAIC (OOB): 213484.26\n\n\nAICc (OOB): 213484.306\n\n\nMean squared error Predicted (Not OOB): 80652745.326\n\n\nR-squared Predicted (Not OOB) %: 99.442\n\n\nAIC Predicted (Not OOB): 188185.531\n\n\nAICc Predicted (Not OOB): 188185.578\n\n\n\nCalculation time (in seconds): 11.5451\n\n# Persist fitted GW-RF\nreadr::write_rds(gwRF_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a computational intensity and time consuming process. It is wiser to save the output as an rds file for future used without having to re-run the process again.\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n# (Later) reload if needed\ngwRF_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n# Combine the test attributes with their coordinates for prediction\ntest_data_nogeom &lt;- cbind(test_data, coords_test) %&gt;%\n  sf::st_drop_geometry()     # drop geometry to keep plain columns\n\n\n\n\n\n# Generate local predictions at test locations\ngwRF_pred &lt;- SpatialML::predict.grf(\n  gwRF_adaptive,     # trained GW-RF model\n  test_data_nogeom,  # test attributes + X,Y columns\n  x.var.name = \"X\",  # column name for X coordinate\n  y.var.name = \"Y\",  # column name for Y coordinate\n  local.w    = 1,    # weight for local component\n  global.w   = 0     # set to 0 to use purely local predictions\n)\n\n# Save the prediction vector\nGRF_pred &lt;- readr::write_rds(gwRF_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\n\n\n\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\n# Reload predictions when needed and coerce to data.frame\nGRF_pred     &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\nGRF_pred_df  &lt;- as.data.frame(GRF_pred)  # single column with predicted values\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\n# Attach predictions to the test data and retain only needed columns\ntest_data_p &lt;- cbind(test_data, GRF_pred_df) %&gt;%\n  dplyr::select(resale_price, GRF_pred)   # rename matches grf() output\n\n\n# Save the paired actual vs predicted for later evaluation/plots\nreadr::write_rds(test_data_p, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data_p.rds\")\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n# Compute RMSE between observed and GW-RF predicted prices\nMetrics::rmse(\n  test_data_p$resale_price,  # actual values\n  test_data_p$GRF_pred       # predicted values\n)\n\n[1] 28160.87\n\n# Example output in the reference workflow: 28160.87\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n# Scatter plot of Predicted (x) vs Actual (y)\nggplot2::ggplot(\n  data = test_data_p,                           # data with both columns\n  ggplot2::aes(x = GRF_pred, y = resale_price)  # map axes\n) +\n  ggplot2::geom_point()                         # draw points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html",
    "href": "In-Class_Ex08/in-class_ex08.html",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "",
    "text": "By the end of this in-class exercise, students will master the skill of:\n\nprepare and geocode HDB resale price data for geospatial modelling; and\nperform proximity analysis to count the number of geographic entities located within a defined distance from each HDB property."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#overview",
    "href": "In-Class_Ex08/in-class_ex08.html#overview",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "Predictive modelling estimates an unknown outcome (e.g., resale price) from known predictors (e.g., floor area, amenities). When observations are georeferenced, relationships may vary across space due to infrastructure, socio-economic and environmental context. Geographically weighted models allow coefficients or model structure to change by location, capturing local effects that global models miss.\nBy the end, we will:\n\nprepare train/test datasets with proper sampling,\n\ncheck collinearity,\n\nfit and save MLR, GWR, RF, and GW-RF models,\n\ngenerate out-of-sample predictions,\n\ncompute RMSE and visualise prediction quality."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#the-data",
    "href": "In-Class_Ex08/in-class_ex08.html#the-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "we will work with:\n\nAspatial table: HDB resale transactions (CSV → converted to sf during preprocessing).\n\nGeospatial layers: URA 2014 Master Plan Planning Subzones (polygon sf).\n\nLocational factors with coordinates: eldercare, hawker centres, parks, supermarkets,\n\nMRT/LRT stations, bus stops, kindergartens, childcare (shapefile/GeoJSON).\n\nLocational factors without coordinates: CBD centroid (derived), shopping malls, primary school rankings (CSV/other)."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#installing-and-loading-r-packages",
    "href": "In-Class_Ex08/in-class_ex08.html#installing-and-loading-r-packages",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "2 Installing and Loading R Packages",
    "text": "2 Installing and Loading R Packages\n\n# Create/load all required packages in one shot\npacman::p_load(httr,tidyverse, sf, tmap,jsonlite, progress, spdep,GWmodel, SpatialML, rsample, Metrics, knitr, kableExtra, spatialRF, randomForestExplainer)"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#preparing-data",
    "href": "In-Class_Ex08/in-class_ex08.html#preparing-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Read the prepared modelling dataset (sf object)\nmdata &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/rawdata/mdata.rds\")\n\n\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n# Set seed to make the split reproducible\nset.seed(1234)\n\n# Split into 65% training and 35% testing using rsample\nresale_split &lt;- rsample::initial_split(mdata, prop = 6.5/10)\n\n# Extract the two partitions\ntrain_data &lt;- rsample::training(resale_split)\ntest_data  &lt;- rsample::testing(resale_split)\n\n\n# Persist the splits for reuse\nreadr::write_rds(train_data, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\nreadr::write_rds(test_data,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#computing-correlation-matrix",
    "href": "In-Class_Ex08/in-class_ex08.html#computing-correlation-matrix",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "Before loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicollinearity.\n\n# Remove geometry to compute numeric correlations only\nmdata_nogeo &lt;- mdata %&gt;% sf::st_drop_geometry()\n\n# Draw an upper-triangle correlation matrix with numbers\ncorrplot::corrplot(\n  cor(mdata_nogeo[, 2:17]), # adjust columns to our numeric predictors\n  diag   = FALSE,           # do not draw the diagonal\n  order  = \"AOE\",           # sort for visual clarity\n  tl.pos = \"td\",            # variable labels on top diagonal\n  tl.cex = 0.5,             # smaller labels\n  method = \"number\",        # show correlation values\n  type   = \"upper\"          # only the upper triangle\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf all absolute correlations are below ~0.8, severe multicollinearity is unlikely."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#retrieving-the-stored-data",
    "href": "In-Class_Ex08/in-class_ex08.html#retrieving-the-stored-data",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Reload saved splits when resuming work\ntrain_data &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/train_data.rds\")\ntest_data  &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data.rds\")"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#building-a-non-spatial-multiple-linear-regression",
    "href": "In-Class_Ex08/in-class_ex08.html#building-a-non-spatial-multiple-linear-regression",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "# Fit a global (non-spatial) linear regression as a baseline\nprice_mlr &lt;- lm(\n  resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data\n)\n\n# Inspect coefficients and diagnostics\nsummary(price_mlr)\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Save the fitted model\nreadr::write_rds(price_mlr, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/price_mlr.rds\")"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#gwr-predictive-method",
    "href": "In-Class_Ex08/in-class_ex08.html#gwr-predictive-method",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "In this section, we will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\n# Determine optimal adaptive bandwidth (in neighbors) using CV\nbw_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,   # training sf\n  approach = \"CV\",         # cross-validation\n  kernel   = \"gaussian\",   # Gaussian kernel\n  adaptive = TRUE,         # adaptive neighbor count\n  longlat  = FALSE         # data are in projected meters\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.966278e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871966e+12 \nAdaptive bandwidth: 41 CV score: 6.793327e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \n\n# Persist the chosen bandwidth (e.g., result may be 40 neighbors)\nreadr::write_rds(bw_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\n\n\n\nFirst, let us call the save bandwidth by using the code chunk below.\n\n# Reload bandwidth when needed\nbw_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n# Calibrate GWR using the selected adaptive bandwidth\ngwr_adaptive &lt;- GWmodel::gwr.basic(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = train_data,    # training sf\n  bw       = bw_adaptive,   # adaptive neighbors (numeric)\n  kernel   = \"gaussian\",    # kernel shape\n  adaptive = TRUE,          # use adaptive bandwidth\n  longlat  = FALSE          # projected coordinates\n)\n\n# Save the fitted GWR object\nreadr::write_rds(gwr_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n\n\n\nThe code chunk below will be used to retrieve the save gwr model object.\n\n# Reload GWR model when resuming work\ngwr_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\n# Printing the object shows the model summary header and timings\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-10-25 13:55:09.364338 \n   Call:\n   GWmodel::gwr.basic(formula = resale_price ~ floor_area_sqm + \n    storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n    PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + \n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n    WITHIN_1KM_PRISCH, data = train_data, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2594e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2291e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1660e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1881e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2489e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5224e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0262e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.8 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209 \n   Residual sum of squares: 4.829177e+12 \n   R-square value:  0.9676571 \n   Adjusted R-square value:  0.9611535 \n\n   ***********************************************************************\n   Program stops at: 2025-10-25 13:56:06.496157 \n\n\n\n\n\n\n# Some workflows also derive a CV bandwidth using the test sf\ngwr_bw_test_adaptive &lt;- GWmodel::bw.gwr(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data     = test_data,\n  approach = \"CV\",\n  kernel   = \"gaussian\",\n  adaptive = TRUE,\n  longlat  = FALSE\n)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3447 CV score: 1.902155e+13 \nAdaptive bandwidth: 2138 CV score: 1.752645e+13 \nAdaptive bandwidth: 1328 CV score: 1.556299e+13 \nAdaptive bandwidth: 828 CV score: 1.357498e+13 \nAdaptive bandwidth: 518 CV score: 1.030751e+13 \nAdaptive bandwidth: 327 CV score: 8.348364e+12 \nAdaptive bandwidth: 208 CV score: 6.860544e+12 \nAdaptive bandwidth: 135 CV score: 5.969504e+12 \nAdaptive bandwidth: 89 CV score: 5.242221e+12 \nAdaptive bandwidth: 62 CV score: 4.742767e+12 \nAdaptive bandwidth: 43 CV score: 4.357839e+12 \nAdaptive bandwidth: 34 CV score: 4.125848e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \nAdaptive bandwidth: 23 CV score: 4.236349e+13 \nAdaptive bandwidth: 30 CV score: 4.074906e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \n\n# Save the test-set bandwidth too (for reference)\nreadr::write_rds(gwr_bw_test_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_bw_test_adaptive.rds\")\n\n\n\n\n\n# gwr_pred &lt;- gwr.predict(\n#   formula = resale_price ~ floor_area_sqm + \n#     storey_order + remaining_lease_mths + \n#     PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n#     PROX_MRT + PROX_PARK + PROX_MALL + \n#     PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#     WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n#     WITHIN_1KM_PRISCH, \n#   data=train_data, \n#   predictdata = test_data, \n#   bw=40, \n#   kernel = 'gaussian', \n#   adaptive=TRUE, \n#   longlat = FALSE)\n# \n# # Save predictions (list-like object)\n# readr::write_rds(gwr_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwr_pred.rds\")\n\n\n\n\nIn this section, we will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\n\n\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\n# Extract XY matrices for convenience (sf → numeric matrix with X,Y)\ncoords       &lt;- sf::st_coordinates(mdata)\ncoords_train &lt;- sf::st_coordinates(train_data)\ncoords_test  &lt;- sf::st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future used.\n\n# Save coordinates used later by GW-RF\nreadr::write_rds(coords_train, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_train.rds\")\nreadr::write_rds(coords_test,  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/coords_test.rds\")\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\n# Random Forest from ranger expects a data.frame without geometry\ntrain_data_nogeom &lt;- train_data %&gt;% sf::st_drop_geometry()\ntest_data_nogeom  &lt;- test_data  %&gt;% sf::st_drop_geometry()\n\n\n\n\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Fit a global RF as another baseline model\nrf &lt;- ranger::ranger(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  data = train_data_nogeom\n)\n\nrf \n\nRanger result\n\nCall:\n ranger::ranger(formula = resale_price ~ floor_area_sqm + storey_order +      remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +      PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n# Save the RF model\nreadr::write_rds(rf, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/rf.rds\")"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#calibrating-geographically-weighted-random-forest-gw-rf",
    "href": "In-Class_Ex08/in-class_ex08.html#calibrating-geographically-weighted-random-forest-gw-rf",
    "title": "Hands-on Ex08",
    "section": "",
    "text": "In this section, we will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\n\n\n# Reuse coordinates and no-geometry training input\nset.seed(1234)\n\ngwRF_adaptive &lt;- SpatialML::grf(\n  formula = resale_price ~ floor_area_sqm +\n    storey_order + remaining_lease_mths +\n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n    PROX_MRT + PROX_PARK + PROX_MALL +\n    PROX_SUPERMARKET +\n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE +\n    WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n  dframe = train_data_nogeom, # data.frame without geometry\n  bw     = 40,                # neighbor size (consistent with earlier choice)\n  kernel = \"adaptive\",        # adaptive kernel for local forests\n  coords = coords_train       # matrix of X,Y for training rows\n)\n\n\nNumber of Observations: 10335\n\n\nNumber of Independent Variables: 14\n\n\nKernel: Adaptive\nNeightbours: 40\n\n\n\n--------------- Global ML Model Summary ---------------\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       697593819 \nR squared (OOB):                  0.9517189 \n\n\n\nImportance:\n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.413197e+12             1.538950e+13             2.890637e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.310066e+13             7.285092e+12             5.568548e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.369745e+12             4.894344e+12             4.223286e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.793853e+12             1.018586e+12             1.710374e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.589501e+12             6.794634e+12 \n\n\n\nMean Square Error (Not OOB): 173951416.766\n\n\nR-squared (Not OOB) %: 98.796\n\n\nAIC (Not OOB): 196129.252\n\n\nAICc (Not OOB): 196129.299\n\n\n\n--------------- Local Model Summary ---------------\n\n\n\nResiduals OOB:\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-207638.9  -13044.4     517.7     668.7   15329.3  380000.0 \n\n\n\nResiduals Predicted (Not OOB):\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-80918.36  -3710.39     80.88     60.19   4119.23  67858.13 \n\n\n\nLocal Variable Importance:\n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 244740042006 10095828392 21812835540\nstorey_order             185976628 195162288767 10691049214 16507251138\nremaining_lease_mths     364347875 420068216877 18525480676 40200134557\nPROX_CBD                         0 261367685029  7551870678 20050530584\nPROX_ELDERLYCARE                 0 260937815286  6797406011 17221278502\nPROX_HAWKER                      0 232903486867  6917875149 17526152433\nPROX_MRT                         0 192977091714  6219493848 14339970325\nPROX_PARK                        0 270208600970  5678244629 13059236132\nPROX_MALL                        0 290212372886  6903172498 18141558541\nPROX_SUPERMARKET                 0 283979765533  6391052599 16785880266\nWITHIN_350M_KINDERGARTEN         0 124340534149  1694367905  7648420853\nWITHIN_350M_CHILDCARE            0 194759670980  3113750592 10738676578\nWITHIN_350M_BUS                  0 132614671785  3014600716  8056548827\nWITHIN_1KM_PRISCH                0 132884456893  1116898699  5401808933\n\n\n\nMean squared error (OOB): 932642623.291\n\n\nR-squared (OOB) %: 93.544\n\n\nAIC (OOB): 213484.26\n\n\nAICc (OOB): 213484.306\n\n\nMean squared error Predicted (Not OOB): 80652745.326\n\n\nR-squared Predicted (Not OOB) %: 99.442\n\n\nAIC Predicted (Not OOB): 188185.531\n\n\nAICc Predicted (Not OOB): 188185.578\n\n\n\nCalculation time (in seconds): 11.9505\n\n# Persist fitted GW-RF\nreadr::write_rds(gwRF_adaptive, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a computational intensity and time consuming process. It is wiser to save the output as an rds file for future used without having to re-run the process again.\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n# (Later) reload if needed\ngwRF_adaptive &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/gwRF_adaptive.rds\")\n\n\n\n\n\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n# Combine the test attributes with their coordinates for prediction\ntest_data_nogeom &lt;- cbind(test_data, coords_test) %&gt;%\n  sf::st_drop_geometry()     # drop geometry to keep plain columns\n\n\n\n\n\n# Generate local predictions at test locations\ngwRF_pred &lt;- SpatialML::predict.grf(\n  gwRF_adaptive,     # trained GW-RF model\n  test_data_nogeom,  # test attributes + X,Y columns\n  x.var.name = \"X\",  # column name for X coordinate\n  y.var.name = \"Y\",  # column name for Y coordinate\n  local.w    = 1,    # weight for local component\n  global.w   = 0     # set to 0 to use purely local predictions\n)\n\n# Save the prediction vector\nGRF_pred &lt;- readr::write_rds(gwRF_pred, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\n\n\n\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\n# Reload predictions when needed and coerce to data.frame\nGRF_pred     &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/GRF_pred.rds\")\nGRF_pred_df  &lt;- as.data.frame(GRF_pred)  # single column with predicted values\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\n# Attach predictions to the test data and retain only needed columns\ntest_data_p &lt;- cbind(test_data, GRF_pred_df) %&gt;%\n  dplyr::select(resale_price, GRF_pred)   # rename matches grf() output\n\n\n# Save the paired actual vs predicted for later evaluation/plots\nreadr::write_rds(test_data_p, \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/model/test_data_p.rds\")\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n# Compute RMSE between observed and GW-RF predicted prices\nMetrics::rmse(\n  test_data_p$resale_price,  # actual values\n  test_data_p$GRF_pred       # predicted values\n)\n\n[1] 28160.87\n\n# Example output in the reference workflow: 28160.87\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n# Scatter plot of Predicted (x) vs Actual (y)\nggplot2::ggplot(\n  data = test_data_p,                           # data with both columns\n  ggplot2::aes(x = GRF_pred, y = resale_price)  # map axes\n) +\n  ggplot2::geom_point()                         # draw points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#geocoding-for-geometric",
    "href": "In-Class_Ex08/in-class_ex08.html#geocoding-for-geometric",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "3 Geocoding for Geometric",
    "text": "3 Geocoding for Geometric\n\n3.1 Importing data\n\n# Downloading the raw data from data.gov.sg. \n# Data source: Resale flat prices based on registration date from Jan-2017 onwards\n\n# load full table once\nHDBresale_raw &lt;- read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex08/data/rawdata/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv\")\n\nRows: 218083 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): month, town, flat_type, block, street_name, storey_range, flat_mode...\ndbl (3): floor_area_sqm, lease_commence_date, resale_price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the loading data\nglimpse(HDBresale_raw)\n\nRows: 218,083\nColumns: 11\n$ month               &lt;chr&gt; \"2017-01\", \"2017-01\", \"2017-01\", \"2017-01\", \"2017-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"2 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               &lt;chr&gt; \"406\", \"108\", \"602\", \"465\", \"601\", \"150\", \"447\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 10\", \"ANG MO KIO AVE 4\", \"ANG MO K…\n$ storey_range        &lt;chr&gt; \"10 TO 12\", \"01 TO 03\", \"01 TO 03\", \"04 TO 06\", \"0…\n$ floor_area_sqm      &lt;dbl&gt; 44, 67, 67, 68, 67, 68, 68, 67, 68, 67, 68, 67, 67…\n$ flat_model          &lt;chr&gt; \"Improved\", \"New Generation\", \"New Generation\", \"N…\n$ lease_commence_date &lt;dbl&gt; 1979, 1978, 1980, 1980, 1980, 1981, 1979, 1976, 19…\n$ remaining_lease     &lt;chr&gt; \"61 years 04 months\", \"60 years 07 months\", \"62 ye…\n$ resale_price        &lt;dbl&gt; 232000, 250000, 262000, 265000, 265000, 275000, 28…\n\n\n\n\n3.2 Reducing and filtering the dataset\n\n# find the most recent month that has 4 ROOM\nmonth_choice &lt;- HDBresale_raw %&gt;%\n  filter(flat_type == \"4 ROOM\") %&gt;%\n  count(month, name = \"n\") %&gt;%\n  filter(n &gt; 0) %&gt;%\n  arrange(desc(month)) %&gt;%\n  slice(1) %&gt;%\n  pull(month)\n\n#Inspect the output\nmonth_choice\n\n[1] \"2025-10\"\n\n\n\n# filter to that month\nHDBresale &lt;- HDBresale_raw %&gt;%\n  filter(flat_type == \"4 ROOM\", month == month_choice)\n\nnrow(HDBresale)   # should be greater than zero\n\n[1] 458\n\n\n\n\n3.3 Geodata pre-processing and cleaning\n\n3.3.1 Address normalisation matters\n\n# street name fix\nHDBresale$street_name &lt;- gsub(\"ST\\\\.\", \"SAINT\", HDBresale$street_name)\n\n\n\n\n\n\n\nNote\n\n\n\nThe gsub() function performs a global substitution (find and replace) in text.\n\n\n\n\n3.3.2 Function to convert address to coordinates (LATITUDE and LONGITUDEY)\nBelow is the code chunk used to build a function that will convert address to coordinates\n\nThe function sends a live API request to OneMap Singapore to obtain geographic coordinates (latitude and longitude) for each given HDB address.\n\nExplicit UTF-8 encoding is included to eliminate the “No encoding supplied” warning and ensure correct text handling for special characters.\n\nAlways returns a tibble with columns LATITUDE and LONGITUDE, even when no match is found, to prevent pipeline breaks during unnesting.\n\nOnly retrieves the first match from the API (pageNum = “1”), which increases speed but may ignore alternative results for ambiguous addresses.\n\nRequires street name standardisation (e.g., “ST.” changed to “SAINT”) before calling the function to improve matching accuracy.\n\nCoordinates returned by OneMap are based on WGS84 (EPSG 4326), so conversion to SVY21 (EPSG 3414) is needed for metric-based distance analysis.\n\nEach function call makes a separate HTTP request; for large datasets, use progress tracking, pauses (Sys.sleep), and checkpoint saving to avoid rate limits.\n\nWrapping the function with tryCatch or purrr::safely prevents process interruption if a request fails or the API returns an unexpected response.\n\nA test call such as geocode(“101”,“JURONG EAST STREET 13”) is recommended before batch processing to confirm valid numeric coordinates are returned.\n\nFor reproducibility and efficiency, store results (block, street, LATITUDE, LONGITUDE) in a CSV file to reuse in future runs without re-querying the API.\n\n\n# professor geocode\n# Reference: https://www.onemap.gov.sg/apidocs/coordinate\n\n\ngeocode &lt;- function(block, streetname) {                          # define a function that takes block and street name and returns coordinates\n  base_url &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"   # store the OneMap search endpoint as a constant for reuse\n  address  &lt;- paste(block, streetname, sep = \" \")                 # build a single query string such as 118 ANG MO KIO AVENUE 4\n  query &lt;- list(                                                  # create a named list of query parameters for the HTTP request\n    searchVal     = address,                                      # the address string to search\n    returnGeom    = \"Y\",                                          # ask the service to return geometry fields\n    getAddrDetails= \"N\",                                          # do not request extra address fields to keep reply small\n    pageNum       = \"1\"                                           # take the first page only for a single best match\n  )                                                               # end of query parameter list\n  res     &lt;- httr::GET(base_url, query = query)                   # send a GET request to OneMap with the query parameters\n  restext &lt;- httr::content(res, as = \"text\", encoding = \"UTF-8\")  # read the response body as text with UTF8 to avoid encoding warnings\n  out     &lt;- jsonlite::fromJSON(restext)                          # parse the JSON text into an R list or data frame\n\n  if (length(out$results) == 0) {                                 # if the service returns no match for this address\n    tibble(LATITUDE = NA_real_, LONGITUDE = NA_real_)             # return a tibble with missing numeric coordinates\n  } else {                                                        # otherwise when there is at least one match\n    tibble(                                                              \n      LATITUDE  = as.numeric(out$results$LATITUDE[1]),            # take the first result latitude and coerce to numeric\n      LONGITUDE = as.numeric(out$results$LONGITUDE[1])            # take the first result longitude and coerce to numeric\n    )                                                             # end tibble creation\n  }                                                               # end branch\n}                                                                 # end function\n\n\n\n3.3.3 Perform minimal unit/smoke test\n\n# Test with a single known HDB address\ngeocode(\"101\", \"JURONG EAST STREET 13\")\n\n# A tibble: 1 × 2\n  LATITUDE LONGITUDE\n     &lt;dbl&gt;     &lt;dbl&gt;\n1     1.34      104.\n\n\n\n\n\n3.4 Merge the LATITUDE and LONGITUDE to dataset\n\n# create LATITUDE and LONGITUDE and merge into the table\nHDBresale &lt;- HDBresale %&gt;%\n  mutate(geo = purrr::map2(block, street_name, geocode)) %&gt;%\n  tidyr::unnest(geo)\n\n### \nglimpse(HDBresale)   # now includes LATITUDE and LONGITUDE\n\nRows: 458\nColumns: 13\n$ month               &lt;chr&gt; \"2025-10\", \"2025-10\", \"2025-10\", \"2025-10\", \"2025-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", …\n$ block               &lt;chr&gt; \"336\", \"415\", \"438\", \"301\", \"302\", \"327\", \"212\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 1\", \"ANG MO KIO AVE 10\", \"ANG MO K…\n$ storey_range        &lt;chr&gt; \"07 TO 09\", \"07 TO 09\", \"01 TO 03\", \"10 TO 12\", \"1…\n$ floor_area_sqm      &lt;dbl&gt; 91, 92, 92, 98, 98, 98, 81, 92, 99, 91, 99, 91, 90…\n$ flat_model          &lt;chr&gt; \"New Generation\", \"New Generation\", \"New Generatio…\n$ lease_commence_date &lt;dbl&gt; 1982, 1979, 1979, 1978, 1978, 1977, 1977, 1978, 19…\n$ remaining_lease     &lt;chr&gt; \"55 years 04 months\", \"52 years 11 months\", \"52 ye…\n$ resale_price        &lt;dbl&gt; 570000, 562000, 465000, 638000, 580000, 600000, 54…\n$ LATITUDE            &lt;dbl&gt; 1.363594, 1.364453, 1.366971, 1.367421, 1.367090, …\n$ LONGITUDE           &lt;dbl&gt; 103.8518, 103.8537, 103.8539, 103.8459, 103.8457, …"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#supplement-to-hands-on-exercise-8",
    "href": "In-Class_Ex08/in-class_ex08.html#supplement-to-hands-on-exercise-8",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "3 Supplement to Hands-on Exercise 8",
    "text": "3 Supplement to Hands-on Exercise 8\n\n3.1 Data import\n\n# Read the prepared modelling dataset (sf object)\nmdata &lt;- readr::read_rds(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex08/data/rawdata/mdata.rds\")\n\n\n\n3.2 Data sampling\nCalibrating predictive models are computational intensive, especially random forest method is used. For quick prototyping, a 10% sample will be selected at random from the data by using the code chunk below.\n\nset.seed(1234)\nHDB_sample &lt;- mdata %&gt;%\n  sample_n(1500)\n\n\n\n3.3 Checking of overlapping point\n\n\n\n\n\n\nWarning\n\n\n\nWhen using GWmodel to calibrate explanatory or predictive models, it is very important to ensure that there are no overlapping point features\n\n\nThe code chunk below is used to check if there are overlapping point features.\n\noverlapping_points &lt;- HDB_sample %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)\n\n\n\n3.4 Spatial jittler\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5 meters to avoid overlapping point features.\n\nHDB_sample &lt;- HDB_sample %&gt;%\n  st_jitter(amount = 5)\n\n\n\n3.5 Data Sampling\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\nset.seed(1234)\nresale_split &lt;- initial_split(HDB_sample, \n                              prop = 6.67/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\n\n\n3.6 Multicollinearity check\nIn order to avoid multicollineariy. In the code chunk below, ggcorrmat() of ggstatsplot is used to plot a correlation matrix to check if there are pairs of highly correlated independent variables.\n\nmdata_nogeo &lt;- mdata %&gt;%\n  st_drop_geometry()\nggstatsplot::ggcorrmat(mdata_nogeo[, 2:17])\n\n\n\n\n\n\n\n\n\n\n3.7 Building a non-spatial multiple linear regression\n\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nolsrr::ols_regress(price_mlr)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.862       RMSE                    60813.316 \nR-Squared                   0.742       MSE                3698259426.779 \nAdj. R-Squared              0.739       Coef. Var                  14.255 \nPred R-Squared              0.734       AIC                     24901.005 \nMAE                     45987.256       SBC                     24979.529 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                      \n-------------------------------------------------------------------------------\n                    Sum of                                                     \n                   Squares         DF         Mean Square       F         Sig. \n-------------------------------------------------------------------------------\nRegression    1.065708e+13         14    761220078101.236    202.745    0.0000 \nResidual      3.698259e+12        985      3754578098.252                      \nTotal         1.435534e+13        999                                          \n-------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                 \n------------------------------------------------------------------------------------------------------------------\n                   model          Beta    Std. Error    Std. Beta       t        Sig          lower         upper \n------------------------------------------------------------------------------------------------------------------\n             (Intercept)    115703.696     34303.409                   3.373    0.001     48387.533    183019.860 \n          floor_area_sqm      2778.618       292.262        0.165      9.507    0.000      2205.089      3352.146 \n            storey_order     12698.165      1070.950        0.211     11.857    0.000     10596.559     14799.771 \n    remaining_lease_mths       350.252        14.596        0.450     23.997    0.000       321.610       378.894 \n                PROX_CBD    -16225.588       630.092       -0.572    -25.751    0.000    -17462.065    -14989.110 \n        PROX_ELDERLYCARE    -11330.930      3220.845       -0.061     -3.518    0.000    -17651.436     -5010.423 \n             PROX_HAWKER    -19964.070      4021.046       -0.087     -4.965    0.000    -27854.872    -12073.268 \n                PROX_MRT    -39652.516      5412.288       -0.130     -7.326    0.000    -50273.456    -29031.577 \n               PROX_PARK    -15878.322      4609.199       -0.061     -3.445    0.001    -24923.300     -6833.344 \n               PROX_MALL    -15910.922      6438.111       -0.048     -2.471    0.014    -28544.911     -3276.933 \n        PROX_SUPERMARKET    -18928.514     13304.965       -0.025     -1.423    0.155    -45037.848      7180.821 \nWITHIN_350M_KINDERGARTEN      9309.735      2024.293        0.079      4.599    0.000      5337.313     13282.157 \n   WITHIN_350M_CHILDCARE     -1619.514      1180.948       -0.026     -1.371    0.171     -3936.977       697.948 \n         WITHIN_350M_BUS      -447.695       738.715       -0.011     -0.606    0.545     -1897.331      1001.940 \n       WITHIN_1KM_PRISCH    -10698.012      1543.511       -0.138     -6.931    0.000    -13726.960     -7669.065 \n------------------------------------------------------------------------------------------------------------------\n\n\n\n\n3.8 Multicollinearity check with VIF\n\nvif &lt;- performance::check_collinearity(price_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %&gt;%\n  kable_styling(font_size = 18) \n\n\nVariance Inflation Factor (VIF) Results\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nfloor_area_sqm\n1.146686\n1.085743\n1.250945\n1.070834\n0.8720785\n0.7993954\n0.9210287\n\n\nstorey_order\n1.206020\n1.135720\n1.312734\n1.098189\n0.8291736\n0.7617690\n0.8804986\n\n\nremaining_lease_mths\n1.343645\n1.254833\n1.463410\n1.159157\n0.7442440\n0.6833358\n0.7969186\n\n\nPROX_CBD\n1.887898\n1.733977\n2.074096\n1.374008\n0.5296898\n0.4821378\n0.5767088\n\n\nPROX_ELDERLYCARE\n1.140418\n1.080572\n1.244716\n1.067904\n0.8768712\n0.8033960\n0.9254357\n\n\nPROX_HAWKER\n1.183865\n1.116887\n1.289223\n1.088056\n0.8446907\n0.7756609\n0.8953457\n\n\nPROX_MRT\n1.211390\n1.140307\n1.318485\n1.100632\n0.8254980\n0.7584464\n0.8769566\n\n\nPROX_PARK\n1.186122\n1.118797\n1.291599\n1.089092\n0.8430839\n0.7742340\n0.8938169\n\n\nPROX_MALL\n1.435504\n1.335252\n1.565736\n1.198125\n0.6966193\n0.6386771\n0.7489224\n\n\nPROX_SUPERMARKET\n1.226727\n1.153448\n1.335000\n1.107577\n0.8151773\n0.7490638\n0.8669656\n\n\nWITHIN_350M_KINDERGARTEN\n1.123989\n1.067172\n1.228865\n1.060183\n0.8896886\n0.8137594\n0.9370564\n\n\nWITHIN_350M_CHILDCARE\n1.387119\n1.292841\n1.511748\n1.177760\n0.7209189\n0.6614860\n0.7734902\n\n\nWITHIN_350M_BUS\n1.193498\n1.125056\n1.299398\n1.092473\n0.8378731\n0.7695869\n0.8888447\n\n\nWITHIN_1KM_PRISCH\n1.508943\n1.399770\n1.647930\n1.228390\n0.6627154\n0.6068219\n0.7144029\n\n\n\n\n\n\nplot(vif) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n3.9 Predictive Modelling with gwr\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nAdaptive bandwidth: 625 CV score: 3.459032e+12 \nAdaptive bandwidth: 394 CV score: 3.231786e+12 \nAdaptive bandwidth: 250 CV score: 2.914736e+12 \nAdaptive bandwidth: 162 CV score: 2.610897e+12 \nAdaptive bandwidth: 107 CV score: 2.240188e+12 \nAdaptive bandwidth: 73 CV score: 1.971641e+12 \nAdaptive bandwidth: 52 CV score: 1.797271e+12 \nAdaptive bandwidth: 39 CV score: 1.659472e+12 \nAdaptive bandwidth: 31 CV score: 1.573963e+12 \nAdaptive bandwidth: 26 CV score: 1.550147e+12 \nAdaptive bandwidth: 23 CV score: 1.542544e+12 \nAdaptive bandwidth: 21 CV score: 1.518885e+12 \nAdaptive bandwidth: 19 CV score: 1.515965e+12 \nAdaptive bandwidth: 19 CV score: 1.515965e+12 \n\n\n\nbw_adaptive\n\n[1] 19\n\n\n\n\n3.10 Model calibration\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\n\n\n3.11 Predictive Modelling with MLR\n\n3.11.1 Predictive Modelling with MLR\n\n3.11.1.1 Test data bw\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\n\n\n3.11.1.2 Predicting\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\n\n\n\n\n3.12 Predictive Modelling: RF method\n\n3.12.1 Data Preparation\nFirstly, code chunk below is used to extract the coordinates of training and test data sets\n\ncoords &lt;- st_coordinates(HDB_sample)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\n\n3.12.2 Calibrating RF model\n\n# set.seed(1234)\n# rf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n#                remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n#                PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n#                PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#                WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n#                WITHIN_1KM_PRISCH,\n#              data=train_nogeom)\n\n\n\n3.12.3 Model output\n\nrf\n\nfunction (data = NULL, dependent.variable.name = NULL, predictor.variable.names = NULL, \n    distance.matrix = NULL, distance.thresholds = NULL, xy = NULL, \n    ranger.arguments = NULL, scaled.importance = FALSE, seed = 1, \n    verbose = TRUE, n.cores = parallel::detectCores() - 1, cluster = NULL) \n{\n    if (!is.null(data) & !is.null(ranger.arguments)) {\n        ranger.arguments$data &lt;- NULL\n        ranger.arguments$dependent.variable.name &lt;- NULL\n        ranger.arguments$predictor.variable.names &lt;- NULL\n    }\n    num.trees &lt;- 500\n    mtry &lt;- NULL\n    mtry &lt;- NULL\n    importance &lt;- \"permutation\"\n    write.forest &lt;- TRUE\n    probability &lt;- FALSE\n    min.node.size &lt;- NULL\n    max.depth &lt;- NULL\n    replace &lt;- TRUE\n    sample.fraction &lt;- ifelse(replace, 1, 0.632)\n    case.weights &lt;- NULL\n    class.weights &lt;- NULL\n    splitrule &lt;- NULL\n    num.random.splits &lt;- 1\n    alpha &lt;- 0.5\n    minprop &lt;- 0.1\n    split.select.weights &lt;- NULL\n    always.split.variables &lt;- NULL\n    respect.unordered.factors &lt;- NULL\n    scale.permutation.importance &lt;- TRUE\n    local.importance &lt;- TRUE\n    regularization.factor &lt;- 1\n    regularization.usedepth &lt;- FALSE\n    keep.inbag &lt;- FALSE\n    inbag &lt;- NULL\n    holdout &lt;- FALSE\n    quantreg &lt;- FALSE\n    oob.error &lt;- TRUE\n    num.threads &lt;- n.cores\n    save.memory &lt;- FALSE\n    classification &lt;- NULL\n    if (!is.null(ranger.arguments)) {\n        list2env(ranger.arguments, envir = environment())\n    }\n    if (inherits(data, \"tbl_df\") | inherits(data, \"tbl\")) {\n        data &lt;- as.data.frame(data)\n    }\n    if (inherits(xy, \"tbl_df\") | inherits(xy, \"tbl\")) {\n        xy &lt;- as.data.frame(xy)\n    }\n    if (inherits(predictor.variable.names, \"variable_selection\")) {\n        predictor.variable.names &lt;- predictor.variable.names$selected.variables\n    }\n    else {\n        if (sum(predictor.variable.names %in% colnames(data)) &lt; \n            length(predictor.variable.names)) {\n            stop(paste0(\"The predictor.variable.names \", paste0(predictor.variable.names[!(predictor.variable.names %in% \n                colnames(data))], collapse = \", \"), \" are missing from 'data'\"))\n        }\n    }\n    if (!(dependent.variable.name %in% colnames(data))) {\n        stop(paste0(\"The dependent.variable.name \", dependent.variable.name, \n            \" is not a column of 'data'.\"))\n    }\n    data &lt;- data[, c(dependent.variable.name, predictor.variable.names)]\n    if (!is.null(seed)) {\n        set.seed(seed)\n    }\n    if (scaled.importance == TRUE) {\n        data.scaled &lt;- as.data.frame(scale(x = data))\n        if (sum(apply(data.scaled, 2, is.nan)) &gt; 0 | sum(apply(data.scaled, \n            2, is.infinite)) &gt; 0) {\n            scaled.importance &lt;- FALSE\n            warning(\"The training data yields NaN or Inf when scaled, setting scaled.importance to FALSE.\")\n        }\n    }\n    is.binary &lt;- is_binary(data = data, dependent.variable.name = dependent.variable.name)\n    if (is.binary == TRUE & is.null(case.weights)) {\n        case.weights &lt;- case_weights(data = data, dependent.variable.name = dependent.variable.name)\n    }\n    m &lt;- ranger::ranger(data = data, dependent.variable.name = dependent.variable.name, \n        num.trees = num.trees, mtry = mtry, importance = importance, \n        write.forest = write.forest, probability = probability, \n        min.node.size = min.node.size, max.depth = max.depth, \n        replace = replace, sample.fraction = sample.fraction, \n        case.weights = case.weights, class.weights = class.weights, \n        splitrule = splitrule, num.random.splits = num.random.splits, \n        alpha = alpha, minprop = minprop, split.select.weights = split.select.weights, \n        always.split.variables = always.split.variables, respect.unordered.factors = respect.unordered.factors, \n        scale.permutation.importance = scale.permutation.importance, \n        local.importance = local.importance, regularization.factor = regularization.factor, \n        regularization.usedepth = regularization.usedepth, keep.inbag = keep.inbag, \n        inbag = inbag, holdout = holdout, quantreg = quantreg, \n        oob.error = oob.error, num.threads = num.threads, save.memory = save.memory, \n        verbose = verbose, seed = seed, classification = classification)\n    variable.importance.global &lt;- m$variable.importance\n    variable.importance.local &lt;- m$variable.importance.local\n    if (scaled.importance == TRUE) {\n        m.scaled &lt;- ranger::ranger(data = data.scaled, dependent.variable.name = dependent.variable.name, \n            num.trees = num.trees, mtry = mtry, importance = importance, \n            write.forest = write.forest, probability = probability, \n            min.node.size = min.node.size, max.depth = max.depth, \n            replace = replace, sample.fraction = sample.fraction, \n            case.weights = case.weights, class.weights = class.weights, \n            splitrule = splitrule, num.random.splits = num.random.splits, \n            alpha = alpha, minprop = minprop, split.select.weights = split.select.weights, \n            always.split.variables = always.split.variables, \n            respect.unordered.factors = respect.unordered.factors, \n            scale.permutation.importance = FALSE, local.importance = local.importance, \n            regularization.factor = regularization.factor, regularization.usedepth = regularization.usedepth, \n            keep.inbag = keep.inbag, inbag = inbag, holdout = holdout, \n            quantreg = quantreg, oob.error = oob.error, num.threads = num.threads, \n            save.memory = save.memory, verbose = verbose, seed = seed, \n            classification = classification)\n        variable.importance.global &lt;- m.scaled$variable.importance\n        variable.importance.local &lt;- m.scaled$variable.importance.local\n    }\n    m$ranger.arguments &lt;- list(data = data, dependent.variable.name = dependent.variable.name, \n        predictor.variable.names = predictor.variable.names, \n        distance.matrix = distance.matrix, distance.thresholds = distance.thresholds, \n        xy = xy, num.trees = num.trees, mtry = mtry, importance = importance, \n        scaled.importance = scaled.importance, write.forest = write.forest, \n        probability = probability, min.node.size = min.node.size, \n        max.depth = max.depth, replace = replace, sample.fraction = sample.fraction, \n        case.weights = case.weights, class.weights = class.weights, \n        splitrule = splitrule, num.random.splits = num.random.splits, \n        alpha = alpha, minprop = minprop, split.select.weights = split.select.weights, \n        always.split.variables = always.split.variables, respect.unordered.factors = respect.unordered.factors, \n        scale.permutation.importance = scale.permutation.importance, \n        local.importance = local.importance, regularization.factor = regularization.factor, \n        regularization.usedepth = regularization.usedepth, keep.inbag = keep.inbag, \n        inbag = inbag, holdout = holdout, quantreg = quantreg, \n        oob.error = oob.error, num.threads = num.threads, save.memory = save.memory, \n        seed = seed, classification = classification)\n    if (importance == \"permutation\") {\n        m$importance &lt;- list()\n        variable.importance.global.sign &lt;- variable.importance.global\n        variable.importance.global.sign[variable.importance.global.sign &gt;= \n            0] &lt;- 1\n        variable.importance.global.sign[variable.importance.global.sign &lt; \n            0] &lt;- -1\n        variable.importance.global &lt;- sqrt(abs(variable.importance.global)) * \n            variable.importance.global.sign\n        m$importance$per.variable &lt;- data.frame(variable = names(variable.importance.global), \n            importance = variable.importance.global) %&gt;% tibble::remove_rownames() %&gt;% \n            dplyr::arrange(dplyr::desc(importance)) %&gt;% dplyr::mutate(importance = round(importance, \n            3)) %&gt;% as.data.frame()\n        m$importance$per.variable.plot &lt;- plot_importance(m$importance$per.variable, \n            verbose = verbose)\n        variable.importance.local.sign &lt;- variable.importance.local\n        variable.importance.local.sign[variable.importance.local.sign &gt;= \n            0] &lt;- 1\n        variable.importance.local.sign[variable.importance.local.sign &lt; \n            0] &lt;- -1\n        variable.importance.local &lt;- sqrt(abs(variable.importance.local)) * \n            variable.importance.local.sign\n        m$importance$local &lt;- variable.importance.local\n    }\n    predicted &lt;- stats::predict(object = m, data = data, type = \"response\")$predictions\n    m$predictions &lt;- list()\n    m$predictions$values &lt;- predicted\n    observed &lt;- data[, dependent.variable.name]\n    m$performance &lt;- list()\n    m$performance$r.squared.oob &lt;- m$r.squared\n    m$performance$r.squared &lt;- cor(observed, predicted)^2\n    m$performance$pseudo.r.squared &lt;- cor(observed, predicted)\n    m$performance$rmse.oob &lt;- sqrt(m$prediction.error)\n    m$performance$rmse &lt;- root_mean_squared_error(o = observed, \n        p = predicted, normalization = \"rmse\")\n    names(m$performance$rmse) &lt;- NULL\n    m$performance$nrmse &lt;- root_mean_squared_error(o = observed, \n        p = predicted, normalization = \"iq\")\n    names(m$performance$nrmse) &lt;- NULL\n    m$performance$auc &lt;- NA\n    m$performance$auc &lt;- auc(o = observed, p = predicted)\n    m$residuals$values &lt;- observed - predicted\n    m$residuals$stats &lt;- summary(m$residuals$values)\n    if (!is.null(distance.matrix)) {\n        m$residuals$autocorrelation &lt;- moran_multithreshold(x = m$residuals$values, \n            distance.matrix = distance.matrix, distance.thresholds = distance.thresholds, \n            verbose = verbose)\n    }\n    m$residuals$normality &lt;- residuals_diagnostics(residuals = m$residuals$values, \n        predictions = predicted)\n    m$residuals$diagnostics &lt;- plot_residuals_diagnostics(m, \n        verbose = verbose)\n    if (!is.null(cluster)) {\n        m$cluster &lt;- cluster\n    }\n    class(m) &lt;- c(\"rf\", \"ranger\")\n    if (verbose == TRUE) {\n        print(m)\n    }\n    m\n}\n&lt;bytecode: 0x12735d858&gt;\n&lt;environment: namespace:spatialRF&gt;\n\n\n\n\n\n3.13 Predictive Modelling: SpatialML method\n\n3.13.1 Determining bandwidth\n\n# set.seed(1234)\n# gwRF_bw &lt;- grf.bw(formula = resale_price ~ floor_area_sqm + \n#                        storey_order + remaining_lease_mths + \n#                        PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n#                        PROX_MRT + PROX_PARK + PROX_MALL + \n#                        PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#                        WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n#                        WITHIN_1KM_PRISCH,\n#                      dataset=train_data, \n#                      kernel=\"adaptive\",\n#                      coords=coords_train)\n\n\n\n3.13.2 Calibrating with grf\n\n# set.seed(1234)\n# gwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + \n#                        storey_order + remaining_lease_mths + \n#                        PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n#                        PROX_MRT + PROX_PARK + PROX_MALL + \n#                        PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n#                        WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n#                        WITHIN_1KM_PRISCH,\n#                      dframe=train_data_nogeom, \n#                      bw=55,\n#                      kernel=\"adaptive\",\n#                      coords=coords_train)\n\n\n\n\n3.14 Predicting by using the test data\n\n3.14.1 Preparing the test data\n\n# test_data_nogeom &lt;- cbind(\n#   test_data, coords_test) %&gt;%\n#   st_drop_geometry()\n\n\n\n3.14.2 Predicting with the test data\nIn the code chunk below, predict.grf() of spatialML for predicting re-sale prices in the test data set (i.e. test_data_nogeom)\n\n# \n# gwRF_pred &lt;- predict.grf(gwRF_adaptive, \n#                            test_data_nogeom, \n#                            x.var.name=\"X\",\n#                            y.var.name=\"Y\", \n#                            local.w=1,\n#                            global.w=0)\n\n\n\n3.14.3 Creating DF\nNext, the code chunk below is used to convert the output from predict.grf() into a data.frame.\n\n# GRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\nThen, cbind() is used to append fields in GRF_pred_df data.frame onto test_data.\n\n# test_data_pred &lt;- cbind(test_data, \n#                         GRF_pred_df)\n\n\n\n\n3.15 Visualising the predicted values\n\n# ggplot(data = test_data_pred,\n#        aes(x = GRF_pred,\n#            y = resale_price)) +\n#   geom_point()"
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#proximity-analysis",
    "href": "In-Class_Ex08/in-class_ex08.html#proximity-analysis",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "4 Proximity Analysis",
    "text": "4 Proximity Analysis\nIn this section, you will learn how to count the number of geographic entities located within a defined distance from each HDB property.\nFor the purpose of this exercise, we are interested to count the number of preschools located with 350m of each resale HDB unit.\n\n4.1 Convert to an sf object\nBefore performing proximity analysis, it is important to ensure that:\n\nboth input data sets must be in sf objects, and\nthey must be in similar projected coordinates systems.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the code chunk below,\n\nst_as_sf() is used to convert HDBresale tibble data.frame to sf object by using values from LONGITUDE and LATITUDE columns to form the geometry features.\nst_transform() is then used to transform the sf object into svy21 projected coordinates system.\nthe output sf object is called HDBresale_sf. It is in sf data.frame format.\n\n\n\n\n# Convert to sf object\nHDBresale_sf &lt;- HDBresale %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n# Inspect and verify the  \nglimpse(HDBresale_sf)\n\nRows: 458\nColumns: 12\n$ month               &lt;chr&gt; \"2025-10\", \"2025-10\", \"2025-10\", \"2025-10\", \"2025-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", …\n$ block               &lt;chr&gt; \"336\", \"415\", \"438\", \"301\", \"302\", \"327\", \"212\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 1\", \"ANG MO KIO AVE 10\", \"ANG MO K…\n$ storey_range        &lt;chr&gt; \"07 TO 09\", \"07 TO 09\", \"01 TO 03\", \"10 TO 12\", \"1…\n$ floor_area_sqm      &lt;dbl&gt; 91, 92, 92, 98, 98, 98, 81, 92, 99, 91, 99, 91, 90…\n$ flat_model          &lt;chr&gt; \"New Generation\", \"New Generation\", \"New Generatio…\n$ lease_commence_date &lt;dbl&gt; 1982, 1979, 1979, 1978, 1978, 1977, 1977, 1978, 19…\n$ remaining_lease     &lt;chr&gt; \"55 years 04 months\", \"52 years 11 months\", \"52 ye…\n$ resale_price        &lt;dbl&gt; 570000, 562000, 465000, 638000, 580000, 600000, 54…\n$ geometry            &lt;POINT [m]&gt; POINT (30055.03 38404.8), POINT (30263.23 38…\n\n\n\n# Check the CRS (Coordinate Reference System)\nst_crs(HDBresale_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n4.2 Locate, download and import Preschool Location data\nNext, we will locate and download Pre-Schools Location data from Singapore’s open data portal. There are both geojson and kml version. In this exercise, the kml version will be used.\nThen, code chunk below will be used to import the preschool location data into R environment by using st_read() of readr package.\n\npreschool = st_read(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex08/data/geospatial/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs= 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex08/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nst-transform() of sf package is used to transform the preschool sf dataframe from wgs84 geographic coodinates system to svy21 projected coordinates system (i.e. EPSG: 3414)\n\n\n\n\n4.3 Counting number of preschools\nLastly, code chunk below will be used to count the number of pre-schools located within 350m of each HDB property.\n\nwithin_350m &lt;- st_is_within_distance(\n  HDBresale_sf, preschool, dist = 350)\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  mutate(WITHIN_350M_PRISCHOOL = map_int(within_350m, length))\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_is_within_distance() of sf package is used to determines whether two spatial objects (HDB property and preschool) are within 350m distance of each other. It returns a logical matrix (or a sparse list of indices if sparse = TRUE) indicating for each geometry in HDB property which geometries in preschool are within 350m distance.\nmutate() of dplyr is used to create a new field called WITHIN_350M_PRISCHOOL.\nmap_int() of purr package is used to the number of preschools found in the newly created WITHIN_350M_PRISCHOOL field.\n\n\n\n\n\n4.4 Computing shortest distance\nInstead of counting number of preschools located within 350m of each HDB resale property, code chunk below is used to compute the distance between the nearest preschool from each HDB resale property.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  mutate(\n    PROX_PRESCHOOL = as.numeric(\n      st_distance(geometry,\n                  preschool[st_nearest_feature(\n                    geometry, preschool), ], \n                  by_element = TRUE)\n      )\n    )\n\n\n\n\n\n\n\nNote\n\n\n\nThe proximity values computed are in metres because svy21 projected coordinates system is in metres."
  },
  {
    "objectID": "In-Class_Ex08/in-class_ex08.html#learning-outcomes",
    "href": "In-Class_Ex08/in-class_ex08.html#learning-outcomes",
    "title": "In-class Ex08: Take-home Exercise 3 Kick Starter",
    "section": "",
    "text": "By the end of this in-class exercise, students will master the skill of:\n\nprepare and geocode HDB resale price data for geospatial modelling; and\nperform proximity analysis to count the number of geographic entities located within a defined distance from each HDB property."
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html",
    "href": "Hands-on_Ex09/hand-on_ex09.html",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this exercise you will learn to model geographical accessibility in R using packages for simple features, mapping and accessibility computation. You will prepare the data, compute accessibility with three methods and create maps and boxplots to interpret the results.\n\n\n\nImport GIS polygon and point data into R as simple feature objects using sf functions\nImport a distance matrix and prepare it for modelling\nCompute accessibility using Hansen, KD2SFCA and Spatial Accessibility Measure methods\nVisualise results with tmap and ggplot2\n\n\n\n\n\nFour data sets are used in this exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\n\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExcept MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage.\n\n\n\n\n\nInstall and load required packages. The code uses pacman::p_load() to install on demand and load in one step.\n\n# install and load required packages in one call\npacman::p_load(tmap, SpatialAcc, sf,   # mapping, accessibility, simple features\n               ggstatsplot, reshape2,  # statistical graphics, data reshaping\n               tidyverse)  \n\n\n\n\n\n\nThree geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n# read URA subzones as simple features\nmpsz &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n                layer = \"MP14_SUBZONE_NO_SEA_PL\")   # polygon layer\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# read hexagon grid as simple features\nhexagons &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"hexagons\") # polygon grid\n\nReading layer `hexagons' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n# read eldercare locations as simple features\neldercare &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"ELDERCARE\")   # point layer\n\nReading layer `ELDERCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n# transform layers to EPSG 3414 for consistent units in metres\nmpsz &lt;- st_transform(mpsz, 3414)           # subzones to SVY21 Singapore TM\neldercare &lt;- st_transform(eldercare, 3414) # eldercare points to SVY21 Singapore TM\nhexagons &lt;- st_transform(hexagons, 3414)   # hexagon grid to SVY21 Singapore TM\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to verify the newly transformed mpsz_svy21.\n\n# verify the CRS of the subzones object\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n# keep only id and address then add constant capacity field for supply locations\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%   # retain unique id and address\n  mutate(capacity = 100)        # placeholder capacity for each facility\n\n\n# keep only id then add constant demand field for each hexagon cell\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%       # retain unique id of each grid cell\n  mutate(demand = 100)  # placeholder demand for each cell\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used.\n\n\n\n\n\n\nThe code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n# import distance matrix as a tibble\nODMatrix &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/aspatial/OD_Matrix.csv\", \n  skip = 0\n  )  # columns include origin, destination and costs\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe imported ODMatrix organised the distance matrix columnwise.\n\nOn the other hands, most of the modelling packages in R is expecting a matrix look similar to the figure below.\n\nThe rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n# build a wide distance matrix using total_cost\n# select needed columns then spread destination ids to wide columns\n# finally drop the origin_id column once it has become the row id\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%  # pick the three fields we need\n  spread(destination_id, total_cost) %&gt;%    # convert thin table to wide table by destination\n  select(c(-c('origin_id')))    # remove origin id column after spreading\n\n\n\n\n\n\n\nNote\n\n\n\nSince tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\n\n\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n# convert metres to kilometres and coerce to numeric matrix\ndistmat_km &lt;- as.matrix(distmat / 1000)   # numeric matrix required by SpatialAcc::ac\n\n\n\n\n\n\n\nNow, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\n# compute Hansen accessibility values per origin cell\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,     # vector of origin demands\n                            eldercare$capacity,  # vector of destination capacities\n                            distmat_km,          # distance matrix in kilometres\n                            #d0 = 50,            # optional cut off distance in kilometres if needed\n                            power = 2,           # distance decay parameter for Hansen\n                            family = \"Hansen\"))  # choose the Hansen method\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\n# rename the single output column to a tidy name\ncolnames(acc_Hansen) &lt;- \"accHansen\"   # set a clear column name\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\n# convert to tibble for nicer printing and dplyr friendliness\nacc_Hansen &lt;- as_tibble(acc_Hansen)   # tibble retains numeric vector with column name\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\n# attach the accessibility values to the hexagon sf object\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen) # geometry from hexagons plus new column\n\nNotice that hexagon_Hansen is a simple feature data frame and not a typical tibble data frame.\n\nWe can also run the entire sequence in one block as shown next. This alternative uses a different power value as demonstrated in the source.\n\n# alternate single block version as shown in the source\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,    # origin demand\n                            eldercare$capacity, # destination capacity\n                            distmat_km,         # distance matrix in kilometres\n                            #d0 = 50,           # optional cut off\n                            power = 0.5,        # alternate decay value used in the example\n                            family = \"Hansen\")) # Hansen method\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"             # tidy column name\nacc_Hansen &lt;- as_tibble(acc_Hansen)             # convert to tibble\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen) # join to geometry\n\n\n\n\n\n\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\n# compute map extent once for reuse\nmapex &lt;- st_bbox(hexagons) \n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\n# choropleth of Hansen accessibility with facility locations overlaid\n\ntm_shape(hexagon_Hansen, bbox = mapex) +        # set the data and map extent\n  tm_polygons(fill = \"accHansen\",               # fill by accessibility field\n              fill.scale = tm_scale_intervals(  # classify values into intervals\n                style = \"quantile\",             # equal number of cells per class\n                n = 10,                         # ten classes\n                values = \"brewer.blues\")) +     # blue palette\n  tm_shape(eldercare) +                         # overlay supply points\n  tm_dots(size = 0.3,                           # small dot size for points\n          fill = \"red\",                         # red fill for emphasis\n          col = \"black\") +                      # black outline\n  tm_title(\"Accessibility to eldercare  Hansen method\") +  # map title\n  tm_layout(frame = TRUE) +                     # draw a neatline frame\n  tm_compass(type = \"8star\", size = 2) +        # compass for orientation\n  tm_scalebar() +                               # scale bar in map units\n  tm_grid(alpha = 0.2)        \n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into haxegon_Hansen simple feature data frame by using the code chunk below.\n\n# spatial join to append region name to each hexagon\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz, join = st_intersects)  # brings REGION_N into the grid\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n# boxplot of log accessibility by planning region\n\nggplot(data = hexagon_Hansen,    # use the joined sf object\n       aes(y = log(accHansen),   # log scale for the response\n           x = REGION_N)) +      # planning region on x axis\n  geom_boxplot() +               # draw boxplots per region\n  geom_point(stat = \"summary\",   # add mean points\n             fun.y = \"mean\",     # function is mean\n             colour = \"red\",     # red mean marker\n             size = 2)           # marker size\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\n# compute KD2SFCA accessibility with a five kilometre catchment and power two\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                             eldercare$capacity,  # destination capacity\n                             distmat_km,          # distance matrix in kilometres\n                             d0 = 5,              # catchment threshold in kilometres\n                             power = 2,           # decay parameter\n                             family = \"KD2SFCA\")) # KD2SFCA method\n\n# tidy column name and bind to geometry\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"               # set column name\nacc_KD2SFCA &lt;- as_tibble(acc_KD2SFCA)               # tibble for convenience\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA) # join to hexagon geometry\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of KD2SFCA accessibility using the same extent and palette\n\ntm_shape(hexagon_KD2SFCA, bbox = mapex) +        # reuse map extent\n  tm_polygons(fill = \"accKD2SFCA\",               # fill by KD2SFCA field\n              fill.scale = tm_scale_intervals(   # classification settings\n                style = \"quantile\",              # quantile style\n                n = 10,                          # ten classes\n                values = \"brewer.blues\")) +      # palette\n  tm_shape(eldercare) +                          # overlay supply points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  KD2SFCA method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, join = st_intersects)  # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_KD2SFCA,  # KD2SFCA results\n       aes(y = accKD2SFCA,      # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean markers\n             fun.y = \"mean\",    # mean function\n             colour = \"red\",    # red mean marker\n             size = 2)          # size of marker\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\n# compute SAM accessibility with the same catchment and decay as KD2SFCA\nacc_SAM &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                         eldercare$capacity,  # destination capacity\n                         distmat_km,          # distance matrix in kilometres\n                         d0 = 5,              # catchment in kilometres\n                         power = 2,           # decay parameter\n                         family = \"SAM\"))     # SAM method\n\n# tidy column name and bind to geometry\ncolnames(acc_SAM) &lt;- \"accSAM\"                 # set column name\nacc_SAM &lt;- as_tibble(acc_SAM)                 # tibble form\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)   # join to geometry\n\n\n\n\nThe code chunk below uses a collection of mapping functions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of SAM accessibility\n\ntm_shape(hexagon_SAM, bbox = mapex) +           # reuse extent\n  tm_polygons(fill = \"accSAM\",                  # fill by SAM field\n              fill.scale = tm_scale_intervals(  # classify into intervals\n                style = \"quantile\",             # quantile style\n                n = 10,                         # ten classes\n                values = \"brewer.blues\")) +     # palette\n  tm_shape(eldercare) +                         # overlay points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  SAM method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz, join = st_intersects)   # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_SAM,      # SAM results\n       aes(y = accSAM,          # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean marker\n             fun.y = \"mean\",    # use mean\n             colour = \"red\",    # red mean\n             size = 2)          # marker size\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#introduction",
    "href": "Hands-on_Ex09/hand-on_ex09.html#introduction",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this exercise you will learn to model geographical accessibility in R using packages for simple features, mapping and accessibility computation. You will prepare the data, compute accessibility with three methods and create maps and boxplots to interpret the results.\n\n\n\nImport GIS polygon and point data into R as simple feature objects using sf functions\nImport a distance matrix and prepare it for modelling\nCompute accessibility using Hansen, KD2SFCA and Spatial Accessibility Measure methods\nVisualise results with tmap and ggplot2"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#the-data",
    "href": "Hands-on_Ex09/hand-on_ex09.html#the-data",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "Four data sets are used in this exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\n\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExcept MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage."
  },
  {
    "objectID": "Hands-on_Ex09/data/geospatial/hexagons.html",
    "href": "Hands-on_Ex09/data/geospatial/hexagons.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "Hands-on_Ex09/data/geospatial/ELDERCARE.html",
    "href": "Hands-on_Ex09/data/geospatial/ELDERCARE.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#getting-started",
    "href": "Hands-on_Ex09/hand-on_ex09.html#getting-started",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "Install and load required packages. The code uses pacman::p_load() to install on demand and load in one step.\n\n# install and load required packages in one call\npacman::p_load(tmap, SpatialAcc, sf,   # mapping, accessibility, simple features\n               ggstatsplot, reshape2,  # statistical graphics, data reshaping\n               tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex09/hand-on_ex09.html#geospatial-data-wrangling",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "Three geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n# read URA subzones as simple features\nmpsz &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n                layer = \"MP14_SUBZONE_NO_SEA_PL\")   # polygon layer\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# read hexagon grid as simple features\nhexagons &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"hexagons\") # polygon grid\n\nReading layer `hexagons' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n# read eldercare locations as simple features\neldercare &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"ELDERCARE\")   # point layer\n\nReading layer `ELDERCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n# transform layers to EPSG 3414 for consistent units in metres\nmpsz &lt;- st_transform(mpsz, 3414)           # subzones to SVY21 Singapore TM\neldercare &lt;- st_transform(eldercare, 3414) # eldercare points to SVY21 Singapore TM\nhexagons &lt;- st_transform(hexagons, 3414)   # hexagon grid to SVY21 Singapore TM\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to verify the newly transformed mpsz_svy21.\n\n# verify the CRS of the subzones object\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n# keep only id and address then add constant capacity field for supply locations\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%   # retain unique id and address\n  mutate(capacity = 100)        # placeholder capacity for each facility\n\n\n# keep only id then add constant demand field for each hexagon cell\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%       # retain unique id of each grid cell\n  mutate(demand = 100)  # placeholder demand for each cell\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used."
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#apsaital-data-handling-and-wrangling",
    "href": "Hands-on_Ex09/hand-on_ex09.html#apsaital-data-handling-and-wrangling",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "The code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n# import distance matrix as a tibble\nODMatrix &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/aspatial/OD_Matrix.csv\", \n  skip = 0\n  )  # columns include origin, destination and costs\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe imported ODMatrix organised the distance matrix columnwise.\n\nOn the other hands, most of the modelling packages in R is expecting a matrix look similar to the figure below.\n\nThe rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n# build a wide distance matrix using total_cost\n# select needed columns then spread destination ids to wide columns\n# finally drop the origin_id column once it has become the row id\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%  # pick the three fields we need\n  spread(destination_id, total_cost) %&gt;%    # convert thin table to wide table by destination\n  select(c(-c('origin_id')))    # remove origin id column after spreading\n\n\n\n\n\n\n\nNote\n\n\n\nSince tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\n\n\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n# convert metres to kilometres and coerce to numeric matrix\ndistmat_km &lt;- as.matrix(distmat / 1000)   # numeric matrix required by SpatialAcc::ac"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-hansen-method",
    "href": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-hansen-method",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "Now, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\n# compute Hansen accessibility values per origin cell\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,     # vector of origin demands\n                            eldercare$capacity,  # vector of destination capacities\n                            distmat_km,          # distance matrix in kilometres\n                            #d0 = 50,            # optional cut off distance in kilometres if needed\n                            power = 2,           # distance decay parameter for Hansen\n                            family = \"Hansen\"))  # choose the Hansen method\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\n# rename the single output column to a tidy name\ncolnames(acc_Hansen) &lt;- \"accHansen\"   # set a clear column name\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\n# convert to tibble for nicer printing and dplyr friendliness\nacc_Hansen &lt;- as_tibble(acc_Hansen)   # tibble retains numeric vector with column name\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\n# attach the accessibility values to the hexagon sf object\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen) # geometry from hexagons plus new column\n\nNotice that hexagon_Hansen is a simple feature data frame and not a typical tibble data frame.\n\nWe can also run the entire sequence in one block as shown next. This alternative uses a different power value as demonstrated in the source.\n\n# alternate single block version as shown in the source\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,    # origin demand\n                            eldercare$capacity, # destination capacity\n                            distmat_km,         # distance matrix in kilometres\n                            #d0 = 50,           # optional cut off\n                            power = 0.5,        # alternate decay value used in the example\n                            family = \"Hansen\")) # Hansen method\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"             # tidy column name\nacc_Hansen &lt;- as_tibble(acc_Hansen)             # convert to tibble\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen) # join to geometry\n\n\n\n\n\n\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\n# compute map extent once for reuse\nmapex &lt;- st_bbox(hexagons) \n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\n# choropleth of Hansen accessibility with facility locations overlaid\n\ntm_shape(hexagon_Hansen, bbox = mapex) +        # set the data and map extent\n  tm_polygons(fill = \"accHansen\",               # fill by accessibility field\n              fill.scale = tm_scale_intervals(  # classify values into intervals\n                style = \"quantile\",             # equal number of cells per class\n                n = 10,                         # ten classes\n                values = \"brewer.blues\")) +     # blue palette\n  tm_shape(eldercare) +                         # overlay supply points\n  tm_dots(size = 0.3,                           # small dot size for points\n          fill = \"red\",                         # red fill for emphasis\n          col = \"black\") +                      # black outline\n  tm_title(\"Accessibility to eldercare  Hansen method\") +  # map title\n  tm_layout(frame = TRUE) +                     # draw a neatline frame\n  tm_compass(type = \"8star\", size = 2) +        # compass for orientation\n  tm_scalebar() +                               # scale bar in map units\n  tm_grid(alpha = 0.2)        \n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into haxegon_Hansen simple feature data frame by using the code chunk below.\n\n# spatial join to append region name to each hexagon\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz, join = st_intersects)  # brings REGION_N into the grid\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n# boxplot of log accessibility by planning region\n\nggplot(data = hexagon_Hansen,    # use the joined sf object\n       aes(y = log(accHansen),   # log scale for the response\n           x = REGION_N)) +      # planning region on x axis\n  geom_boxplot() +               # draw boxplots per region\n  geom_point(stat = \"summary\",   # add mean points\n             fun.y = \"mean\",     # function is mean\n             colour = \"red\",     # red mean marker\n             size = 2)           # marker size\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "href": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\n# compute KD2SFCA accessibility with a five kilometre catchment and power two\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                             eldercare$capacity,  # destination capacity\n                             distmat_km,          # distance matrix in kilometres\n                             d0 = 5,              # catchment threshold in kilometres\n                             power = 2,           # decay parameter\n                             family = \"KD2SFCA\")) # KD2SFCA method\n\n# tidy column name and bind to geometry\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"               # set column name\nacc_KD2SFCA &lt;- as_tibble(acc_KD2SFCA)               # tibble for convenience\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA) # join to hexagon geometry\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of KD2SFCA accessibility using the same extent and palette\n\ntm_shape(hexagon_KD2SFCA, bbox = mapex) +        # reuse map extent\n  tm_polygons(fill = \"accKD2SFCA\",               # fill by KD2SFCA field\n              fill.scale = tm_scale_intervals(   # classification settings\n                style = \"quantile\",              # quantile style\n                n = 10,                          # ten classes\n                values = \"brewer.blues\")) +      # palette\n  tm_shape(eldercare) +                          # overlay supply points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  KD2SFCA method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, join = st_intersects)  # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_KD2SFCA,  # KD2SFCA results\n       aes(y = accKD2SFCA,      # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean markers\n             fun.y = \"mean\",    # mean function\n             colour = \"red\",    # red mean marker\n             size = 2)          # size of marker\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "href": "Hands-on_Ex09/hand-on_ex09.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\n# compute SAM accessibility with the same catchment and decay as KD2SFCA\nacc_SAM &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                         eldercare$capacity,  # destination capacity\n                         distmat_km,          # distance matrix in kilometres\n                         d0 = 5,              # catchment in kilometres\n                         power = 2,           # decay parameter\n                         family = \"SAM\"))     # SAM method\n\n# tidy column name and bind to geometry\ncolnames(acc_SAM) &lt;- \"accSAM\"                 # set column name\nacc_SAM &lt;- as_tibble(acc_SAM)                 # tibble form\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)   # join to geometry\n\n\n\n\nThe code chunk below uses a collection of mapping functions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of SAM accessibility\n\ntm_shape(hexagon_SAM, bbox = mapex) +           # reuse extent\n  tm_polygons(fill = \"accSAM\",                  # fill by SAM field\n              fill.scale = tm_scale_intervals(  # classify into intervals\n                style = \"quantile\",             # quantile style\n                n = 10,                         # ten classes\n                values = \"brewer.blues\")) +     # palette\n  tm_shape(eldercare) +                         # overlay points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  SAM method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz, join = st_intersects)   # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_SAM,      # SAM results\n       aes(y = accSAM,          # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean marker\n             fun.y = \"mean\",    # use mean\n             colour = \"red\",    # red mean\n             size = 2)          # marker size\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#introduction",
    "href": "Take-home_Ex03/take-home_ex03.html#introduction",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis study examines the determinants of Housing and Development Board resale prices in Singapore using an explanatory spatial modelling framework. We focus on 4 room flats during 1 January 2025 to 30 September 2025 to maintain comparability and reflect current family preferences for this size category (EdgeProp Singapore, 2024). The objective is to identify structural and locational attributes associated with transaction prices and to reveal how those associations vary across space. Structural variables include floor area in square meters, floor level, age in years, and remaining lease in years. Locational variables include proximity to rail transit, parks and open space, primary schools including selected high performing schools, childcare and preschool supply, supermarkets, shopping malls, and bus stops. Count variables are computed within 350 meters and 1,000 meters buffers, and point to point proximity is measured in meters after projecting all layers to SVY21. The remainder of this report states the research questions, documents reproducible data preparation, specifies the global and geographically weighted models, and presents spatially explicit findings with concise implications."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#research-questions",
    "href": "Take-home_Ex03/take-home_ex03.html#research-questions",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "2 Research Questions",
    "text": "2 Research Questions\nThis section states operational questions that guide an explanatory analysis of HDB resale prices for 4-room flats from 1 January 2025 to 30 September 2025. The questions align variable construction, model calibration, and interpretation with the objective of identifying price drivers and their spatial variation using a global Multiple Linear Regression baseline and a local geographically weighted model.\nRQ1: Which structural attributes of a flat such as floor area, floor level, age, and remaining lease are significantly associated with resale price during the study window\nRQ2: Which locational attributes such as proximity to rail transit, parks, primary schools, childcare supply, supermarkets, shopping malls, and bus stops are significantly associated with resale price\nRQ3: To what extent do these associations vary across space, as evidenced by spatially varying coefficients from a geographically weighted model\nRQ4: Does the geographically weighted model materially improve fit and residual behaviour relative to a global Multiple Linear Regression specification\nRQ5: Where are local effects notably stronger or weaker, and what practical implications follow for buyers, valuers, and planners\nTogether these questions structure the analytical framework and ensure direct alignment between the study objectives and subsequent methods."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#the-data",
    "href": "Take-home_Ex03/take-home_ex03.html#the-data",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "3 The Data",
    "text": "3 The Data\nIn this section, we document every dataset used to construct a transparent, auditable, and fully reproducible sandbox for explanatory modelling of HDB resale prices. The unit of analysis is individual transactions for 4 room flats within 1 January 2025 to 30 September 2025. The core input is the official Resale Flat Prices table. All other layers are supporting geographic data used strictly to engineer the required structural and location variables. To make verification straightforward, sources are presented in a compact 4 column table that lists the dataset name, its modelling purpose, the file format, and a direct source link. All spatial layers are ingested as simple features and projected to SVY21 EPSG 3414 so distance and buffer computations are expressed in meters. Where an application programming interface is involved, we will cache responses to local files to allow repeated runs without network variance.\n\n\n\nTable 1 Data sources used in Take home Ex03\n\n\nDataset\nPurposes\nFormat\nSource\n\n\n\n\nHDB Resale Flat Prices 2017 Onward\nCore transactions filtered to 4 rooms and study window\nCSV\ndata.gov.sg\n\n\nURA Master Plan 2019 Subzone Boundary No Sea\nMap context and optional mainland mask\nKML\ndata.gov.sg\n\n\nMRT Stations\nTransit proximity\nGeoJSON\ndata.gov.sg\n\n\nParks\nGreen space proximity\nGeoJSON\ndata.gov.sg\n\n\nEldercare Services\nEldercare proximity\nGeoJSON\ndata.gov.sg\n\n\nHawker centre\nFood services proximity\nGeoJSON\ndata.gov.sg\n\n\nSupermarkets\nSupermarket proximity\nGeoJSON\ndata.gov.sg\n\n\nShopping Malls\nRetail centre proximity\nCSV\nkaggle\n\n\nGeneral Information of School\nElite school proximity within 1 km\nCSV\ndata.gov.sg\n\n\nChildcare Centres\nChildcare counts within 350 meters\nGeoJSON\ndata.gov.sg\n\n\nKindergartens\nKindergarten counts within 350 meters\nGeoJSON\ndata.gov.sg\n\n\nBus Stops\nBus stop counts within 350 meters\nSHP\nLand Transport DataMall\n\n\nCBD Reference\nReference for CBD distance\nPoint or Polygon\ndata.gov.sg\n\n\nOneMap Geocoding Service\nAddress/Postcode to coordinate lookup\nAPI JSON\nOneMap"
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html",
    "href": "In-Class_Ex09/in-class_ex09.html",
    "title": "In-class Ex09",
    "section": "",
    "text": "In this exercise you will learn to model geographical accessibility in R using packages for simple features, mapping and accessibility computation. We will prepare the data, evaluate Hansen accessibility at five power settings (0.5, 1.0, 1.5, 2.0, and 2.5) and plot a single summary graph.\n\n\n\nInstall and load required packages. The code uses pacman::p_load() to install on demand and load in one step.\n\n# install and load required packages in one call\npacman::p_load(tmap, SpatialAcc, sf,   # mapping, accessibility, simple features\n               ggstatsplot, reshape2,  # statistical graphics, data reshaping\n               tidyverse, ggplot2)  \n\n\n\n\n\n\nThree geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n# read URA subzones as simple features\nmpsz &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n                layer = \"MP14_SUBZONE_NO_SEA_PL\")   # polygon layer\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# read hexagon grid as simple features\nhexagons &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"hexagons\") # polygon grid\n\nReading layer `hexagons' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n# read eldercare locations as simple features\neldercare &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"ELDERCARE\")   # point layer\n\nReading layer `ELDERCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n# keep only id and address then add constant capacity field for supply locations\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%   # retain unique id and address\n  mutate(capacity = 100)        # placeholder capacity for each facility\n\n\n# keep only id then add constant demand field for each hexagon cell\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%       # retain unique id of each grid cell\n  mutate(demand = 100)  # placeholder demand for each cell\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used.\n\n\n\n\n\n\nThe code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n# import distance matrix as a tibble\nODMatrix &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/aspatial/OD_Matrix.csv\",\n  skip = 0\n  )  # columns include origin, destination and costs\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n# build a wide distance matrix using total_cost\n# select needed columns then spread destination ids to wide columns\n# finally drop the origin_id column once it has become the row id\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%  # pick the three fields we need\n  spread(destination_id, total_cost) %&gt;%    # convert thin table to wide table by destination\n  select(c(-c('origin_id')))    # remove origin id column after spreading\n\n\n\n\n\n\n\nNote\n\n\n\nSince tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\n\n\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n# convert metres to kilometres and coerce to numeric matrix\ndistmat_km &lt;- as.matrix(distmat / 1000)   # numeric matrix required by SpatialAcc::ac\n\n\n\n\n\n\n\n\n# evaluate Hansen accessibility at five power settings and plot a single summary graph\npowers &lt;- c(0.5, 1.0, 1.5, 2.0, 2.5) # the required power values\n\n# compute accessibility for each power value and collect results in a long table\nacc_long &lt;- lapply(powers, function(p) {\ntibble::tibble(\npower = p, # record the power value used\nacc = as.numeric( # coerce to numeric vector\nSpatialAcc::ac(hexagons$demand, # demand at each origin cell\neldercare$capacity, # capacity at each supply point\ndistmat_km, # distance matrix in kilometres\npower = p, # current power value\nfamily = \"Hansen\")) # Hansen method\n)\n}) %&gt;% dplyr::bind_rows()\n\n\n\n\nDraw a box plot on log scale that is used to compare levels and spread clearly\n\n# draw one graph that shows how the distribution changes across power values\nggplot(acc_long, aes(x = factor(power), y = acc)) +\ngeom_boxplot() +\nscale_y_log10() +\nlabs(title = \"Hansen accessibility across power values\",\nx = \"Power\",\ny = \"Accessibility value log scale\")\n\n\n\n\n\n\n\n\n\n\n\nHere is what the box plot is telling us.\n\nAs power increases from 0.5 to 2.5 the median accessibility drops steadily. A larger power applies stronger distance decay, so far facilities contribute far less. The system becomes dominated by very near supply.\nThe spread expands toward very small values as power rises. Many cells fall to extremely low accessibility once distance is penalised more strongly. In other words, places that rely on distant facilities become the most disadvantaged when power is large.\nA small set of cells near facilities keep relatively high values even at larger power. This shows that the index increasingly rewards immediate proximity as distance sensitivity grows.\n\nImplications for analysis and planning\n\nRanking of cells is model sensitive. Reports should state the chosen power and show a sensitivity band across the five settings, since policy conclusions may change if a different decay is assumed.\nEquity signals sharpen with larger power. Peripheral or poorly served neighbourhoods look much worse when travel impedance matters more. Target these for new capacity or improved transport links.\nSite selection logic changes with the setting. Smaller power spreads benefits widely and is suitable when people are willing to travel farther. Larger power prioritises very local access and is suitable when travel is difficult or costly for the users.\nChoose power using evidence. Calibrate with observed travel behaviour or expert judgement, then confirm that conclusions are stable across nearby values."
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#introduction",
    "href": "In-Class_Ex09/in-class_ex09.html#introduction",
    "title": "In-class Ex09",
    "section": "",
    "text": "In this exercise you will learn to model geographical accessibility in R using packages for simple features, mapping and accessibility computation. We will prepare the data, evaluate Hansen accessibility at five power settings (0.5, 1.0, 1.5, 2.0, and 2.5) and plot a single summary graph."
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#the-data",
    "href": "In-Class_Ex09/in-class_ex09.html#the-data",
    "title": "In-class Ex09",
    "section": "",
    "text": "Four data sets are used in this exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\n\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExcept MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage."
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#getting-started",
    "href": "In-Class_Ex09/in-class_ex09.html#getting-started",
    "title": "In-class Ex09",
    "section": "",
    "text": "Install and load required packages. The code uses pacman::p_load() to install on demand and load in one step.\n\n# install and load required packages in one call\npacman::p_load(tmap, SpatialAcc, sf,   # mapping, accessibility, simple features\n               ggstatsplot, reshape2,  # statistical graphics, data reshaping\n               tidyverse, ggplot2)"
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#geospatial-data-wrangling",
    "href": "In-Class_Ex09/in-class_ex09.html#geospatial-data-wrangling",
    "title": "In-class Ex09",
    "section": "",
    "text": "Three geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n# read URA subzones as simple features\nmpsz &lt;- st_read(dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n                layer = \"MP14_SUBZONE_NO_SEA_PL\")   # polygon layer\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# read hexagon grid as simple features\nhexagons &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"hexagons\") # polygon grid\n\nReading layer `hexagons' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n# read eldercare locations as simple features\neldercare &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial\", \n  layer = \"ELDERCARE\")   # point layer\n\nReading layer `ELDERCARE' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n# keep only id and address then add constant capacity field for supply locations\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%   # retain unique id and address\n  mutate(capacity = 100)        # placeholder capacity for each facility\n\n\n# keep only id then add constant demand field for each hexagon cell\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%       # retain unique id of each grid cell\n  mutate(demand = 100)  # placeholder demand for each cell\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used."
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#apsaital-data-handling-and-wrangling",
    "href": "In-Class_Ex09/in-class_ex09.html#apsaital-data-handling-and-wrangling",
    "title": "In-class Ex09",
    "section": "",
    "text": "The code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n# import distance matrix as a tibble\nODMatrix &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Hands-on_Ex09/data/aspatial/OD_Matrix.csv\",\n  skip = 0\n  )  # columns include origin, destination and costs\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n# build a wide distance matrix using total_cost\n# select needed columns then spread destination ids to wide columns\n# finally drop the origin_id column once it has become the row id\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%  # pick the three fields we need\n  spread(destination_id, total_cost) %&gt;%    # convert thin table to wide table by destination\n  select(c(-c('origin_id')))    # remove origin id column after spreading\n\n\n\n\n\n\n\nNote\n\n\n\nSince tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\n\n\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n# convert metres to kilometres and coerce to numeric matrix\ndistmat_km &lt;- as.matrix(distmat / 1000)   # numeric matrix required by SpatialAcc::ac"
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-hansen-method",
    "href": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-hansen-method",
    "title": "In-class Ex09",
    "section": "",
    "text": "# evaluate Hansen accessibility at five power settings and plot a single summary graph\npowers &lt;- c(0.5, 1.0, 1.5, 2.0, 2.5) # the required power values\n\n# compute accessibility for each power value and collect results in a long table\nacc_long &lt;- lapply(powers, function(p) {\ntibble::tibble(\npower = p, # record the power value used\nacc = as.numeric( # coerce to numeric vector\nSpatialAcc::ac(hexagons$demand, # demand at each origin cell\neldercare$capacity, # capacity at each supply point\ndistmat_km, # distance matrix in kilometres\npower = p, # current power value\nfamily = \"Hansen\")) # Hansen method\n)\n}) %&gt;% dplyr::bind_rows()\n\n\n\n\nDraw a box plot on log scale that is used to compare levels and spread clearly\n\n# draw one graph that shows how the distribution changes across power values\nggplot(acc_long, aes(x = factor(power), y = acc)) +\ngeom_boxplot() +\nscale_y_log10() +\nlabs(title = \"Hansen accessibility across power values\",\nx = \"Power\",\ny = \"Accessibility value log scale\")\n\n\n\n\n\n\n\n\n\n\n\nHere is what the box plot is telling us.\n\nAs power increases from 0.5 to 2.5 the median accessibility drops steadily. A larger power applies stronger distance decay, so far facilities contribute far less. The system becomes dominated by very near supply.\nThe spread expands toward very small values as power rises. Many cells fall to extremely low accessibility once distance is penalised more strongly. In other words, places that rely on distant facilities become the most disadvantaged when power is large.\nA small set of cells near facilities keep relatively high values even at larger power. This shows that the index increasingly rewards immediate proximity as distance sensitivity grows.\n\nImplications for analysis and planning\n\nRanking of cells is model sensitive. Reports should state the chosen power and show a sensitivity band across the five settings, since policy conclusions may change if a different decay is assumed.\nEquity signals sharpen with larger power. Peripheral or poorly served neighbourhoods look much worse when travel impedance matters more. Target these for new capacity or improved transport links.\nSite selection logic changes with the setting. Smaller power spreads benefits widely and is suitable when people are willing to travel farther. Larger power prioritises very local access and is suitable when travel is difficult or costly for the users.\nChoose power using evidence. Calibrate with observed travel behaviour or expert judgement, then confirm that conclusions are stable across nearby values."
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "href": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\n# compute KD2SFCA accessibility with a five kilometre catchment and power two\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                             eldercare$capacity,  # destination capacity\n                             distmat_km,          # distance matrix in kilometres\n                             d0 = 5,              # catchment threshold in kilometres\n                             power = 2,           # decay parameter\n                             family = \"KD2SFCA\")) # KD2SFCA method\n\n# tidy column name and bind to geometry\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"               # set column name\nacc_KD2SFCA &lt;- as_tibble(acc_KD2SFCA)               # tibble for convenience\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA) # join to hexagon geometry\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of KD2SFCA accessibility using the same extent and palette\n\ntm_shape(hexagon_KD2SFCA, bbox = mapex) +        # reuse map extent\n  tm_polygons(fill = \"accKD2SFCA\",               # fill by KD2SFCA field\n              fill.scale = tm_scale_intervals(   # classification settings\n                style = \"quantile\",              # quantile style\n                n = 10,                          # ten classes\n                values = \"brewer.blues\")) +      # palette\n  tm_shape(eldercare) +                          # overlay supply points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  KD2SFCA method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, join = st_intersects)  # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_KD2SFCA,  # KD2SFCA results\n       aes(y = accKD2SFCA,      # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean markers\n             fun.y = \"mean\",    # mean function\n             colour = \"red\",    # red mean marker\n             size = 2)          # size of marker\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "href": "In-Class_Ex09/in-class_ex09.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "title": "Hands-on Ex09",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\n# compute SAM accessibility with the same catchment and decay as KD2SFCA\nacc_SAM &lt;- data.frame(ac(hexagons$demand,     # origin demand\n                         eldercare$capacity,  # destination capacity\n                         distmat_km,          # distance matrix in kilometres\n                         d0 = 5,              # catchment in kilometres\n                         power = 2,           # decay parameter\n                         family = \"SAM\"))     # SAM method\n\n# tidy column name and bind to geometry\ncolnames(acc_SAM) &lt;- \"accSAM\"                 # set column name\nacc_SAM &lt;- as_tibble(acc_SAM)                 # tibble form\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)   # join to geometry\n\n\n\n\nThe code chunk below uses a collection of mapping functions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n# map of SAM accessibility\n\ntm_shape(hexagon_SAM, bbox = mapex) +           # reuse extent\n  tm_polygons(fill = \"accSAM\",                  # fill by SAM field\n              fill.scale = tm_scale_intervals(  # classify into intervals\n                style = \"quantile\",             # quantile style\n                n = 10,                         # ten classes\n                values = \"brewer.blues\")) +     # palette\n  tm_shape(eldercare) +                         # overlay points\n  tm_dots(size = 0.3, fill = \"red\", col = \"black\") +  # style points\n  tm_title(\"Accessibility to eldercare  SAM method\") +\n  tm_layout(frame = TRUE) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\n# append region names then plot distribution by region\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz, join = st_intersects)   # add REGION_N\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data = hexagon_SAM,      # SAM results\n       aes(y = accSAM,          # response on original scale\n           x = REGION_N)) +     # planning region\n  geom_boxplot() +              # boxplots\n  geom_point(stat = \"summary\",  # mean marker\n             fun.y = \"mean\",    # use mean\n             colour = \"red\",    # red mean\n             size = 2)          # marker size\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "In-Class_Ex09/in-class_ex09.html#evaluate-hansen-accessibility-at-five-power-settings-and-plot-a-single-summary-graph",
    "href": "In-Class_Ex09/in-class_ex09.html#evaluate-hansen-accessibility-at-five-power-settings-and-plot-a-single-summary-graph",
    "title": "In-class Ex09",
    "section": "",
    "text": "# evaluate Hansen accessibility at five power settings and plot a single summary graph\npowers &lt;- c(0.5, 1.0, 1.5, 2.0, 2.5) # the required power values\n\n\n# compute accessibility for each power value and collect results in a long table\nacc_long &lt;- lapply(powers, function(p) {\ntibble::tibble(\npower = p, # record the power value used\nacc = as.numeric( # coerce to numeric vector\nSpatialAcc::ac(hexagons$demand, # demand at each origin cell\neldercare$capacity, # capacity at each supply point\ndistmat_km, # distance matrix in kilometres\npower = p, # current power value\nfamily = \"Hansen\")) # Hansen method\n)\n}) %&gt;% dplyr::bind_rows()\n\n\n# draw one graph that shows how the distribution changes across power values\n# a box plot on log scale is used so that you can compare levels and spread clearly\nlibrary(ggplot2)\n\n\nggplot(acc_long, aes(x = factor(power), y = acc)) +\ngeom_boxplot() +\nscale_y_log10() +\nlabs(title = \"Hansen accessibility across power values\",\nx = \"Power\",\ny = \"Accessibility value log scale\")\n\n\n\n\n\n\n\n\n\n\nHere is what the box plot is telling us.\n\nAs power increases from 0.5 to 2.5 the median accessibility drops steadily. A larger power applies stronger distance decay, so far facilities contribute far less. The system becomes dominated by very near supply.\nThe spread expands toward very small values as power rises. Many cells fall to extremely low accessibility once distance is penalised more strongly. In other words, places that rely on distant facilities become the most disadvantaged when power is large.\nA small set of cells near facilities keep relatively high values even at larger power. This shows that the index increasingly rewards immediate proximity as distance sensitivity grows.\n\nImplications for analysis and planning\n\nRanking of cells is model sensitive. Reports should state the chosen power and show a sensitivity band across the five settings, since policy conclusions may change if a different decay is assumed.\nEquity signals sharpen with larger power. Peripheral or poorly served neighbourhoods look much worse when travel impedance matters more. Target these for new capacity or improved transport links.\nSite selection logic changes with the setting. Smaller power spreads benefits widely and is suitable when people are willing to travel farther. Larger power prioritises very local access and is suitable when travel is difficult or costly for the users.\nChoose power using evidence. Calibrate with observed travel behaviour or expert judgement, then confirm that conclusions are stable across nearby values."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#setup-the-environment",
    "href": "Take-home_Ex03/take-home_ex03.html#setup-the-environment",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "4 Setup the Environment",
    "text": "4 Setup the Environment\nA consistent analytical environment ensures reproducibility and transparency in spatial data analysis. This section defines the R environment used in the study. All scripts were executed in RStudio using packages that support data wrangling, spatial statistics and visualisation. The pacman package is used to automate installation and loading of required libraries. Each library serves a specific purpose within the analytical workflow, and a random seed is set to guarantee consistent statistical outputs across repeated runs.\n\n# ensure the package manager is available\nif (!require(pacman)) install.packages(\"pacman\")  # install pacman once if missing\n\n# load all required libraries in one call\npacman::p_load(\n  tidyverse,    # data wrangling and plotting\n  sf,           # spatial vector data handling\n  GWmodel,      # geographically weighted modelling\n  sfdep,        # spatial diagnostics used in the report\n  tmap,         # cartographic maps (static and interactive)\n  ggplot2,      # grammar of graphics for charts\n  httr,         # HTTP requests for geocoding if used\n  jsonlite,     # parsing JSON responses\n  progress,     # simple progress bars for batch jobs\n  knitr,        # report tables\n  kableExtra,   # table styling for HTML output\n  purrr,\n  stringr,\n  Hmisc,\n  ggpubr,\n  gtsummary,\n  RColorBrewer, \n  see,\n  performance,  # used to test multicollinearity.\n  olsrr,        # building OLS and performing diagnostics tests \n  qqplotr,\n  corrplot      # multivariate data visualisation and analysis\n)\n\n# set reproducibility and friendly numeric printing\nset.seed(626)                       # fixed seed for repeatable results\noptions(stringsAsFactors = FALSE)   # keep character columns as character\noptions(scipen = 999)               # print numbers without scientific notation"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#data-preprocessing-and-wrangling",
    "href": "Take-home_Ex03/take-home_ex03.html#data-preprocessing-and-wrangling",
    "title": "Take-home Ex03",
    "section": "5 Data Preprocessing and Wrangling",
    "text": "5 Data Preprocessing and Wrangling\nThis section turns two aspatial sources, HDB Resale Flat Prices 2017 Onward and General Information of School, into spatial features for analysis. For HDB transactions, we filter to 4-room flats between 1 Jan 2025 and 30 Sep 2025, standardise street names, geocode addresses with OneMap, cache results, and convert to sf points in WGS84 before transforming to SVY21 (EPSG 3414, Transverse Mercator) so distances are in metres. For schools, we keep only Primary level entries, normalise and de-duplicate 6-digit postcodes, query OneMap by postcode, then join X, Y coordinates and convert directly to SVY21 since OneMap returns SVY21 coordinates for postal lookups. All objects, paths, and names match your screenshots. Every step is commented, deterministic, and cache-friendly so reruns do not hit the API unnecessarily. The outputs are two clean spatial objects: HDBresale_sf and primary_schools_sf, ready for Section 6 where we will engineer proximity and count variables (for example, 350 m buffers and 1,000 m counts) and integrate the amenity layers.\n\n5.1 5.1 GGeocoding HDB resale transactions (addresses)\n\n5.1.1 5.1.1 Filter to study window and flat type\nRead the CSV, keep 4-room, and filter to 2025-01 through 2025-09.\n\n# read the full resale table from your rawdata folder\nHDBresale_raw &lt;- readr::read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex03/data/aspatial/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv\") # load CSV into memory\n\n# filter to four room within your study window shown earlier\nHDBresale &lt;- HDBresale_raw %&gt;%                                          # start pipeline\n  dplyr::filter(flat_type == \"4 ROOM\") %&gt;%                              # keep four room only\n  dplyr::filter(month &gt;= \"2025-01\", month &lt;= \"2025-09\")                 # keep working period\n\n\n\n5.1.2 5.1.2 Normalise street tokens\nLight token fixes improve geocoding hit-rate.\n\n# normalise street tokens for better match rate before calling OneMap\nHDBresale$street_name &lt;- gsub(\"ST\\\\.\", \"SAINT\", HDBresale$street_name)  # expand ST. to SAINT\n\n\n\n5.1.3 5.1.3 OneMap address geocoder with caching\nCall OneMap once per unique address; cache results to CSV.\n\n# define the professor style OneMap geocode function for address strings\ngeocode &lt;- function(block, streetname) {                          # function takes two strings\n  base_url &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"   # OneMap search endpoint\n  address  &lt;- paste(block, streetname, sep = \" \")                 # join block and street\n  query &lt;- list(                                                  # build query parameter list\n    searchVal      = address,                                     # address to search\n    returnGeom     = \"Y\",                                         # ask for geometry\n    getAddrDetails = \"N\",                                         # no extra details\n    pageNum        = \"1\"                                          # take first page only\n  )\n  res_txt &lt;- httr::content(httr::GET(base_url, query = query),    # send request and read text\n                           as = \"text\", encoding = \"UTF-8\")       # ensure UTF eight decoding\n  out &lt;- jsonlite::fromJSON(res_txt)                              # parse JSON to a list\n  if (length(out$results) == 0) {                                 # if no hit, return NA values\n    tibble::tibble(LATITUDE = NA_real_, LONGITUDE = NA_real_)     # keep column names fixed\n  } else {                                                        # if we have at least one hit\n    tibble::tibble(                                               # return first result only\n      LATITUDE  = as.numeric(out$results$LATITUDE[1]),            # numeric latitude\n      LONGITUDE = as.numeric(out$results$LONGITUDE[1])            # numeric longitude\n    )\n  }\n}\n\n# # smoke test on a known HDB address to verify numeric output\n# geocode(\"101\", \"JURONG EAST STREET 13\")                         # quick single call check\n\n# batch geocode with a small progress bar and cache to file to avoid requery\ncache_csv &lt;- \"data/aspatial/geocode_hdb_cache.csv\"                # path for cache file\nif (file.exists(cache_csv)) {                                     # if cache already exists\n  cache_tbl &lt;- readr::read_csv(cache_csv, show_col_types = FALSE) # read cached results\n} else {\n  pb &lt;- progress::progress_bar$new(total = nrow(HDBresale))       # create a progress bar\n  cache_tbl &lt;- HDBresale %&gt;%                                      # start from filtered table\n    dplyr::mutate(                                                # add a list column of tibbles\n      geo = purrr::map2(block, street_name, function(b, s) {      # iterate per row\n        pb$tick()                                                 # advance progress\n        geocode(b, s)                                             # call OneMap\n      })\n    ) %&gt;% \n    tidyr::unnest(geo) %&gt;%                                        # expand LATITUDE and LONGITUDE\n    dplyr::select(block, street_name, LATITUDE, LONGITUDE)        # keep minimal fields\n  readr::write_csv(cache_tbl, cache_csv)                          # save cache for reuse\n}\n\n\n\n5.1.4 5.1.4 Attach coordinates and create SF points\nBind LATITUDE, LONGITUDE, convert to WGS84 points, then to SVY21 (TM).\n\n# merge coordinates into the HDB table\nHDBresale &lt;- HDBresale %&gt;%                                        # start from filtered HDB\n  dplyr::left_join(cache_tbl, by = c(\"block\",\"street_name\"))      # append LAT and LON\n\n# convert to sf in WGS eighty four then project to SVY twenty one metres\nHDBresale_sf &lt;- HDBresale %&gt;%                                       # start from geocoded HDB\n  sf::st_as_sf(coords = c(\"LONGITUDE\",\"LATITUDE\"), crs = 4326) %&gt;%  # create point geometry\n  sf::st_transform(3414)                                            # transform to SVY twenty one\n\n\n\n5.1.5 5.1.5 Validate the geometry and reference system of HDB\nVerify the geometry output by using glimpse().\n\n# quick verification of geometry and reference system\ndplyr::glimpse(HDBresale_sf)    # should show POINT [m] and EPSG 3414\n\nRows: 44,956\nColumns: 12\n$ month               &lt;chr&gt; \"2025-01\", \"2025-01\", \"2025-01\", \"2025-01\", \"2025-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", \"4 ROOM\", …\n$ block               &lt;chr&gt; \"337\", \"337\", \"337\", \"337\", \"337\", \"337\", \"337\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 1\", \"ANG MO KIO AVE 1\", \"ANG MO KI…\n$ storey_range        &lt;chr&gt; \"13 TO 15\", \"13 TO 15\", \"13 TO 15\", \"13 TO 15\", \"1…\n$ floor_area_sqm      &lt;dbl&gt; 91, 91, 91, 91, 91, 91, 91, 94, 94, 94, 94, 94, 91…\n$ flat_model          &lt;chr&gt; \"New Generation\", \"New Generation\", \"New Generatio…\n$ lease_commence_date &lt;dbl&gt; 1982, 1982, 1982, 1982, 1982, 1982, 1982, 2012, 20…\n$ remaining_lease     &lt;chr&gt; \"56 years 03 months\", \"56 years 03 months\", \"56 ye…\n$ resale_price        &lt;dbl&gt; 582000, 582000, 582000, 582000, 582000, 582000, 58…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (30036.29 3…\n\n\n\n\n\n5.2 5.2 Geocoding schools (postcodes)\n\n5.2.1 5.2.1 Load and keep primary level\nRead the table and filter mainlevel_code == \"PRIMARY\".\n\n# read the MOE schools table as shown in your screenshots\nschools &lt;- readr::read_csv(\"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/Take-home_Ex03/data/aspatial/Generalinformationofschools.csv\")     # load school CSV\n\n# keep primary level only for this study\nprimary_schools &lt;- dplyr::filter(schools, mainlevel_code == \"PRIMARY\")         # filter to primary\n\n\n\n5.2.2 5.2.2 Prepare unique 6-digit postcodes\nPad to 6 digits and deduplicate.\n\n# build a unique vector of six digit postcodes, left padded if required\npostcodes &lt;- primary_schools %&gt;%                                                # start from primary\n  dplyr::mutate(postal_code = stringr::str_pad(postal_code, width = 6, pad = \"0\")) %&gt;%  # pad left\n  dplyr::pull(postal_code) %&gt;% unique()                                         # unique codes only\n\n\n\n5.2.3 5.2.3 OneMap postal geocoder\nQuery OneMap by postal code to obtain X, Y in SVY21.\n\n# OneMap elastic search by postal code, modelled on the in class approach\nom_url &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"                     # same endpoint\nfound &lt;- tibble::tibble()                                                       # holder for matches\nnot_found &lt;- tibble::tibble(postal_code = character())                          # holder for misses\n\nfor (pc in postcodes) {                                                         # loop over codes\n  qry &lt;- list(searchVal = pc, returnGeom = \"Y\", getAddrDetails = \"Y\", pageNum = \"1\")  # parameters\n  res &lt;- httr::GET(om_url, query = qry)                                         # send the request\n  js  &lt;- httr::content(res)                                                     # parse to list\n  if (!is.null(js$found) && js$found != 0) {                                    # if at least one hit\n    df &lt;- as.data.frame(js$results, stringsAsFactors = FALSE)                   # convert to frame\n    df$POSTAL &lt;- as.character(df$POSTAL)                                        # ensure char type\n    found &lt;- dplyr::bind_rows(found, tibble::as_tibble(df))                     # accumulate rows\n  } else {\n    not_found &lt;- dplyr::bind_rows(not_found, tibble::tibble(postal_code = pc))  # record miss\n  }\n}\n\n\n\n5.2.4 5.2.4 Join X, Y and create SF points\nMerge coordinates and convert directly to SVY21.\n\n# keep only essentials from the OneMap results\nfound &lt;- found %&gt;% dplyr::select(POSTAL, X, Y)                        # POS, X, Y fields\n\n# join X and Y to the school table by postal\nprimary_schools_xy &lt;- primary_schools %&gt;%                                        # start from primary\n  dplyr::mutate(postal_code = stringr::str_pad(postal_code, 6, pad = \"0\")) %&gt;%   # ensure six digits\n  dplyr::left_join(found, by = c(\"postal_code\" = \"POSTAL\"))                      # append X and Y\n\n# convert to sf in SVY twenty one directly since X and Y are in SVY twenty one metres\nprimary_schools_sf &lt;- sf::st_as_sf(primary_schools_xy,                # table with X, Y\n                                   coords = c(\"X\",\"Y\"), crs = 3414)   # build sf object\n\n\n\n5.2.5 5.2.5 Validate the geometry and reference system of primary school\nVerify the geometry output by using glimpse().\n\n# check the object for geometry and reference system\ndplyr::glimpse(primary_schools_sf)    # should show POINT [m]\n\nRows: 179\nColumns: 32\n$ school_name        &lt;chr&gt; \"ADMIRALTY PRIMARY SCHOOL\", \"AHMAD IBRAHIM PRIMARY …\n$ url_address        &lt;chr&gt; \"https://admiraltypri.moe.edu.sg/\", \"http://www.ahm…\n$ address            &lt;chr&gt; \"11 WOODLANDS CIRCLE\", \"10 YISHUN STREET 11\", \"100 …\n$ postal_code        &lt;chr&gt; \"738907\", \"768643\", \"579646\", \"159016\", \"544969\", \"…\n$ telephone_no       &lt;chr&gt; \"63620598\", \"67592906\", \"64547672\", \"62485400\", \"68…\n$ telephone_no_2     &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"67337912…\n$ fax_no             &lt;chr&gt; \"63627512\", \"67592927\", \"64532726\", \"62485409\", \"63…\n$ fax_no_2           &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na…\n$ email_address      &lt;chr&gt; \"ADMIRALTY_PS@MOE.EDU.SG\", \"AIPS@MOE.EDU.SG\", \"AITO…\n$ mrt_desc           &lt;chr&gt; \"Admiralty Station\", \"Yishun\", \"Bishan MRT\", \"Redhi…\n$ bus_desc           &lt;chr&gt; \"TIBS 965, 964, 913\", \"Yishun Ring Road - 812 (whit…\n$ principal_name     &lt;chr&gt; \"MR CHEN ZHONGYI\", \"MS BAEY EE-LYN\", \"MR WONG JIA W…\n$ first_vp_name      &lt;chr&gt; \"MADAM CHUA MUI LING\", \"MADAM YEO SIAU YEN\", \"MR YA…\n$ second_vp_name     &lt;chr&gt; \"MR HAMRI BIN A JALIL\", \"MADAM ZHANG JUNLI\", \"MR CH…\n$ third_vp_name      &lt;chr&gt; \"na\", \"na\", \"MR KHOO THIAM HUAT\", \"na\", \"na\", \"na\",…\n$ fourth_vp_name     &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na…\n$ fifth_vp_name      &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na…\n$ sixth_vp_name      &lt;chr&gt; \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na…\n$ dgp_code           &lt;chr&gt; \"WOODLANDS\", \"YISHUN\", \"BISHAN\", \"BUKIT MERAH\", \"SE…\n$ zone_code          &lt;chr&gt; \"NORTH\", \"NORTH\", \"SOUTH\", \"SOUTH\", \"NORTH\", \"NORTH…\n$ type_code          &lt;chr&gt; \"GOVERNMENT SCHOOL\", \"GOVERNMENT SCHOOL\", \"GOVERNME…\n$ nature_code        &lt;chr&gt; \"CO-ED SCHOOL\", \"CO-ED SCHOOL\", \"CO-ED SCHOOL\", \"CO…\n$ session_code       &lt;chr&gt; \"FULL DAY\", \"SINGLE SESSION\", \"SINGLE SESSION\", \"SI…\n$ mainlevel_code     &lt;chr&gt; \"PRIMARY\", \"PRIMARY\", \"PRIMARY\", \"PRIMARY\", \"PRIMAR…\n$ sap_ind            &lt;chr&gt; \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ autonomous_ind     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ gifted_ind         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Ye…\n$ ip_ind             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ mothertongue1_code &lt;chr&gt; \"CHINESE\", \"CHINESE\", \"CHINESE\", \"CHINESE\", \"CHINES…\n$ mothertongue2_code &lt;chr&gt; \"MALAY\", \"MALAY\", \"na\", \"MALAY\", \"MALAY\", \"MALAY\", …\n$ mothertongue3_code &lt;chr&gt; \"TAMIL\", \"TAMIL\", \"na\", \"TAMIL\", \"TAMIL\", \"TAMIL\", …\n$ geometry           &lt;POINT [m]&gt; POINT (24296.63 47144.77), POINT (27958.14 46…\n\n\n\n\n\n5.3 5.3 Persist clean spatial tables\n\n# save both objects for downstream steps\nreadr::write_rds(HDBresale_sf,       \"data/rds/HDBresale_sf.rds\")       # geocoded HDB\nreadr::write_rds(primary_schools_sf, \"data/rds/primary_schools_sf.rds\") # geocoded schools"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#geocoding-for-geometric",
    "href": "Take-home_Ex03/take-home_ex03.html#geocoding-for-geometric",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "5 Geocoding for Geometric",
    "text": "5 Geocoding for Geometric\nThis section turns two aspatial sources, HDB Resale Flat Prices 2017 Onward and General Information of School, into spatial features for analysis. For HDB transactions, we filter to 4-room flats between 1 Jan 2025 and 30 Sep 2025, standardise street names, geocode addresses with OneMap, cache results, and convert to sf points in WGS84 before transforming to SVY21 (EPSG 3414, Transverse Mercator) so distances are in meters. For schools, we keep only Primary level entries, normalise and de-duplicate 6-digit postcodes, query OneMap by postcode, then join X, Y coordinates and convert directly to SVY21 since OneMap returns SVY21 coordinates for postal lookups. The outputs are two clean spatial objects: HDBresale_sf and primary_schools_sf, ready for Section 6 where we will engineer proximity and count variables (for example, 350 meters buffers and 1,000 meters counts) and integrate the amenity layers.\n\n5.1 Geocoding HDB resale transactions by addresses\nWe filter 4 room transactions in the study window, tidy street tokens for better matches, geocode each unique address with OneMap, cache results, join coordinates, build POINT geometry, reproject to EPSG 3414, validate, and save.\n\n5.1.1 Load and filter to study window and flat type\n\n# load resale table\nHDBresale_raw &lt;- readr::read_csv(\"data/aspatial/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv\")   # read CSV\n\n# filter to 4 room and window 2025 01 to 2025 09\nHDBresale &lt;- HDBresale_raw %&gt;%                           # start pipeline\n  dplyr::filter(flat_type == \"4 ROOM\") %&gt;%               # keep 4 room\n  dplyr::filter(month &gt;= \"2025-01\", month &lt;= \"2025-09\")  # keep study months\n\n\n\n5.1.2 Normalise street tokens\nLight token fixes improve geocoding hit-rate.\n\n# normalise street tokens for better match rate before calling OneMap\nHDBresale$street_name &lt;- gsub(\"ST\\\\.\", \"SAINT\", HDBresale$street_name)  # expand ST. to SAINT\n\n\n\n5.1.3 Define OneMap address geocoder with caching\nCall OneMap once per unique address; cache results to CSV.\n\n# function to call OneMap elastic search and return first LAT LON\ngeocode_addr &lt;- function(block, streetname) {                                       # define function\n  url &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"                          # endpoint\n  q &lt;- list(searchVal = paste(block, streetname),                                   # query text\n            returnGeom = \"Y\", getAddrDetails = \"N\", pageNum = \"1\")                  # parameters\n  txt &lt;- httr::content(httr::GET(url, query = q), as = \"text\", encoding = \"UTF-8\")  # request and read\n  js  &lt;- jsonlite::fromJSON(txt)                                                    # parse JSON\n  if (length(js$results) == 0)                                                      # case no result\n    return(tibble::tibble(LATITUDE = NA_real_, LONGITUDE = NA_real_))               # return NA coords\n  tibble::tibble(LATITUDE = as.numeric(js$results$LATITUDE[1]),                     # numeric lat\n                 LONGITUDE = as.numeric(js$results$LONGITUDE[1]))                   # numeric lon\n}\n\n# build or reload cache of unique address to coordinates\n# path to the cache file\ncache_path &lt;- \"data/aspatial/geocode_hdb_cache.csv\" # csv cache for address results\n\nif (file.exists(cache_path)) {                      # use cache if it exists\n  hdb_cache &lt;- readr::read_csv(cache_path, show_col_types = FALSE)   # load cached coordinates\n} else {                                            # otherwise build the cache now\n\n  uniq_addr &lt;- HDBresale %&gt;%                        # start from filtered HDB table\n    dplyr::distinct(block, street_name)             # keep unique block street pairs\n\n  pb &lt;- progress::progress_bar$new(total = nrow(uniq_addr)) # progress indicator\n\n  hdb_cache &lt;- uniq_addr %&gt;%                        # begin cache pipeline\n    dplyr::mutate(                                  # add a list column named geo\n      geo = purrr::map2(                            # call function for each pair\n        block,                                      # first argument is block\n        street_name,                                # second argument is street name\n        function(.x, .y) {                          # inline function wrapper\n          pb$tick()                                 # advance the progress bar\n          geocode_addr(.x, .y)                      # call OneMap geocoder and return tibble\n        }\n      )\n    ) %&gt;%                                           # end mutate and continue pipe\n    tidyr::unnest(cols = c(geo))                    # expand LATITUDE and LONGITUDE columns\n\n  readr::write_csv(hdb_cache, cache_path)           # save cache to disk for reuse\n}\n\n\n\n5.1.4 Enforce one record per address and join coordinates\nBind LATITUDE, LONGITUDE, convert to WGS84 points, then to SVY21 (TM).\n\n# keep one coordinate per address and ensure numeric\nhdb_cache_fixed &lt;- hdb_cache %&gt;%\n  dplyr::mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)) %&gt;%  # numeric\n  dplyr::filter(!is.na(LATITUDE), !is.na(LONGITUDE)) %&gt;%                                 # drop NA rows\n  dplyr::group_by(block, street_name) %&gt;% dplyr::slice_head(n = 1) %&gt;% dplyr::ungroup()  # one per pair\n\n# attach coordinates to filtered HDB table\nHDBresale_geo &lt;- HDBresale %&gt;%                                      # base table\n  dplyr::left_join(hdb_cache_fixed, by = c(\"block\",\"street_name\"))  # join LAT LON\n\n\n\n5.1.5 Create POINT geometry and validate\nVerify the geometry output by using glimpse().\n\n# build sf points in WGS 84 then transform to SVY21 EPSG 3414\nHDBresale_sf &lt;- HDBresale_geo %&gt;%\n  sf::st_as_sf(coords = c(\"LONGITUDE\",\"LATITUDE\"), crs = 4326) %&gt;%    # create POINT\n  sf::st_transform(3414)                                              # project to 3414\n\n# validate type and reference system\nst_crs(HDBresale_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n5.2 Geocoding schools by postcodes\nWe filter to PRIMARY level, standardise 6 digit postcodes, query OneMap by postcode to obtain X and Y in SVY21, keep one result per code, join to the table, convert to POINT, validate EPSG 3414, and save.\n\n5.2.1 Load and keep primary level\nRead the table and filter mainlevel_code == \"PRIMARY\".\n\n# read school table and keep primary level\nschools &lt;- readr::read_csv(\"data/aspatial/Generalinformationofschools.csv\", show_col_types = FALSE)\nprimary_schools &lt;- dplyr::filter(schools, mainlevel_code == \"PRIMARY\")                            \n\n# standardise 6 digit postcodes\nprimary_schools &lt;- primary_schools %&gt;%\n  dplyr::mutate(postal_code = stringr::str_pad(postal_code, 6, pad = \"0\"))  # pad\n\n\n\n5.2.2 OneMap postcode lookup and clean\n\n# query OneMap by postcode to get X Y in SVY21\nom_url &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"                              # endpoint\npostcodes &lt;- unique(primary_schools$postal_code)                                         # unique codes\nfound_xy &lt;- tibble::tibble()                                                             # holder\n\nfor (pc in postcodes) {                                                                  # loop codes\n  q &lt;- list(searchVal = pc, returnGeom = \"Y\", getAddrDetails = \"Y\", pageNum = \"1\")       # params\n  js &lt;- httr::content(httr::GET(om_url, query = q))                                      # request parse\n  if (!is.null(js$found) && js$found &gt; 0) {                                              # keep matches\n    df &lt;- as.data.frame(js$results, stringsAsFactors = FALSE)                            # to data frame\n    found_xy &lt;- dplyr::bind_rows(found_xy, tibble::as_tibble(df[, c(\"POSTAL\",\"X\",\"Y\")])) # keep fields\n  }\n}\n\n# coerce and keep one row per code\nfound_xy_clean &lt;- found_xy %&gt;%\n  dplyr::mutate(POSTAL = as.character(POSTAL), X = as.numeric(X), Y = as.numeric(Y)) %&gt;% # numeric\n  dplyr::filter(!is.na(X), !is.na(Y)) %&gt;%                                                # drop NA\n  dplyr::group_by(POSTAL) %&gt;% dplyr::slice_head(n = 1) %&gt;% dplyr::ungroup()              # one per code\n\n\n\n5.2.3 Join coordinates and build POINT geometry\n\n# attach X Y to school table\nprimary_schools_xy &lt;- primary_schools %&gt;%\n  dplyr::left_join(found_xy_clean, by = c(\"postal_code\" = \"POSTAL\"))                       # join coords\n\n# convert to sf POINT in SVY21\nprimary_schools_sf &lt;- sf::st_as_sf(primary_schools_xy, coords = c(\"X\",\"Y\"), crs = 3414)    # build POINT\n\n# validate geometry and reference system\nst_crs(primary_schools_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n5.3 Persist clean spatial tables\nWe save both sf objects to RDS for reuse. This avoids repeated API calls and guarantees consistent inputs for the next section.\n\n# save both objects for downstream steps\nreadr::write_rds(HDBresale_sf, \n                 \"data/rds/HDBresale_sf.rds\")       # geocoded HDB\nreadr::write_rds(primary_schools_sf, \n                 \"data/rds/primary_schools_sf.rds\") # geocoded schools"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#predictors-preprocessing-and-wrangling",
    "href": "Take-home_Ex03/take-home_ex03.html#predictors-preprocessing-and-wrangling",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "6 Predictors Preprocessing and Wrangling",
    "text": "6 Predictors Preprocessing and Wrangling\nThis section prepares every predictor that will feed into the modelling phase. We start from our geocoded HDBresale_sf which already contains POINT geometry in EPSG 3414 SVY21 meters and covers the window from 2025-01 to 2025-09. The structural predictors transform raw strings into numeric features that a model can use directly. Remaining lease is parsed into number of months. Storey range is converted into a single floor level by averaging the lower and upper level. Age of unit is computed from lease commence year against the reference year 2025. For location predictors we strictly follow the kick starter pattern. All target layers are POINTS in EPSG 3414. If a source is polygon or multipolygon we create a safe interior point with st_point_on_surface and drop any Z or M dimensions. For proximity we compute two styles when relevant. First, a continuous distance in meters to the nearest feature. Second, a within buffer count where the requirement asks for numbers within 350 meters or 1 000 meters. Every step below is short, repeatable, and fully commented for beginners.\n\n6.1 Structural predictors\n\n6.1.1 Remaining lease in months\nWe need a numeric predictor for the time left on each lease so that models can read it directly. The source field Remaining lease is plain text such as “56 years 03 months” or sometimes simply “52 years”. The code below does four things. First, it makes sure our HDB table is available as an sf POINT layer in EPSG 3414, loading the saved RDS if needed or building POINTs from latitude and longitude if that is what we currently have. Second, it validates that geometry is POINT and the reference system is SVY21 meters. Third, it parses the text into two integers, years and months, with missing months treated as zero. Fourth, it computes the final integer REMAINING_LEASE_MTH and drops the temporary helpers. The result is a tidy numeric feature that is ready for joins and modelling, and we also get a quick compact check that confirms type and a small sample.\n\n# ensure we have an sf POINT layer named HDBresale_sf in EPSG 3414\nif (!exists(\"HDBresale_sf\")) {                            # create if not present\n  if (file.exists(\"data/rds/HDBresale_sf.rds\")) {         # load saved object\n    HDBresale_sf &lt;- readRDS(\"data/rds/HDBresale_sf.rds\")  # read rds\n  } else if (exists(\"HDBresale_geo\")) {                   # else use geocoded table\n    HDBresale_sf &lt;- HDBresale_geo                         # assign geocoded\n  } else {                                                # nothing found\n    stop(\"HDBresale_sf was not found and no source is available\") # stop with message\n  }\n}\n\n# convert to sf if needed using LATITUDE and LONGITUDE then project to 3414\nif (!inherits(HDBresale_sf, \"sf\")) {      # not an sf yet\n  stopifnot(all(c(\"LONGITUDE\",\"LATITUDE\") %in% names(HDBresale_sf))) # require coords\n  HDBresale_sf &lt;- sf::st_as_sf(           # build geometry\n    HDBresale_sf,\n    coords = c(\"LONGITUDE\",\"LATITUDE\"),\n    crs = 4326                            # WGS 84 degrees\n  ) %&gt;% \n    sf::st_transform(3414)                # convert to SVY21 meters\n}\n\n# validate geometry type and reference system\nstopifnot(all(sf::st_geometry_type(HDBresale_sf) == \"POINT\")) # geometry must be POINT\nstopifnot(sf::st_crs(HDBresale_sf)$epsg == 3414)              # CRS must be 3414\n\n# helper that converts one text like \"56 years 03 months\" to total months\nparse_rl_to_months &lt;- function(x) {   # define helper\n  txt &lt;- tolower(x)                   # normalise case\n  yr  &lt;- suppressWarnings(as.integer(str_extract(txt, \"\\\\d+(?=\\\\s*year)\")))   # pull years\n  mo  &lt;- suppressWarnings(as.integer(str_extract(txt, \"\\\\d+(?=\\\\s*month)\")))  # pull months\n  yr  &lt;- ifelse(is.na(yr), 0L, yr)    # missing years to zero\n  mo  &lt;- ifelse(is.na(mo), 0L, mo)    # missing months to zero\n  12L * yr + mo                       # return total months\n}\n\n# compute the numeric feature\nHDBresale_sf &lt;- HDBresale_sf %&gt;%      # start transform\n  mutate(\n    REMAINING_LEASE_MTH = parse_rl_to_months(remaining_lease) # new integer field\n  )\n\n# compact checks and a quick peek\nstopifnot(is.numeric(HDBresale_sf$REMAINING_LEASE_MTH))       # must be numeric\ndplyr::glimpse(                                               # show a compact view\n  dplyr::select(HDBresale_sf, remaining_lease, REMAINING_LEASE_MTH, geometry)\n)\n\nRows: 8,679\nColumns: 3\n$ remaining_lease     &lt;chr&gt; \"56 years 03 months\", \"86 years 09 months\", \"56 ye…\n$ REMAINING_LEASE_MTH &lt;int&gt; 675, 1041, 674, 624, 1040, 624, 613, 614, 1039, 10…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3…\n\nhead(dplyr::select(HDBresale_sf, remaining_lease, REMAINING_LEASE_MTH), 5)  # first rows\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28566.05 ymin: 38360.76 xmax: 30036.29 ymax: 38861.93\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 5 × 3\n  remaining_lease    REMAINING_LEASE_MTH            geometry\n  &lt;chr&gt;                            &lt;int&gt;         &lt;POINT [m]&gt;\n1 56 years 03 months                 675 (30036.29 38360.76)\n2 86 years 09 months                1041 (29283.45 38517.37)\n3 56 years 02 months                 674 (30036.29 38360.76)\n4 52 years                           624 (28566.05 38861.93)\n5 86 years 08 months                1040 (29283.45 38517.37)\n\n\n\n\n6.1.2 Floor level from storey range\nWe average the two numbers inside storey_range. Non numeric and missing cases are handled safely and leave NA.\n\n# start from the existing sf table                                               \nstopifnot(inherits(HDBresale_sf, \"sf\"))                # must be an sf object   \nstopifnot(sf::st_crs(HDBresale_sf)$epsg == 3414)       # confirm EPSG 3414      \n\n# compute average floor level from ranges like \"04 TO 06\"                        \nHDBresale_sf &lt;- HDBresale_sf %&gt;%                      \n  dplyr::mutate(                                       \n    .rng = stringr::str_extract_all(storey_range, \"\\\\d+\"), # grab all numbers  \n    .low = suppressWarnings(as.integer(purrr::map_chr(.rng, dplyr::first))),    # first\n    .high = suppressWarnings(as.integer(purrr::map_chr(.rng, dplyr::last))),    # last \n    FLOOR_LEVEL = as.numeric(.low + .high) / 2                                  # average\n  ) %&gt;%                                                                          \n  dplyr::select(-.rng, -.low, -.high)                                           # drop helpers\n\n# quick checks                                                                    \nstopifnot(is.numeric(HDBresale_sf$FLOOR_LEVEL) | is.na(HDBresale_sf$FLOOR_LEVEL)) # numeric or NA\ndplyr::glimpse(dplyr::select(HDBresale_sf, storey_range, FLOOR_LEVEL, geometry))  # preview\n\nRows: 8,679\nColumns: 3\n$ storey_range &lt;chr&gt; \"13 TO 15\", \"10 TO 12\", \"10 TO 12\", \"07 TO 09\", \"28 TO 30…\n$ FLOOR_LEVEL  &lt;dbl&gt; 14, 11, 11, 8, 29, 5, 11, 5, 17, 17, 5, 8, 11, 5, 2, 11, …\n$ geometry     &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 38517.37…\n\n\n\n\n6.1.3 Age of the unit in years\nThe compute Age as 2025 minus lease_commence_date. Negative or missing values are set to NA.\n\n# compute Age using the stated analysis year                                      \nanalysis_year &lt;- 2025              # set current analysis year\nHDBresale_sf &lt;- HDBresale_sf %&gt;%   # continue same table      \n  dplyr::mutate(                                     \n    AGE_2025 = suppressWarnings(   # new numeric field        \n      as.numeric(analysis_year - as.integer(lease_commence_date))                \n    ),                                                                          \n    AGE_2025 = dplyr::if_else(AGE_2025 &gt;= 0, AGE_2025, as.numeric(NA))      # guard\n  )\n\n# validate                                                                         \nstopifnot(is.numeric(HDBresale_sf$AGE_2025) | is.na(HDBresale_sf$AGE_2025)) # numeric or NA\ndplyr::glimpse(dplyr::select(HDBresale_sf, lease_commence_date, AGE_2025, geometry)) # preview\n\nRows: 8,679\nColumns: 3\n$ lease_commence_date &lt;dbl&gt; 1982, 2012, 1982, 1978, 2012, 1978, 1977, 1977, 20…\n$ AGE_2025            &lt;dbl&gt; 43, 13, 43, 47, 13, 47, 48, 48, 13, 13, 49, 49, 47…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3…\n\n\n\n\n6.1.4 Structural predictors ready\nAt this point, our structural predictors are present on HDBresale_sf as REMAINING_LEASE_MTH, FLOOR_LEVEL, and AGE_2025.\n\n# final compact view of structural fields                                          \ndplyr::glimpse(dplyr::select(HDBresale_sf, \n                             REMAINING_LEASE_MTH, FLOOR_LEVEL, AGE_2025, geometry))\n\nRows: 8,679\nColumns: 4\n$ REMAINING_LEASE_MTH &lt;int&gt; 675, 1041, 674, 624, 1040, 624, 613, 614, 1039, 10…\n$ FLOOR_LEVEL         &lt;dbl&gt; 14, 11, 11, 8, 29, 5, 11, 5, 17, 17, 5, 8, 11, 5, …\n$ AGE_2025            &lt;dbl&gt; 43, 13, 43, 47, 13, 47, 48, 48, 13, 13, 49, 49, 47…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3…\n\n\n\n\n\n6.2 Preparing target layers for location factors\nAll location sources must be POINTS in EPSG 3414. We create a function that reads any vector file, sets CRS if missing, projects to 3414, converts polygons to safe interior points, and drops Z or M if present. We then prepare the exact layers as listed: MRT stations as GeoJSON, eldercare, hawker centres, parks, kindergartens, childcare centres, supermarkets, bus stops, primary schools, and shopping malls. Primary schools use the geocoded POINTS saved in Section 5. For elite schools we filter a named list. For counts we will reuse the prepared primary schools for the one kilometer radius. For CBD we read a user supplied vector file and convert to a single point using the interior point of the union. If a file is missing the code stops with a clear message so that we can place the file before continuing.\n\n6.2.1 Function and global checks\nBefore preparing any single layer, we define a function that reads a vector dataset, normalises its coordinate reference system, and guarantees a POINT result. This keeps every later subsection short and predictable. The function accepts common formats such as GeoJSON, Shapefile, KML, and GPKG through sf::st_read. If a source comes in with Z or M dimensions, those are dropped since our analysis is two dimensional in meters. If the input geometry is not a POINT, we safely convert to an interior point which always falls inside each feature. Finally we project to EPSG 3414 so distances are in meters. We also confirm our HDB layer is an sf POINT in EPSG 3414.\n\n# Function to read any vector file and return POINTS in EPSG 3414\nread_as_points_3414 &lt;- function(path, assume_epsg = NULL) {\n  stopifnot(file.exists(path))\n  g &lt;- sf::st_read(path, quiet = TRUE)\n\n  # if CRS is missing, set it without transforming\n  if (is.na(sf::st_crs(g)$epsg)) {\n    stopifnot(!is.null(assume_epsg))         # we must be told what it is\n    g &lt;- sf::st_set_crs(g, assume_epsg)      # &lt;— use st_set_crs(), no warning\n  }\n\n  g &lt;- sf::st_transform(g, 3414)             # project to meters\n  g &lt;- sf::st_zm(g, drop = TRUE, what = \"ZM\")# drop Z/M if present\n\n  if (!all(sf::st_geometry_type(g) == \"POINT\")) {\n    g &lt;- sf::st_point_on_surface(g) |&gt;       # safe interior point\n         sf::st_as_sf(crs = 3414)\n  }\n  g\n}\n\n# Function for CSVs with lon and lat columns\nread_csv_points_3414 = function(path, lon, lat, crs_in = 4326) {\n  stopifnot(file.exists(path))                              # csv must exist\n  df = readr::read_csv(path, show_col_types = FALSE)        # read csv\n  stopifnot(all(c(lon, lat) %in% names(df)))                # require columns\n  g = sf::st_as_sf(df, coords = c(lon, lat), crs = crs_in)  # build points\n  g = sf::st_transform(g, 3414)                             # project to meters\n  g                                                         # return points\n}\n\n# confirm HDB layer is ready\nstopifnot(inherits(HDBresale_sf, \"sf\"))                       # must be sf\nstopifnot(all(sf::st_geometry_type(HDBresale_sf) == \"POINT\")) # must be points\nstopifnot(sf::st_crs(HDBresale_sf)$epsg == 3414)              # must be 3414\n\n\n\n6.2.2 MRT station exits from GeoJSON\nTransit proximity is a key location factor. Our Table 1 in Section 3 states that the MRT station source is GeoJSON. We therefore load the MRT exits GeoJSON, discard any elevation or measure values, and ensure the coordinates are in EPSG 3414 so that all nearest distance calculations return values in meters. A simple glimpse() confirms that the result contains only POINT geometry.\n\n# read MRT exits GeoJSON and prepare POINTS in 3414\nmrt_pt = read_as_points_3414(\"data/geospatial/LTAMRTStationExitGEOJSON.geojson\")  # read and convert\nstopifnot(inherits(mrt_pt, \"sf\"))                         # must be sf\nstopifnot(all(sf::st_geometry_type(mrt_pt) == \"POINT\"))   # must be points\nstopifnot(sf::st_crs(mrt_pt)$epsg == 3414)                # meters\n\n# Quick check and validation\nglimpse(mrt_pt)\n\nRows: 597\nColumns: 5\n$ OBJECTID   &lt;int&gt; 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 146…\n$ EXIT_CODE  &lt;chr&gt; \"Exit A\", \"Exit A\", \"Exit A\", \"Exit A\", \"Exit A\", \"Exit A\",…\n$ INC_CRC    &lt;chr&gt; \"45CDB0D9BDC73079\", \"54BFE6EB924CBA6F\", \"A324402CF624723A\",…\n$ FMEL_UPD_D &lt;chr&gt; \"20231019120510\", \"20231019120510\", \"20231019120510\", \"2023…\n$ geometry   &lt;POINT [m]&gt; POINT (20361.05 40974.54), POINT (20887.79 41082.76),…\n\n\n\n\n6.2.3 Primary schools and elite subset\nElite school proximity uses the same school geocodes we already created in Section 5. We load the prepared RDS primary_schools_sf.rds, confirm EPSG 3414 and POINT geometry, then build a filtered view containing only the 20 elite schools specified. This subset will be used to compute continuous distance to the nearest elite school point and also optional counts within buffers if needed. Keeping both the full set and the elite subset in memory gives flexibility for later features such as count of any primary school within one kilometer and distance to the nearest elite school. A quick print confirms the expected number of rows.\n\n# load prepared primary schools points from our RDS\nprimary_schools_sf = readr::read_rds(\"data/rds/primary_schools_sf.rds\") # load points\nstopifnot(inherits(primary_schools_sf, \"sf\"))                           # must be sf\nstopifnot(all(sf::st_geometry_type(primary_schools_sf) == \"POINT\"))     # must be points\nstopifnot(sf::st_crs(primary_schools_sf)$epsg == 3414)                  # meters\n\n# filter to elite schools list\nelite_names = c(\n  \"NANYANG PRIMARY SCHOOL\",\"TAO NAN SCHOOL\",\"HONG WEN SCHOOL\",\"NAN HUA PRIMARY SCHOOL\",\n  \"ST. HILDA'S PRIMARY SCHOOL\",\"HENRY PARK PRIMARY SCHOOL\",\"ANGLO-CHINESE SCHOOL (PRIMARY)\",\n  \"RAFFLES GIRLS' PRIMARY SCHOOL\",\"PEI HWA PRESBYTERIAN PRIMARY SCHOOL\",\"RADIN MAS PRIMARY SCHOOL\",\n  \"ROSYTH SCHOOL\",\"KONG HWA SCHOOL\",\"POI CHING SCHOOL\",\"HOLY INNOCENTS' PRIMARY SCHOOL\",\n  \"AI TONG SCHOOL\",\"RED SWASTIKA SCHOOL\",\"MAHA BODHI SCHOOL\",\"RULANG PRIMARY SCHOOL\",\n  \"PEI CHUN PUBLIC SCHOOL\",\"SINGAPORE CHINESE GIRLS' PRIMARY SCHOOL\"\n)\n\nelite_primary_sf = primary_schools_sf %&gt;%\n  dplyr::filter(school_name %in% elite_names)   # keep elite only\n\nnrow(elite_primary_sf)                          # confirm row count\n\n[1] 20\n\n\n\n\n6.2.4 Kindergartens GeoJSON\nCounts of kindergartens within a local radius offer a proxy for family oriented amenities. The dataset in the project folder is a GeoJSON. We read it with the helper, which ensures every record becomes a POINT in EPSG 3414 with Z and M removed. We store the prepared layer as kindergartens_pt. This layer will be used to compute the number of locations inside a radius around each HDB resale point, for example within 350 meters. The preparation step does not aggregate or filter because the raw count requires the complete set. A short check confirms geometry and projection are correct.\n\n# read kindergartens as points\nkindergartens_pt = read_as_points_3414(\"data/geospatial/Kindergartens.geojson\") # prepare layer\nstopifnot(all(sf::st_geometry_type(kindergartens_pt) == \"POINT\"))   # confirm points\n\n\n\n6.2.5 Childcare centres GeoJSON\nChildcare centres will be counted within a three hundred and fifty meter radius per HDB resale observation. The input format is GeoJSON according to our directory. We load the file with the helper so that geometry is POINT, projection is SVY21 meters, and extra dimensions are removed. We keep the data as is without filters, since the later count requires complete coverage. A simple glimpse() appears after the code so that we can verify the feature count and the geometry column. This prepared layer is named childcare_pt and is used later to compute the required counts column.\n\n# read childcare centres as points\nchildcare_pt = read_as_points_3414(\"data/geospatial/ChildCareServices.geojson\") # prepare layer\nstopifnot(all(sf::st_geometry_type(childcare_pt) == \"POINT\"))                   # confirm points\n\n\n\n6.2.6 Bus stops Shapefile\nWe will read the feature class BusStop.shp. Shapefiles often carry mixed attributes and sometimes include Z or M. The helper removes those and converts any non point geometry to a stable interior point. After projection to EPSG 3414, the prepared layer supports both nearest distance and counts within a three hundred and fifty meter radius. This is important because bus networks are dense and small geometric errors can change counts near buffer edges. The result is stored as busstop_pt.\n\nbusstop_pt &lt;- read_as_points_3414(\n  \"data/geospatial/BusStopLocation_Aug2025/BusStop.shp\",\n  assume_epsg = 3414\n)\n\nstopifnot(inherits(busstop_pt, \"sf\"))\nstopifnot(all(sf::st_geometry_type(busstop_pt) == \"POINT\"))\nstopifnot(sf::st_crs(busstop_pt)$epsg == 3414)\n\ndplyr::glimpse(dplyr::select(busstop_pt, geometry))\n\nRows: 5,172\nColumns: 1\n$ geometry &lt;POINT [m]&gt; POINT (35565.66 41659.52), POINT (21439.91 31253.63), P…\n\n\n\n\n6.2.7 Eldercare GeoJSON\nEldercare proximity measures access to aged care services. The source file is GeoJSON. We treat it the same way as the previous categories. The prepared layer is eldercare_pt which holds POINT geometry in meters. Later sections will compute continuous nearest distance from each HDB resale observation to the nearest eldercare site. That feature will be named following the pattern we set earlier. The code below ensures that the geometry types are consistent which helps prevent confusing errors during distance calculations or spatial joins.\n\n# read eldercare services as points\neldercare_pt = read_as_points_3414(\"data/geospatial/EldercareServices.geojson\")     # prepare layer\nstopifnot(all(sf::st_geometry_type(eldercare_pt) == \"POINT\"))                       # confirm points\n\n\n\n6.2.8 Hawker centres GeoJSON\nFood access through hawker centres is an essential amenity. The input is GeoJSON. We load and normalise with the helper and hold the result as POINTs in EPSG 3414. This prepared layer will support the continuous distance feature, usually named proximity to hawker. Because later analysis may also examine counts of food places within a given radius, we keep every record without filtering. The checks at the bottom confirm geometry and projection which keeps later computations safe and repeatable.\n\n# read hawker centres as points\nhawker_pt = read_as_points_3414(\"data/geospatial/HawkerCentresGEOJSON.geojson\")    # prepare layer\nstopifnot(all(sf::st_geometry_type(hawker_pt) == \"POINT\"))                         # confirm points\n\n\n\n6.2.9 Parks GeoJSON\nGreen space proximity is derived from the parks layer. Some parks are polygons. The helper converts these to interior points so that nearest distance behaves as expected even for large irregular shapes. The layer is stored in meters which allows clean thresholds if we decide to add a within buffer count later. The code below reads the file, converts geometry if necessary, and performs simple validity checks. The prepared object is named parks_pt.\n\n# read parks and convert to points\nparks_pt = read_as_points_3414(\"data/geospatial/Parks.geojson\")   # prepare layer\nstopifnot(all(sf::st_geometry_type(parks_pt) == \"POINT\"))         # confirm points\n\n\n\n6.2.10 Shopping malls CSV\nWe read the CSV and build POINT geometry from those two columns. The default source reference for such files is usually WGS84, so we assign EPSG 4326 and then project to EPSG 3414. After preparation, nearest distance and within buffer counts are straightforward. The code names the resulting object mall_pt and runs the same geometry and projection checks as the other sections so everything remains consistent across features.\n\n# read shopping mall coordinates from CSV and build points\nmall_pt = read_csv_points_3414(\n  path = \"data/geospatial/shopping_mall_coordinates.csv\",    # csv path\n  lon  = \"LONGITUDE\",                                        # name of longitude column\n  lat  = \"LATITUDE\",                                         # name of latitude column\n  crs_in = 4326                                              # source is WGS84\n)\nstopifnot(all(sf::st_geometry_type(mall_pt) == \"POINT\"))     # confirm points\nstopifnot(sf::st_crs(mall_pt)$epsg == 3414)                  # meters\n\n\n\n6.2.11 Supermarkets GeoJSON\nSupermarket coverage is prepared exactly like hawker and eldercare. We use the helper to read the GeoJSON, transform to meters, and convert any footprint polygons to interior points. The resulting layer supermarket_pt supports both continuous distance and within buffer counts if needed. As with all other layers, we store the prepared set without additional filters so feature engineering in later sections can proceed without surprises. The code includes quick checks to ensure geometry and projection are correct.\n\n# read supermarkets as points\nsupermarket_pt = read_as_points_3414(\"data/geospatial/SupermarketsGEOJSON.geojson\") # prepare\nstopifnot(all(sf::st_geometry_type(supermarket_pt) == \"POINT\"))                     # confirm points\n\n\n\n6.2.12 CBD point from Master Plan subzones\nCBD proximity uses the DOWNTOWN CORE planning area from the master plan subzones we prepared earlier. We load mpsz_cl.rds, filter to DOWNTOWN CORE, dissolve to a single geometry, and convert the polygon to a safe interior point. This produces a single POINT named cbd_pt in EPSG 3414. Later sections compute the straight line distance from each HDB resale observation to this point and store that feature in meters. The conversion to an interior point avoids centroid outside issues for complex shapes, which keeps distances stable.\n\n# load master plan subzones and create a CBD reference point\nmpsz = readr::read_rds(\"data/rds/mpsz_cl.rds\")           # load polygons\nstopifnot(sf::st_crs(mpsz)$epsg == 3414)                 # meters\n\ncbd_poly = mpsz %&gt;%\n  dplyr::filter(PLN_AREA_N == \"DOWNTOWN CORE\") %&gt;%       # select area\n  sf::st_union()                                         # dissolve\n\ncbd_pt = cbd_poly %&gt;%\n  sf::st_point_on_surface() %&gt;%                          # interior point\n  sf::st_as_sf(crs = 3414)                               # as sf with crs\n\nstopifnot(all(sf::st_geometry_type(cbd_pt) == \"POINT\"))  # confirm point\n\n\n\n6.2.13 Save prepared layers for repeatable use\nTo make later sections fast and deterministic, we save each prepared POINT layer as an RDS file under the data rds folder. This avoids re reading and re projecting each time the notebook runs. The saved objects are ready for joins, distance computation, and within buffer counts. The names follow a clear pattern so that we can reload any single layer by name. If we update a source file in the future, simply rerun the corresponding subsection and the saved RDS will refresh.\n\n# save every prepared layer to RDS for quick reuse\nreadr::write_rds(mrt_pt,            \"data/rds/mrt_pt.rds\")\nreadr::write_rds(primary_schools_sf,\"data/rds/primary_schools_pt.rds\")\nreadr::write_rds(elite_primary_sf,  \"data/rds/elite_primary_pt.rds\")\nreadr::write_rds(kindergartens_pt,  \"data/rds/kindergartens_pt.rds\")\nreadr::write_rds(childcare_pt,      \"data/rds/childcare_pt.rds\")\nreadr::write_rds(busstop_pt,        \"data/rds/busstop_pt.rds\")\nreadr::write_rds(eldercare_pt,      \"data/rds/eldercare_pt.rds\")\nreadr::write_rds(hawker_pt,         \"data/rds/hawker_pt.rds\")\nreadr::write_rds(parks_pt,          \"data/rds/parks_pt.rds\")\nreadr::write_rds(mall_pt,           \"data/rds/mall_pt.rds\")\nreadr::write_rds(supermarket_pt,    \"data/rds/supermarket_pt.rds\")\nreadr::write_rds(cbd_pt,            \"data/rds/cbd_pt.rds\")\n\n\n\n\n6.3 Location predictors\nWe compute exactly the predictors requested. Distances are nearest neighbour distances in meters using st_nearest_feature and st_distance with by element equal TRUE. Counts use st_is_within_distance with the required radius - We use names CNT_350M_KINDER, CNT_350M_CHILDCARE, CNT_350M_BUS, and CNT_1KM_PRIMARY for clarity. Diagnostics verify that the new fields are in the correct type and that the layer remains POINT in EPSG 3414. At the end, we show a compact glimpse of the new columns and write an updated RDS so later sections can load the prepared table immediately.\n\n6.3.1 Helpers used by all location predictors\nTo keep every later step short, reliable, and readable, we first define two tiny helpers. The first computes a nearest distance from every HDB point to the closest target point. It uses a nearest neighbour index so the work scales well even with several thousand features. The function guards that both layers are in the same projected system so the output is in meters. The second helper counts how many target points fall inside a circular buffer around each HDB point. It uses a fast predicate that returns logical relations, then converts those relations into simple integer counts. With these two helpers in place, every later subsection is a one or two line call that is easy to audit and reproduce.\n\n# all layers must be POINT and EPSG 3414 before we call these\n\nnearest_distance_m &lt;- function(from_pt, to_pt) {\n  stopifnot(sf::st_crs(from_pt)$epsg == 3414, sf::st_crs(to_pt)$epsg == 3414)\n  idx &lt;- sf::st_nearest_feature(from_pt, to_pt)    # index of nearest target\n  d   &lt;- sf::st_distance(from_pt, to_pt[idx, ], by_element = TRUE)\n  as.numeric(d)                                    # meters\n}\n\ncount_within_m &lt;- function(from_pt, to_pt, radius_m) {\n  stopifnot(sf::st_crs(from_pt)$epsg == 3414, sf::st_crs(to_pt)$epsg == 3414)\n  rel &lt;- sf::st_is_within_distance(from_pt, to_pt, dist = radius_m)\n  vapply(rel, length, integer(1))                  # integer counts\n}\n\n# convenience: load all prepared target layers once\nmrt_pt          &lt;- readr::read_rds(\"data/rds/mrt_pt.rds\")\nparks_pt        &lt;- readr::read_rds(\"data/rds/parks_pt.rds\")\neldercare_pt    &lt;- readr::read_rds(\"data/rds/eldercare_pt.rds\")\nhawker_pt       &lt;- readr::read_rds(\"data/rds/hawker_pt.rds\")\nsupermarket_pt  &lt;- readr::read_rds(\"data/rds/supermarket_pt.rds\")\nmall_pt         &lt;- readr::read_rds(\"data/rds/mall_pt.rds\")\nkindergartens_pt&lt;- readr::read_rds(\"data/rds/kindergartens_pt.rds\")\nchildcare_pt    &lt;- readr::read_rds(\"data/rds/childcare_pt.rds\")\nbusstop_pt      &lt;- readr::read_rds(\"data/rds/busstop_pt.rds\")\nprimary_pt      &lt;- readr::read_rds(\"data/rds/primary_schools_pt.rds\")\nelite_primary_pt&lt;- readr::read_rds(\"data/rds/elite_primary_pt.rds\")\ncbd_pt          &lt;- readr::read_rds(\"data/rds/cbd_pt.rds\")\n\n\n\n6.3.2 Proximity to MRT stations\nAccess to rail transit is a well known driver of residential prices in compact cities. Distance to the nearest station approximates expected walk time, first mile effort, and overall transit convenience. We compute a single continuous feature in meters from each flat to its closest MRT exit. Using exits rather than line segments avoids ambiguity from long station footprints and guarantees a sensible origin for walking access. Values near zero mean the block is beside an exit, and larger values reflect locations that require a longer walk or an intermediate bus connection. This continuous metric works well in both global and local models and is straightforward to interpret.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_MRT_M = nearest_distance_m(geometry, mrt_pt))\n\n# Quick check and inspection \nglimpse(HDBresale_sf$DIST_MRT_M)\n\n num [1:8679] 718 731 718 384 731 ...\n\n\n\n\n6.3.3 Proximity to parks\nGreen space can influence prices by improving neighborhood comfort, providing recreation, and raising perceived environmental quality. We treat the park layer as a set of access points. For polygonal parks, we already converted to interior points in the preparation step to avoid edges and holes. We now compute a continuous metric in meters from each flat to the nearest park access point. The interpretation is similar to the transit feature. Smaller values indicate ready access to open space which can be especially valuable in dense estates. The feature is robust since it uses a direct nearest neighbour distance in a projected meter system.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_PARK_M = nearest_distance_m(geometry, parks_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_PARK_M)\n\n num [1:8679] 426 636 426 356 636 ...\n\n\n\n\n6.3.4 Proximity to eldercare services\nEldercare access matters to multi-generational households and can be reflected in housing choices. We measure straight line distance from each transaction to the closest registered eldercare service. While travel time would be ideal, Euclidean distance in a small city is a strong proxy once layers are in a meter based projection. The feature can capture both convenience and localized supply. In later diagnostics, we can examine whether this variable has stronger effects in towns with older blocks or where family structures produce higher demand for caregiving services.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_ELDERCARE_M = nearest_distance_m(geometry, eldercare_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_ELDERCARE_M)\n\n num [1:8679] 260 255 260 398 255 ...\n\n\n\n\n6.3.5 Proximity to hawker centres\nFood access is a daily need. Hawker centres provide affordable options and are commonly used reference points for walkable amenities. We add a continuous distance in meters to the nearest hawker centre point. This variable complements the supermarket feature because the two cover different price tiers and operating hours. If we wish to explore threshold effects later, we can also build indicator variables for within a short walking distance. For now, the continuous measure keeps models simple and comparable across towns.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_HAWKER_M = nearest_distance_m(geometry, hawker_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_HAWKER_M)\n\n num [1:8679] 389 381 389 132 381 ...\n\n\n\n\n6.3.6 Proximity to supermarkets\nSupermarkets indicate stable food supply and often co locate with other services such as pharmacies and financial services. We compute the distance to the nearest supermarket point. The variable captures everyday convenience that families value when selecting a flat. It can also proxy for local retail vitality. Be aware that this feature sometimes correlates with shopping mall distance in central areas, so we will check variance inflation later. For now we keep it as a straightforward continuous measure that is easy to explain to a non technical audience.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_SUPERMKT_M = nearest_distance_m(geometry, supermarket_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_SUPERMKT_M)\n\n num [1:8679] 285 314 285 186 314 ...\n\n\n\n\n6.3.7 Proximity to shopping malls\nRegional and town centre malls bundle retail, dining, and services in one place. Proximity can raise perceived convenience and social activity. Our mall layer comes from a table with latitude and longitude that was converted to points and projected to meters earlier. We now compute a single nearest distance feature. Remember that this is a straight line proxy. If we later have access to a street network, the same helper can be swapped for a travel time routine without changing the structure of the model script.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_MALL_M = nearest_distance_m(geometry, mall_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_MALL_M)\n\n num [1:8679] 756 652 756 960 652 ...\n\n\n\n\n6.3.8 Proximity to the CBD reference point\nThe central business district concentrates high wage employment and premium services. Distance to that centre is often a strong global gradient in price models. we created a stable CBD reference by dissolving the Downtown Core planning area and converting it to a safe interior point. We now measure a continuous distance from every flat to this single reference. The value helps the model separate broad city wide trends from local amenity effects. In later maps of local coefficients, we can observe where the CBD gradient weakens or strengthens relative to neighbourhood factors.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_CBD_M = nearest_distance_m(geometry, cbd_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_CBD_M)\n\n num [1:8679] 8486 8718 8486 9185 8718 ...\n\n\n\n\n6.3.9 Proximity to elite primary schools\nSchool prestige can affect price expectations for families with young children. We already derived an elite subset with twenty schools from the full primary list. We compute a continuous distance to the nearest elite school point. This gives a clear and interpretable number in meters that can be compared across towns. In later robustness checks, we may also include the distance to the nearest ordinary primary school to differentiate general access from prestige access. For now we focus on the prestige signal since that is more likely to be reflected in marginal willingness to pay.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(DIST_ELITEPRI_M = nearest_distance_m(geometry, elite_primary_pt))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$DIST_ELITEPRI_M)\n\n num [1:8679] 2090 1390 2090 992 1390 ...\n\n\n\n\n6.3.10 Amenity counts within 350 meters\nCounts within a short walking radius capture micro scale density of child related and transport services. We build three integers. The first is the number of kindergartens within 350 meters. The second is the number of childcare centres within the same radius. The third is the number of bus stops within 350 meters which acts as a proxy for feeder service availability. These are simple and robust indicators that often improve model fit by capturing local intensity rather than only the nearest distance. Because all layers are already in EPSG 3414 meters, the buffer threshold is exact and reproducible.\n\nRADIUS_SHORT &lt;- 350\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(\n    CNT_KINDER_350 = count_within_m(geometry, kindergartens_pt, RADIUS_SHORT),\n    CNT_CHILD_350  = count_within_m(geometry, childcare_pt,     RADIUS_SHORT),\n    CNT_BUS_350    = count_within_m(geometry, busstop_pt,       RADIUS_SHORT)\n  )\n\n# Quick check and inspection\nglimpse(HDBresale_sf$CNT_KINDER_350)\n\n int [1:8679] 1 1 1 2 1 2 1 1 0 0 ...\n\nglimpse(HDBresale_sf$CNT_CHILD_350)\n\n int [1:8679] 3 5 3 5 5 5 5 7 2 2 ...\n\nglimpse(HDBresale_sf$CNT_BUS_350)\n\n int [1:8679] 7 4 7 7 4 7 6 7 5 5 ...\n\n\n\n\n6.3.11 Primary schools within 1 kilometer\nGeneral access to basic education is best represented by the number of primary schools within a larger but still local catchment. We therefore compute an integer count of primary schools within one thousand meters of each flat. This measure captures broader choice sets and helps control for areas where many schools cluster. It complements the elite distance feature by distinguishing overall supply from prestige proximity. The value is easy to interpret in policy terms since it reads as the number of nearby schools reachable by a short ride or longer walk.\n\nHDBresale_sf &lt;- HDBresale_sf %&gt;%\n  dplyr::mutate(CNT_PRIMARY_1000 = count_within_m(geometry, primary_pt, 1000))\n\n# Quick check and inspection\nglimpse(HDBresale_sf$CNT_PRIMARY_1000)\n\n int [1:8679] 3 2 3 2 2 2 3 3 2 2 ...\n\n\n\n\n6.3.12 Quick compact check\nAs a final sanity step for this section, we print a compact view of the new fields. This confirms types, units, and a few example values.\n\ndplyr::glimpse(\n  dplyr::select(\n    HDBresale_sf,\n    DIST_MRT_M, DIST_PARK_M, DIST_ELDERCARE_M, DIST_HAWKER_M,\n    DIST_SUPERMKT_M, DIST_MALL_M, DIST_CBD_M, DIST_ELITEPRI_M,\n    CNT_KINDER_350, CNT_CHILD_350, CNT_BUS_350, CNT_PRIMARY_1000, geometry\n  )\n)\n\nRows: 8,679\nColumns: 13\n$ DIST_MRT_M       &lt;dbl&gt; 717.8940, 731.4224, 717.8940, 383.6496, 731.4224, 383…\n$ DIST_PARK_M      &lt;dbl&gt; 425.5483, 635.8962, 425.5483, 356.4783, 635.8962, 356…\n$ DIST_ELDERCARE_M &lt;dbl&gt; 259.7684, 254.9544, 259.7684, 397.6108, 254.9544, 397…\n$ DIST_HAWKER_M    &lt;dbl&gt; 388.7647, 381.4729, 388.7647, 132.0052, 381.4729, 132…\n$ DIST_SUPERMKT_M  &lt;dbl&gt; 284.9091, 314.4801, 284.9091, 186.2702, 314.4801, 186…\n$ DIST_MALL_M      &lt;dbl&gt; 756.2998, 652.1127, 756.2998, 959.9891, 652.1127, 959…\n$ DIST_CBD_M       &lt;dbl&gt; 8485.514, 8717.684, 8485.514, 9185.376, 8717.684, 918…\n$ DIST_ELITEPRI_M  &lt;dbl&gt; 2089.5384, 1389.9549, 2089.5384, 991.5717, 1389.9549,…\n$ CNT_KINDER_350   &lt;int&gt; 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1,…\n$ CNT_CHILD_350    &lt;int&gt; 3, 5, 3, 5, 5, 5, 5, 7, 2, 2, 2, 2, 4, 6, 5, 5, 5, 3,…\n$ CNT_BUS_350      &lt;int&gt; 7, 4, 7, 7, 4, 7, 6, 7, 5, 5, 5, 5, 6, 4, 4, 4, 7, 6,…\n$ CNT_PRIMARY_1000 &lt;int&gt; 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3,…\n$ geometry         &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3851…"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#exploratory-data-analysis-eda",
    "href": "Take-home_Ex03/take-home_ex03.html#exploratory-data-analysis-eda",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "7 Exploratory Data Analysis (EDA)",
    "text": "7 Exploratory Data Analysis (EDA)\n\n7.1 Build an analysis table\nBefore exploring, we create one tidy table that holds the dependent variable and the predictors we prepared earlier. Doing this once prevents repeated joins and keeps the later code short and reliable. We keep only 4-room transactions as per our study scope and confirm that the layer is a simple feature point layer in the meter based EPSG 3414 system. We also add a log transform of resale price to reduce skew and to make linear patterns easier to see in scatter plots and in correlation summaries. Finally, we collect all numeric predictors into a vector so that later summary loops are compact and easy to audit.\n\n# keep only four room transactions and select fields used in Section 6\nanalysis_sf &lt;- HDBresale_sf %&gt;%               # start from the prepared layer\n  filter(flat_type == \"4 ROOM\") %&gt;%           # enforce study scope\n  mutate(LOG_PRICE = log(resale_price)) %&gt;%   # add log price for normality\n  select(                                     # keep only fields we will explore\n    LOG_PRICE,\n    resale_price,\n    FLOOR_LEVEL,\n    REMAINING_LEASE_MTH,\n    AGE_2025,\n    DIST_CBD_M,\n    DIST_MRT_M,\n    DIST_PARK_M,\n    DIST_ELDERCARE_M,\n    DIST_HAWKER_M,\n    DIST_SUPERMKT_M,\n    DIST_MALL_M,\n    DIST_ELITEPRI_M,\n    CNT_KINDER_350,\n    CNT_CHILD_350,\n    CNT_BUS_350,\n    CNT_PRIMARY_1000,\n    geometry\n  ) %&gt;%\n  st_as_sf()                                  # keep sf class explicitly\n\n# quick integrity checks\nstopifnot(inherits(analysis_sf, \"sf\"))        # must be simple features\nstopifnot(st_crs(analysis_sf)$epsg == 3414)   # must be in meters\n\n# a handy vector of numeric predictor names for loops\nnum_vars &lt;- c(\n  \"FLOOR_LEVEL\",\"REMAINING_LEASE_MTH\",\"AGE_2025\",\n  \"DIST_CBD_M\",\"DIST_MRT_M\",\"DIST_PARK_M\",\"DIST_ELDERCARE_M\",\n  \"DIST_HAWKER_M\",\"DIST_SUPERMKT_M\",\"DIST_MALL_M\",\"DIST_ELITEPRI_M\",\n  \"CNT_KINDER_350\",\"CNT_CHILD_350\",\"CNT_BUS_350\",\"CNT_PRIMARY_1000\"\n)\n\n\n\n7.2 Univariate summaries and distributions\nWe begin with very simple questions. What ranges do we see for price and for each predictor. Are there obvious outliers that are likely to be data errors. Are the distributions symmetric or strongly skewed. The following code prints summary statistics for price and every predictor, then draws histograms with a shared theme so that we can visually scan shapes and tails quickly. Using the log of price is helpful because it removes much of the right tail and makes interpretation of percentage changes easier later. This step often reveals small cleaning tasks before modelling, so it is worth doing carefully.\n\n# print compact summaries for price and predictors\nsummary(select(st_drop_geometry(analysis_sf), LOG_PRICE, all_of(num_vars)))   # plain stats for a quick scan\n\n   LOG_PRICE      FLOOR_LEVEL    REMAINING_LEASE_MTH    AGE_2025    \n Min.   :12.61   Min.   : 2.00   Min.   : 485.0      Min.   : 4.00  \n 1st Qu.:13.24   1st Qu.: 5.00   1st Qu.: 750.0      1st Qu.: 9.00  \n Median :13.35   Median : 8.00   Median : 910.0      Median :23.00  \n Mean   :13.39   Mean   : 9.29   Mean   : 916.1      Mean   :22.85  \n 3rd Qu.:13.50   3rd Qu.:11.00   3rd Qu.:1080.0      3rd Qu.:37.00  \n Max.   :14.23   Max.   :50.00   Max.   :1144.0      Max.   :58.00  \n   DIST_CBD_M      DIST_MRT_M       DIST_PARK_M      DIST_ELDERCARE_M\n Min.   : 1023   Min.   :  21.79   Min.   :  63.81   Min.   :   0.0  \n 1st Qu.: 9881   1st Qu.: 286.93   1st Qu.: 458.13   1st Qu.: 339.8  \n Median :13202   Median : 519.71   Median : 684.68   Median : 663.4  \n Mean   :12516   Mean   : 586.66   Mean   : 758.61   Mean   : 824.8  \n 3rd Qu.:15768   3rd Qu.: 793.65   3rd Qu.: 974.97   3rd Qu.:1145.1  \n Max.   :19874   Max.   :2120.10   Max.   :2205.03   Max.   :3282.3  \n DIST_HAWKER_M     DIST_SUPERMKT_M      DIST_MALL_M      DIST_ELITEPRI_M   \n Min.   :  25.35   Min.   :   0.0003   Min.   :  39.14   Min.   :   74.37  \n 1st Qu.: 376.85   1st Qu.: 179.1599   1st Qu.: 378.70   1st Qu.: 1312.01  \n Median : 647.65   Median : 278.4398   Median : 600.73   Median : 2412.29  \n Mean   : 730.12   Mean   : 308.4312   Mean   : 664.70   Mean   : 3551.73  \n 3rd Qu.: 961.37   3rd Qu.: 397.2718   3rd Qu.: 886.89   3rd Qu.: 4997.38  \n Max.   :2500.14   Max.   :1415.3217   Max.   :2446.77   Max.   :11490.86  \n CNT_KINDER_350   CNT_CHILD_350     CNT_BUS_350     CNT_PRIMARY_1000\n Min.   :0.0000   Min.   : 0.000   Min.   : 0.000   Min.   :0.00    \n 1st Qu.:0.0000   1st Qu.: 3.000   1st Qu.: 6.000   1st Qu.:2.00    \n Median :1.0000   Median : 4.000   Median : 8.000   Median :3.00    \n Mean   :0.9289   Mean   : 4.653   Mean   : 7.985   Mean   :3.04    \n 3rd Qu.:1.0000   3rd Qu.: 6.000   3rd Qu.:10.000   3rd Qu.:4.00    \n Max.   :8.0000   Max.   :19.000   Max.   :19.000   Max.   :9.00    \n\n\n\n7.2.1 EDA using statistical graphics\nWe can plot the distribution of selling price in logarithm by using appropriate EDA as shown in the code chunk below.\n\n# draw a histogram for log price\nggplot(analysis_sf, aes(LOG_PRICE)) +          # map column to x\n  geom_histogram(bins = 40, color = \"black\", fill = \"steelblue\") +                  # use many bins for detail\n  labs(title = \"Distribution of log price\",    # plot title\n       x = \"log price\", y = \"count\") +         # axis labels\n  theme_minimal()                              # simple theme\n\n\n\n\n\n\n\n\nThe histogram is unimodal and concentrated around 13.3 to 13.5 with a visibly longer right tail beyond 13.6, indicating positive skew. Mass is dense near 13.35 and thins gradually toward 14.1, with very few values below 13.0. This pattern supports modelling price in logs since the transformation has largely stabilized variance and produced an approximately symmetric core with moderate high end skew. Implications are to use OLS on LOG_PRICE with robust errors, inspect influence diagnostics for high end points, complement Ordinary Least Square (OLS) with quantile regression to capture tail behaviour, and apply a smearing estimator when converting fitted log values back to dollars.\n\n\n7.2.2 Multiple histogram plots distribution of variables\nIn this section, we will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 15 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 5 rows small multiple plot.\n\n# create an empty list to store plots\nplot_list &lt;- list()\n\n# loop and generate histograms\nfor(v in num_vars){\n  p &lt;- ggplot(analysis_sf, aes(.data[[v]])) +\n    geom_histogram(bins = 40, color = \"black\", fill = \"steelblue\") +\n    labs(\n      title = paste(\"Distribution of\", v),\n      x = v,\n      y = \"count\"\n    ) +\n    theme_minimal()\n\n  plot_list[[v]] &lt;- p      # store plot by name\n}\n\n# arrange into 3 columns × 4 rows\nggarrange(\n  plotlist = plot_list,\n  ncol = 3,\n  nrow = 5\n)\n\n\n\n\n\n\n\n\nFLOOR_LEVEL is concentrated in low to mid storeys with a long right tail beyond 20, indicating diminishing frequency at high floors. REMAINING_LEASE_MTH and AGE_2025 are multi modal, showing construction and lease cohorts rather than a single smooth pattern. DIST_CBD_M spans about 1 to 20 km with weak multi modality, while all amenity distances are strongly right skewed with many homes within 200 to 800 m and long sparse tails. DIST_ELITEPRI_M is broad with several modes, reflecting uneven elite school locations. CNT_KINDER_350 is zero heavy, CNT_CHILD_350 is skewed to small counts, CNT_BUS_350 is roughly symmetric around 8, and CNT_PRIMARY_1000 centres near 3. This implies the necessary to transform skewed distances using log, capture cohort effects for age and lease with categorical dummies or piecewise functions, treat zero heavy counts carefully, and check collinearity among proximity measures. We may expect steep price gradients at short distances and flattening beyond about 800 to 1000 m.\n\n\n\n7.3 Simple bivariate relationships\nNext we look at how price moves with each predictor one at a time. A scatter plot with a smooth line is a fast way to see direction and potential nonlinearity. We also compute a plain correlation matrix that includes log price and all numeric predictors. Correlation does not prove a causal story and it ignores spatial structure, but it is useful as a first pass to understand which predictors are strongly aligned and which pairs might cause instability if both are used together in a linear model. Later, more formal diagnostics will revisit collinearity with variance inflation factors.\n\n# build one scatter with loess per numeric predictor\nscat_list &lt;- lapply(num_vars, function(v){\n  ggplot(analysis_sf, aes(x = .data[[v]], y = LOG_PRICE)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(method = \"loess\", se = FALSE) +\n    labs(title = paste(\"Log price versus\", v), x = v, y = \"log price\") +\n    theme_minimal()\n})\n\n# arrange as 3 by 5 small multiples\nggarrange(plotlist = scat_list, ncol = 3, nrow = 5, align = \"hv\")\n\n\n\n\n\n\n\n\nLog price rises with FLOOR_LEVEL and with REMAINING_LEASE_MTH at a diminishing rate, and declines with AGE_2025. Greater DIST_CBD_M, DIST_MRT_M, DIST_PARK_M, DIST_ELDERCARE_M, and DIST_HAWKER_M associate with lower prices, indicating centrality and amenity proximity premiums. DIST_SUPERMKT_M shows an inverted U shape, suggesting saturation effects for very near or very far supermarkets. DIST_MALL_M is weakly related. DIST_ELITEPRI_M is mildly negative, consistent with school proximity premiums. Higher CNT_CHILD_350 correlates with lower prices, while CNT_BUS_350 trends upward then flattens. CNT_KINDER_350 and CNT_PRIMARY_1000 show weak effects. It implies we need to investigate and model non-linear terms, include centrality and amenity gradients, and test interactions between structure and access, acknowledging heterogeneity."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#exploratory-data-spatial-analysis-edsa",
    "href": "Take-home_Ex03/take-home_ex03.html#exploratory-data-spatial-analysis-edsa",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "8 Exploratory Data Spatial Analysis (EDSA)",
    "text": "8 Exploratory Data Spatial Analysis (EDSA)\nSpatial exploration starts with maps. Because we have a dense point sample, it is often clearer to map quantiles rather than raw values. The first map shows log price in five quantile bands so central and suburban gradients can be seen quickly. The second map shows the distance to the CBD in five quantiles to compare spatial patterns. Keeping a consistent theme and coordinate system makes visual comparison easier. These first views often reveal data issues such as points falling in the sea or outside planning areas which we can then fix before running statistics that assume clean geometry.\n\n# use tmap in static mode\ntmap_mode(\"plot\")\n\n# 5 explicit colours for v4 (actual colours, not a palette name string)\npal5 &lt;- cols4all::c4a(\"YlOrRd\", 5)\n\n# helper: five-quantile dot map with boundary, centred bold title, legend inside frame\nq5_map_v4 &lt;- function(boundary_sf, point_sf, value_col, title) {\n  stopifnot(inherits(boundary_sf, \"sf\"),\n            inherits(point_sf, \"sf\"),\n            sf::st_crs(boundary_sf)$epsg == 3414,\n            sf::st_crs(point_sf)$epsg == 3414,\n            is.numeric(point_sf[[value_col]]))\n\n  tm_shape(boundary_sf) +\n    tm_borders(col = \"grey40\", lwd = 0.6) +\n    tm_shape(point_sf) +\n    tm_dots(\n      fill = value_col,                 # map the numeric field\n      size = 0.3,                       # slightly larger points\n      fill.scale = tm_scale_intervals(  # compute quantiles here\n        n = 5,                          # five classes\n        style = \"quantile\",\n        values = pal5                   # explicit colours\n      )\n    ) +\n    tm_title(\n      title,\n      position = c(\"center\", \"top\"),\n      fontface = \"bold\",\n      size = 1.4\n    ) +\n    tm_legend(\n      outside = FALSE,   # keep legend inside the map frame\n      position = c(\"right\", \"bottom\"),\n      format = list(digits = 1),\n      bg.color = \"white\",\n      bg.alpha = 1,\n      frame = TRUE\n    ) + \n    tm_layout(\n      frame = TRUE,     # draw a neatline\n      inner.margins = c(0.11, 0.04, 0.12, 0.04),  # room for title and legend\n      asp = NA          # allow flexible height so nothing squashes\n    ) +\n    tm_view(set_zoom_limits = c(11,14))\n}\n\n# ensure the mapped fields are numeric\nanalysis_sf$LOG_PRICE  &lt;- as.numeric(analysis_sf$LOG_PRICE)\n# analysis_sf$DIST_CBD_M &lt;- as.numeric(analysis_sf$DIST_CBD_M)\n\n# build the two maps from the real numeric columns\ntm_price &lt;- q5_map_v4(mpsz, analysis_sf, \"LOG_PRICE\",  \"Log price in 5 quantiles\")\ntm_cbd &lt;- q5_map_v4(mpsz, analysis_sf, \"DIST_CBD_M\", \"Distance to CBD in 5 quantiles\")\n\n# draw the plot\ntm_price\n\n\n\n\n\n\n\ntm_cbd\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe 5 quantile map of LOG_PRICE shows a pronounced central premium with a clear spatial gradient. The darkest red points that mark the top quantile cluster in the downtown and inner ring, then trace corridors toward the north-east and the east. The middle quantiles form transitional belts around this core. The lightest tones concentrate along the outer arc in the far north and far west, indicating the lowest tier. Scattered red pockets beyond the core align with regional town hubs and interchange nodes, implying that concentrated amenities and rail access can offset distance penalties. This pattern indicates distance decay that is smooth along some corridors but punctuated by local clusters. For explanation, measure centrality with network travel time to the main employment core, add proximity to mass rapid transit and town centres, and include bus stop density to reflect observed clusters. Allow nonlinear distance effects, test residual spatial dependence, and apply Geographically Weighted Regression (GWR) to quantify where premiums are strongest."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#hedonic-pricing-modelling",
    "href": "Take-home_Ex03/take-home_ex03.html#hedonic-pricing-modelling",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "9 Hedonic Pricing Modelling",
    "text": "9 Hedonic Pricing Modelling\nWhy do two similar flats sell at very different prices? This section answers that practical question by decomposing observed resale price into contributions from structure, location, and nearby amenities. We proceed with a disciplined modelling pathway that is faithful to our prepared data and variable choices. First, we inspect pairwise associations among predictors to anticipate stability issues. Next, we estimate a full specification, then streamline it to a tighter version guided by evidence. Results are formatted into a clear table suitable for publication. Finally, we verify assumptions and test for spatial patterning in residuals using a distance band approach. The outcome is a transparent, reproducible, and defensible explanation of price variation.\n\n9.1 Multiple Linear Regression (MLR) method\nBegin with a simple proposition. Resale price may be expressed as an additive function of structural attributes, distances to services, and counts of facilities within fixed catchments. Multiple linear regression operationalises this idea and yields coefficients that read as marginal effects with other factors held constant. The method is favoured for clarity, ease of diagnostic checking, and compatibility with downstream spatial tests. In this work, estimation adheres to our exact predictor list and uses standard ordinary least squares, ensuring that inferences are grounded in widely accepted practice while remaining aligned to our study design.\n\n9.1.1 Visualising the relationships of the independent variables\nBefore fitting any equation, we look first. A compact correlation matrix, ordered by eigenvectors, reveals clusters of related predictors and flags potential inflation of uncertainty. Numeric labels keep the display succinct, and the upper triangle focuses attention. This is an advisory step rather than a decision rule, but it equips later diagnostics with context and avoids surprises when interpreting coefficients.\n\n# collect the exact predictors for the Multiple model into a character vector\nmm_vars &lt;- c(\n  \"FLOOR_LEVEL\",\"REMAINING_LEASE_MTH\",\"AGE_2025\",\n  \"DIST_CBD_M\",\"DIST_MRT_M\",\"DIST_PARK_M\",\"DIST_ELDERCARE_M\",\n  \"DIST_HAWKER_M\",\"DIST_SUPERMKT_M\",\"DIST_MALL_M\",\"DIST_ELITEPRI_M\",\n  \"CNT_KINDER_350\",\"CNT_CHILD_350\",\"CNT_BUS_350\",\"CNT_PRIMARY_1000\"\n)\n\n# build a numeric data frame for correlation by dropping geometry and selecting variables\ncor_df &lt;- dplyr::select(sf::st_drop_geometry(analysis_sf), dplyr::all_of(mm_vars))  # keep only model vars\n\n# compute the correlation matrix using pairwise complete observations\ncor_mat &lt;- cor(cor_df, use = \"pairwise.complete.obs\")   # handle missing values pairwise\n\n# display the summary of the correlation matrix\ncor_mat\n\n                     FLOOR_LEVEL REMAINING_LEASE_MTH    AGE_2025   DIST_CBD_M\nFLOOR_LEVEL          1.000000000         0.296554139 -0.29707499 -0.247305942\nREMAINING_LEASE_MTH  0.296554139         1.000000000 -0.99963873  0.091772439\nAGE_2025            -0.297074994        -0.999638725  1.00000000 -0.091516744\nDIST_CBD_M          -0.247305942         0.091772439 -0.09151674  1.000000000\nDIST_MRT_M          -0.107713184         0.025677014 -0.02504814  0.170664542\nDIST_PARK_M         -0.051986857         0.111014527 -0.11190740  0.338694246\nDIST_ELDERCARE_M    -0.126627904         0.026535546 -0.02503077  0.399274924\nDIST_HAWKER_M       -0.139778703         0.029158714 -0.03004163  0.344602149\nDIST_SUPERMKT_M     -0.038831406         0.045386897 -0.04479530  0.016896068\nDIST_MALL_M         -0.011655741        -0.084027939  0.08562340 -0.225004071\nDIST_ELITEPRI_M     -0.129972260         0.120900782 -0.12009114  0.705061750\nCNT_KINDER_350       0.002303374        -0.161439591  0.16082083 -0.004500193\nCNT_CHILD_350        0.019279142         0.021688788 -0.02287158  0.113950711\nCNT_BUS_350         -0.032800232         0.009937946 -0.01054252  0.183472813\nCNT_PRIMARY_1000    -0.096965172        -0.031512373  0.03079923  0.260404601\n                       DIST_MRT_M DIST_PARK_M DIST_ELDERCARE_M DIST_HAWKER_M\nFLOOR_LEVEL         -0.1077131843 -0.05198686      -0.12662790   -0.13977870\nREMAINING_LEASE_MTH  0.0256770139  0.11101453       0.02653555    0.02915871\nAGE_2025            -0.0250481409 -0.11190740      -0.02503077   -0.03004163\nDIST_CBD_M           0.1706645423  0.33869425       0.39927492    0.34460215\nDIST_MRT_M           1.0000000000  0.09026758       0.10676686    0.01277210\nDIST_PARK_M          0.0902675794  1.00000000      -0.03338244    0.11819275\nDIST_ELDERCARE_M     0.1067668594 -0.03338244       1.00000000    0.13245089\nDIST_HAWKER_M        0.0127721005  0.11819275       0.13245089    1.00000000\nDIST_SUPERMKT_M      0.1765673011 -0.11986959       0.03450565   -0.02562024\nDIST_MALL_M          0.3388069011 -0.04258508       0.02584431   -0.15787007\nDIST_ELITEPRI_M     -0.0001624268  0.23282594       0.35573127    0.23130821\nCNT_KINDER_350      -0.0592377617  0.02556129       0.03447085    0.06446977\nCNT_CHILD_350       -0.1840478956  0.16796939      -0.02019895    0.10158066\nCNT_BUS_350         -0.0012617027  0.12903762       0.01914944   -0.01850288\nCNT_PRIMARY_1000    -0.2816911139  0.23296458      -0.08188842    0.06737123\n                    DIST_SUPERMKT_M DIST_MALL_M DIST_ELITEPRI_M CNT_KINDER_350\nFLOOR_LEVEL             -0.03883141 -0.01165574   -0.1299722595    0.002303374\nREMAINING_LEASE_MTH      0.04538690 -0.08402794    0.1209007817   -0.161439591\nAGE_2025                -0.04479530  0.08562340   -0.1200911403    0.160820834\nDIST_CBD_M               0.01689607 -0.22500407    0.7050617496   -0.004500193\nDIST_MRT_M               0.17656730  0.33880690   -0.0001624268   -0.059237762\nDIST_PARK_M             -0.11986959 -0.04258508    0.2328259410    0.025561293\nDIST_ELDERCARE_M         0.03450565  0.02584431    0.3557312671    0.034470854\nDIST_HAWKER_M           -0.02562024 -0.15787007    0.2313082059    0.064469774\nDIST_SUPERMKT_M          1.00000000  0.20820232   -0.0677967866   -0.118708737\nDIST_MALL_M              0.20820232  1.00000000   -0.2732240341   -0.050023131\nDIST_ELITEPRI_M         -0.06779679 -0.27322403    1.0000000000   -0.047414174\nCNT_KINDER_350          -0.11870874 -0.05002313   -0.0474141744    1.000000000\nCNT_CHILD_350           -0.23729483 -0.26179038    0.0727460753    0.471435322\nCNT_BUS_350             -0.08543510 -0.23892133    0.0551058117    0.072713779\nCNT_PRIMARY_1000        -0.17782225 -0.35451228    0.1335564522    0.033401170\n                    CNT_CHILD_350  CNT_BUS_350 CNT_PRIMARY_1000\nFLOOR_LEVEL            0.01927914 -0.032800232      -0.09696517\nREMAINING_LEASE_MTH    0.02168879  0.009937946      -0.03151237\nAGE_2025              -0.02287158 -0.010542525       0.03079923\nDIST_CBD_M             0.11395071  0.183472813       0.26040460\nDIST_MRT_M            -0.18404790 -0.001261703      -0.28169111\nDIST_PARK_M            0.16796939  0.129037619       0.23296458\nDIST_ELDERCARE_M      -0.02019895  0.019149444      -0.08188842\nDIST_HAWKER_M          0.10158066 -0.018502881       0.06737123\nDIST_SUPERMKT_M       -0.23729483 -0.085435098      -0.17782225\nDIST_MALL_M           -0.26179038 -0.238921335      -0.35451228\nDIST_ELITEPRI_M        0.07274608  0.055105812       0.13355645\nCNT_KINDER_350         0.47143532  0.072713779       0.03340117\nCNT_CHILD_350          1.00000000  0.240766976       0.31886264\nCNT_BUS_350            0.24076698  1.000000000       0.21454825\nCNT_PRIMARY_1000       0.31886264  0.214548250       1.00000000\n\n# visualise the matrix with a compact upper triangle layout\nlibrary(corrplot)       # load correlation plot package\ncorrplot(               # draw the heat map\n  cor_mat,              # supply the matrix\n  diag  = FALSE,         # hide the diagonal squares\n  order = \"AOE\",        # use angular order of eigenvectors\n  tl.pos = \"td\",        # place text labels at top\n  tl.cex = 0.8,         # use small text size\n  method = \"number\",    # print numbers instead of tiles\n  type = \"upper\"        # draw only the upper triangle\n)\n\n\n\n\n\n\n\n\nFirst, REMAINING_LEASE_MTH and AGE_2025 have a correlation of about −0.9996. They are almost perfect substitutes: a flat that is older in AGE_2025 almost always has a shorter remaining lease, and vice versa. In the multiple model this pair will create extremely severe multicollinearity, so in later steps we must either drop one of them or be prepared for very unstable coefficient estimates and very high variance inflation.\nSecond, the largest correlation among the location variables is between DIST_CBD_M and DIST_ELITEPRI_M, around 0.71. Condos that are farther from the CBD also tend to be farther from elite primary schools. This means these 2 distance measures share substantial spatial pattern, so their individual coefficients in the multiple model will be harder to interpret cleanly, even though they are not perfectly redundant.\nOther distance measures are only moderately associated. For example, DIST_CBD_M with DIST_PARK_M, DIST_ELDERCARE_M, and DIST_HAWKER_M are in the 0.33 to 0.40 range, and most other pairs are below 0.30 in magnitude. DIST_MALL_M has modest negative correlation with DIST_CBD_M (about −0.23). These suggest some shared geography but not critical multicollinearity.\nThe count variables form a loose cluster: CNT_KINDER_350 and CNT_CHILD_350 correlate at about 0.47, and CNT_CHILD_350 with CNT_PRIMARY_1000 at about 0.32, with smaller positive correlations involving CNT_BUS_350. They measure related aspects of family oriented amenities but still contain non overlapping information.\nFLOOR_LEVEL has only small to moderate correlations with all other variables, so it adds largely independent vertical characteristics.\nImplication for modelling: the only truly problematic pair is REMAINING_LEASE_MTH with AGE_2025. DIST_CBD_M with DIST_ELITEPRI_M and the childcare related counts may inflate variance somewhat but are still acceptable. The rest of the predictors are not strongly correlated, so the multiple regression can proceed, with careful collinearity diagnostics and possibly removing one of AGE_2025 or REMAINING_LEASE_MTH in the final explanatory model.\n\n\n9.1.2 Building a hedonic pricing model using multiple linear regression method\nHere the analysis moves from inspection to estimation. The full specification places all required predictors into a single equation so that each coefficient measures the expected change in resale price associated with a one unit change in that predictor when the rest are held fixed. The summary reports effect size, direction, and precision along with overall fit statistics. We comment on the economic reasonableness of signs, the relative magnitude of location versus structure variables, and whether the model captures a substantial share of variation. Residuals and fitted values from this run become inputs to the validation steps that follow.\n\n# multiple linear regression with the exact predictors provided\nmlr.full &lt;- lm(\n  formula = resale_price ~\n    FLOOR_LEVEL + REMAINING_LEASE_MTH + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M + DIST_PARK_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf\n)\n\n# model summary\nols_regress(mlr.full)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.873       RMSE                    78436.147 \nR-Squared                   0.761       MSE                6152229192.516 \nAdj. R-Squared              0.761       Coef. Var                  11.675 \nPred R-Squared              0.761       AIC                    220289.292 \nMAE                     60536.435       SBC                    220409.459 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                          ANOVA                                            \n------------------------------------------------------------------------------------------\n                           Sum of                                                         \n                          Squares          DF           Mean Square       F          Sig. \n------------------------------------------------------------------------------------------\nRegression    170474487949160.562          15    11364965863277.371    1843.887    0.0000 \nResidual       53395197161850.531        8663        6163591961.428                       \nTotal         223869685111011.094        8678                                             \n------------------------------------------------------------------------------------------\n\n                                              Parameter Estimates                                               \n---------------------------------------------------------------------------------------------------------------\n              model           Beta    Std. Error    Std. Beta       t        Sig          lower          upper \n---------------------------------------------------------------------------------------------------------------\n        (Intercept)    1085699.508    221577.906                   4.900    0.000    651354.108    1520044.909 \n        FLOOR_LEVEL       5854.876       146.318        0.232     40.015    0.000      5568.059       6141.693 \nREMAINING_LEASE_MTH         -3.347       186.073       -0.004     -0.018    0.986      -368.095        361.400 \n           AGE_2025      -5438.039      2235.751       -0.477     -2.432    0.015     -9820.642      -1055.435 \n         DIST_CBD_M        -19.751         0.334       -0.540    -59.097    0.000       -20.406        -19.096 \n         DIST_MRT_M        -56.538         2.642       -0.131    -21.401    0.000       -61.716        -51.359 \n        DIST_PARK_M         -2.737         2.356       -0.007     -1.162    0.245        -7.355          1.881 \n   DIST_ELDERCARE_M        -10.786         1.528       -0.043     -7.059    0.000       -13.781         -7.791 \n      DIST_HAWKER_M        -25.306         1.998       -0.072    -12.667    0.000       -29.222        -21.390 \n    DIST_SUPERMKT_M         15.585         4.983        0.018      3.127    0.002         5.816         25.354 \n        DIST_MALL_M        -27.091         2.739       -0.063     -9.891    0.000       -32.460        -21.722 \n    DIST_ELITEPRI_M          1.886         0.422        0.035      4.474    0.000         1.060          2.713 \n     CNT_KINDER_350      10194.973       930.214        0.068     10.960    0.000      8371.532      12018.414 \n      CNT_CHILD_350      -1910.268       467.163       -0.027     -4.089    0.000     -2826.018       -994.518 \n        CNT_BUS_350       1477.776       320.857        0.026      4.606    0.000       848.820       2106.733 \n   CNT_PRIMARY_1000     -11971.804       662.369       -0.116    -18.074    0.000    -13270.204     -10673.404 \n---------------------------------------------------------------------------------------------------------------\n\n\nThe model has strong overall explanatory power. R squared is 0.761 with an F statistic of 1840.225 and p value below 0.001, so the 15 predictors jointly explain about 76 percent of the variation in the response. Predicted R squared (0.760), RMSE around 78496, and MAE around 60588 show that out of sample error is broadly consistent with in sample fit.\nWithin the coefficient table, most predictors are statistically significant at the 0.05 level. Exceptions are REMAINING_LEASE_MTH and DIST_PARK_M, whose p values (0.965 and 0.253) indicate that, once the other variables are included, they contribute little unique explanatory power. This is consistent with the near perfect correlation between REMAINING_LEASE_MTH and AGE_2025, which makes the lease variable redundant while AGE_2025 remains significant.\nStandardised betas identify the strongest effects. DIST_CBD_M has the largest magnitude (standardised beta about −0.541): locations further from the central area are associated with substantially lower predicted outcome, all else equal. AGE_2025 also has a large negative standardised beta (about −0.483), indicating that newer developments are valued more highly than older ones after controlling for other factors. FLOOR_LEVEL has a sizeable positive effect (standardised beta about 0.232), so higher floors are linked to higher predicted outcome. DIST_MRT_M, DIST_ELDERCARE_M, DIST_HAWKER_M, DIST_MALL_M, and CNT_PRIMARY_1000 all have significant negative coefficients, meaning greater distance from these amenities or more primary schools within 1000 meters is associated with lower predicted outcome, holding other variables constant.\nSome amenities work in the opposite direction. DIST_SUPERMKT_M, DIST_ELITEPRI_M, CNT_KINDER_350, and CNT_BUS_350 have positive coefficients, so, after controlling for the rest of the urban environment, blocks with more kindergartens or bus stops nearby and those a little further from supermarkets and elite primary schools tend to have higher predicted outcomes in this sample.\nOverall, the regression suggests that centrality, building age, vertical position, access to several key facilities, and the surrounding mix of schools and transport services are the dominant drivers of the response, while remaining lease length and park distance add little once these correlated factors are taken into account.\n\n\n9.1.3 Revising the model\nTake the fitted baseline and refine it. Terms with weak support or redundancy are removed while preserving the intended meaning of the specification. The revised version aims for parsimony, stability, and interpretability. We compare key coefficients and global fit between the initial and revised results and ensure that important relationships persist with tighter intervals. This becomes the reference model for formal presentation and for all diagnostic procedures.\n\n# multiple linear regression with the exact predictors provided\nmlr1.full &lt;- lm(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf\n)\n\n# model summary\nols_regress(mlr1.full)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.873       RMSE                    78442.260 \nR-Squared                   0.761       MSE                6153188118.823 \nAdj. R-Squared              0.761       Coef. Var                  11.675 \nPred R-Squared              0.761       AIC                    220286.645 \nMAE                     60502.030       SBC                    220392.675 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                          ANOVA                                            \n------------------------------------------------------------------------------------------\n                           Sum of                                                         \n                          Squares          DF           Mean Square       F          Sig. \n------------------------------------------------------------------------------------------\nRegression    170466165427744.312          13    13112781955980.332    2127.617    0.0000 \nResidual       53403519683266.719        8665        6163129796.107                       \nTotal         223869685111011.031        8678                                             \n------------------------------------------------------------------------------------------\n\n                                             Parameter Estimates                                              \n-------------------------------------------------------------------------------------------------------------\n           model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-------------------------------------------------------------------------------------------------------------\n     (Intercept)    1081180.563      5873.482                 184.078    0.000    1069667.142    1092693.984 \n     FLOOR_LEVEL       5853.856       146.301        0.232     40.013    0.000       5567.072       6140.640 \n        AGE_2025      -5390.455        65.359       -0.473    -82.474    0.000      -5518.574      -5262.336 \n      DIST_CBD_M        -19.831         0.327       -0.542    -60.650    0.000        -20.472        -19.190 \n      DIST_MRT_M        -56.780         2.633       -0.132    -21.561    0.000        -61.943        -51.618 \nDIST_ELDERCARE_M        -10.470         1.502       -0.041     -6.970    0.000        -13.415         -7.526 \n   DIST_HAWKER_M        -25.344         1.996       -0.072    -12.695    0.000        -29.258        -21.431 \n DIST_SUPERMKT_M         16.269         4.948        0.018      3.288    0.001          6.569         25.969 \n     DIST_MALL_M        -27.510         2.712       -0.064    -10.144    0.000        -32.826        -22.194 \n DIST_ELITEPRI_M          1.864         0.421        0.035      4.428    0.000          1.039          2.689 \n  CNT_KINDER_350      10196.160       930.157        0.068     10.962    0.000       8372.831      12019.489 \n   CNT_CHILD_350      -1953.087       465.630       -0.028     -4.195    0.000      -2865.833      -1040.342 \n     CNT_BUS_350       1461.605       320.541        0.026      4.560    0.000        833.269       2089.941 \nCNT_PRIMARY_1000     -12074.626       656.393       -0.117    -18.395    0.000     -13361.312     -10787.939 \n-------------------------------------------------------------------------------------------------------------\n\n\nThis final model keeps 13 predictors and fits very well. R squared is 0.761 with adjusted and predicted R squared both 0.761 and 0.760. The F statistic of 2123.406 with \\(p\\) value below 0.001 shows the retained predictors jointly explain the dependent variable strongly. RMSE is about 78,501 and MAE about 60,554, very similar to the previous model, so parsimony is gained without losing explanatory power.\nCompared with the earlier specification, REMAINING_LEASE_MTH and DIST_PARK_M have been removed because their p values were very large once other variables were included and REMAINING_LEASE_MTH was almost perfectly collinear with AGE_2025. Their removal simplifies interpretation while keeping model fit essentially unchanged.\nLet Y be the predicted resale price. The ordinary least squares equation is\n\\[\n\\hat Y \\approx 1,080,598 + 5,855\\cdot Floor - 5393 \\cdot AGE\\_2025 - 20\\cdot DIST\\_CBD\\_M - 57 \\cdot DIST\\_MRT\\_M - 10 \\cdot DIST\\_ELDERCARE\\_M - 25 \\cdot DIST\\_HAWKER\\_M + 17 \\cdot DIST\\_SUPERMKT\\_M - 27 \\cdot DIST\\_MALL\\_M + 1.9 \\cdot DIST\\_ELITEPRI\\_M + 10195 \\cdot CNT\\_KINDER\\_350 - 1967 \\cdot CNT\\_CHILD\\_350 + 1440 \\cdot CNT\\_BUS\\_350 - 11794 \\cdot CNT\\_PRIMARY\\_1000\n\\]\nInterpreting unit changes: holding all other variables constant, moving up 1 floor raises expected price by about 5,855. An increase of 1 unit in AGE_2025 reduces price by about 5,393, confirming that newer developments command higher prices. Every additional unit of distance from the central business district reduces price by about 19.9, and every additional unit of distance from the nearest MRT station reduces price by about 56.6. Distances from eldercare, hawker centres and malls also have significant negative effects.\nAmenity counts matter as well. Each extra kindergarten within 350 units increases price by roughly 10195, while each extra primary school within 1000 units reduces price by about 11,794 after controlling for the rest, suggesting that the mix of school types in the immediate environment is important rather than sheer number. Bus stop density has a modest positive effect, and the small positive coefficients for distance to supermarkets and elite primary schools indicate that, in this dataset, higher priced condos tend to sit slightly further from those specific facilities once other location attributes are controlled. Overall, the retained predictors describe a coherent hedonic structure in which height, building age, centrality and the surrounding service mix jointly drive resale prices.\n\n\n\n9.2 Regression diagnostics\nTrust in the model is earned through testing. This subsection assembles checks that speak to stability, functional form, distributional behaviour of errors, and spatial independence. We first quantify variance inflation to assess redundancy among predictors. We then examine residual patterns for departures from linearity. Next, we evaluate normality using both a formal test and visual evidence. Finally, we test residual spatial autocorrelation using a distance band neighbour list with row standardised weights and a permutation based Moran statistic. Together, these checks provide a rigorous audit of model adequacy.\n\n9.2.1 Multicollinearity test\nThe variance inflation profile answers a simple question. Are predictors supplying distinct information or echoing each other? Values near one suggest little redundancy, while larger values warn of unstable estimates. The accompanying plot highlights any terms approaching concern thresholds and supports reasoned choices about retention in the revised equation.\nIn the code chunk below, we test if there are sign of multicollinearity.\n\n# Check for Multicollinearity\n# check_collinearity(mlr1.full)\nols_vif_tol(mlr1.full)\n\n          Variables Tolerance      VIF\n1       FLOOR_LEVEL 0.8181507 1.222269\n2          AGE_2025 0.8358685 1.196360\n3        DIST_CBD_M 0.3442546 2.904827\n4        DIST_MRT_M 0.7342463 1.361941\n5  DIST_ELDERCARE_M 0.7827319 1.277577\n6     DIST_HAWKER_M 0.8473710 1.180121\n7   DIST_SUPERMKT_M 0.8912908 1.121968\n8       DIST_MALL_M 0.6896892 1.449928\n9   DIST_ELITEPRI_M 0.4456876 2.243724\n10   CNT_KINDER_350 0.7220808 1.384887\n11    CNT_CHILD_350 0.6249226 1.600198\n12      CNT_BUS_350 0.8543371 1.170498\n13 CNT_PRIMARY_1000 0.6810065 1.468415\n\n# plot the \nmlr1.vif &lt;- check_collinearity(mlr1.full)\nplot(mlr1.vif)\n\n\n\n\n\n\n\n\nThe variance inflation results show that the revised model has very little multi collinearity and the coefficients should be numerically stable. All 13 predictors have variance inflation factor values between about 1.1 and 2.9, with tolerances above 0.34. Common rules of thumb treat variance inflation factor values below 5 as low concern and values above 10 as serious, so every predictor is well inside the safe zone.\nThe 2 strongest correlations are reflected in DIST_CBD_M with variance inflation factor about 2.9 and DIST_ELITEPRI_M with variance inflation factor about 2.2. These confirm some shared spatial structure between centrality and distance to elite primary schools but still indicate that each variable contributes enough unique information to justify retention. All other predictors, including AGE_2025, FLOOR_LEVEL and the amenity counts, have variance inflation factor values close to 1, meaning they are largely independent once the others are controlled.\nThe plot reinforces this conclusion visually. Every point lies in the green band labelled low (variance inflation factor below 5), and none approach the blue or red regions that would signal unstable estimates. Combined with the earlier step that removed REMAINING_LEASE_MTH and DIST_PARK_M, these diagnostics show that the final hedonic equation is free from harmful multi collinearity, so the standard errors, confidence intervals and p values reported for the coefficients can be interpreted with high confidence.\n\n\n9.2.2 Test for non-linearity\nResiduals should show no systematic curve against fitted values if the linear form is appropriate. A smooth trend line provides a quick visual guide. A flat band supports the additive form, whereas pronounced curvature indicates that transformations or interactions may be warranted in later extensions.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n# ols_plot_resid_fit(mlr1.full)\n\ncheck_model(mlr1.full,\n            check = \"linearity\")\n\n\n\n\n\n\n\n\nThe residual versus fitted plot shows a clear curved trend instead of a flat band around 0, so the strict linearity assumption is not fully satisfied. For low fitted values the smooth line lies above 0, indicating that the model under predicts prices for the cheapest units. Around the middle range the line dips below 0, so prices in this segment are slightly over predicted. At the upper end the line rises above 0 again, which means the most expensive units are again under predicted. The spread of residuals also increases with fitted value, suggesting mild heteroscedasticity.\nThese patterns imply that the global ordinary least squares specification is only an approximation and that some systematic structure remains unexplained. Beyond possible transformations or interaction terms, an important next step is to recognise that price determinants may vary across space. Section 10 will therefore extend the analysis with geographically weighted regression, allowing regression coefficients to vary by location, refining local fitted values, and testing whether key covariate effects differ across neighbourhoods rather than being forced to share a single global relationship.\n\n\n9.2.3 Test for normality assumption\nStudentised residuals are tested and visualised to judge whether the normal approximation that underpins interval estimates is reasonable. The Q Q display should track the reference line except at the tails. When formal tests detect minor deviations in large samples, practical interpretation relies on the visual pattern and on the robustness of conclusions.\nIn the code chunk below, ols_plot_resid_hist() of olssr package is used to perform normality assumption test on mlr1.full model.\n\nolsrr::ols_plot_resid_hist(mlr1.full)\n\n\n\n\n\n\n\n# check_normality(mlr1.full)\n\nThis pattern suggests that the normality assumption is acceptable for inference on regression coefficients and mean effects, although prediction intervals for extreme high priced units will be less reliable. The mild skewness also reinforces the earlier evidence of non constant variance and non linearity, motivating later refinements such as geographically weighted regression and possibly robust or transformed specifications to improve fit in the tails of the price distribution.\nInstead of showing the test statistic, plot() of see package can be used to plot a the output of check_normality() for visual inspection as shown below.\n\nplot(performance::check_normality(mlr1.full), \n     type = \"qq\")\n\n\n\n\n\n\n\n\nThe Q Q plot shows that residuals follow the reference line very closely through the central part of the distribution. From about the 10th to the 90th percentile the points stay inside the grey band, so the normal approximation is reasonable for most observations and standard OLS inference on mean effects is reliable.\nDepartures appear mainly in the upper tail, where points bend above the line and move outside the band, indicating more large positive residuals than a normal distribution would predict. A mild deviation is also visible in the extreme lower tail. These patterns confirm slight right skew and heavier tails, consistent with the histogram. In practice this means that coefficient estimates and their confidence intervals remain trustworthy given the very large sample, but prediction intervals for the most expensive condominiums are less accurate and a small number of high priced outliers or omitted local factors are not fully captured by the global model.\nAnother way to check for normality assumption visual is by using check_model() of performance package as shown in the code chunk below.\n\nperformance::check_model(mlr1.full,\n            check= \"normality\")\n\n\n\n\n\n\n\n\nThe kernel density of residuals aligns very closely with the overlaid normal curve and is centred near 0, indicating that the bulk of the studentised residuals follow an approximately normal pattern. The peak and spread of the empirical distribution almost coincide with the theoretical curve, with only a slightly longer right tail where a small number of observations record large positive residuals. Hence the ordinary least squares estimates and their interval estimates can be interpreted with confidence, while recognising that predictions for a small subset of high priced units may involve larger uncertainty due to the mild departure in the upper tail.\n\n\n9.2.4 Testing for Spatial Autocorrelation\nResiduals are attached to the feature layer, mapped in quantile classes for visual screening, and then assessed with a distance band Moran test using row standardised weights and 999 permutations. A significant positive statistic indicates clustering of errors and suggests that spatial processes remain unmodelled. Evidence of such structure motivates the spatially aware methods that follow in the broader study.\nIn order to perform spatial autocorrelation test, we need to export the residual of the hedonic pricing model and save it as a data frame first.\n\nmlr1.output &lt;- as.data.frame(mlr1.full$residuals)\n\nNext, we will join the newly created data frame with mlr1_resale.sf object.\n\nmlr1_resale.sf &lt;- cbind(analysis_sf, \n                        mlr1.full$residuals) %&gt;%\nrename(`MLR_RES` = `mlr1.full.residuals`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz) +\ntm_polygons(fill_alpha = 0.4) + # semi‑transparent base\ntm_shape(mlr1_resale.sf) +\ntm_dots(\nfill = \"MLR_RES\", # color by residual value\nsize = 0.5, # point size\ncol = \"black\", # thin border\nfill.scale = tm_scale( # custom diverging palette\nn = 10,\nvalues = rev(brewer.pal(11, \"RdBu\")), # red‑blue diverging\nstyle = \"quantile\",\nmidpoint = NA),\nfill.legend = tm_legend(title = \"Residuals\")\n) +\ntm_title(\"LM Residuals (Quantile Classification)\") +\ntm_layout(legend.outside = FALSE) +\ntm_view(set_zoom_limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe residual map reveals that model errors are not spatially random. Large positive residuals in dark red form clusters in several neighbourhoods, meaning resale prices there are systematically higher than the global model predicts. Large negative residuals in dark blue cluster elsewhere, indicating over prediction of prices in those areas. Pale symbols around zero are more scattered and show locations where the model fits reasonably well.\nThis spatial clustering implies that important local influences on price are not fully captured by the current global ordinary least squares equation. The strength or direction of covariate effects likely varies across space, and unobserved neighbourhood attributes are exerting local impacts. These patterns justify the next step of applying spatial diagnostics and geographically weighted regression in Section 10, so that coefficients can vary by location and the fitted values can better reflect local housing market conditions rather than a single city wide average relationship.\nTo proof that our observation is indeed true, Global Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using st_dist_band() function of sfdep.\n\n# Build distance‑band neighbors and row‑standardized weights ------------------\nanalysis_sf &lt;- analysis_sf %&gt;%\n  mutate(\n    nb = sfdep::st_dist_band(\n      st_geometry(geometry), \n      upper = 1500), # neighbors &lt;= 1.5 km\n    wt = st_weights(\n      nb, style = \"W\"), # row‑standardized W\n    .before = 1)\n\nNext, global_moran_perm() of sfdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\n# Permutation Moran’s I test on residuals -------------------------------------\nsfdep::global_moran_perm(\n  mlr1_resale.sf$MLR_RES,\n  nb = analysis_sf$nb,\n  wt = analysis_sf$wt,\n  alternative = \"two.sided\",\n  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.32591, observed rank = 1000, p-value &lt;\n0.00000000000000022\nalternative hypothesis: two.sided\n\n\nThe Monte Carlo Moran I result shows strong and highly significant spatial autocorrelation in the OLS residuals. The statistic is about 0.33 with an observed rank of 1 000 out of 1 000 simulations and a p value effectively equal to 0. This means none of the simulated patterns under spatial randomness produced an I value as large as the observed one, so the null hypothesis of spatially independent residuals is decisively rejected.\nIn practical terms, nearby condominiums tend to share similar overpricing or underpricing after controlling for the hedonic covariates. This confirms that important spatial structure remains unexplained and that the OLS error term is not independent. As a result, standard errors from the global OLS may be underestimated and significance levels somewhat optimistic, even though the main effect directions are still informative. These findings support the decision to move to spatial models in Section 10, such as geographically weighted regression or spatial error specifications, to capture local market effects and produce more reliable inference and location sensitive fitted values."
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html",
    "href": "In-Class_Ex10/in-class_ex10.html",
    "title": "In-class Ex10",
    "section": "",
    "text": "Market area analysis defines and studies where customers originate, using demographics, spending, competition, and observation to guide location, marketing, and expansion decisions. The Huff model estimates store choice probabilities and market share from attractiveness and distance, helping businesses delineate trade areas and forecast revenue among competing outlets and potential profitability.\n\n\n\nBefore we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.\n\npacman::p_load(sf, tidyverse, MCI, tmap, ggstatsplot)\n\n\n\n\n\nstores &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"STORES_TC\")\n\nReading layer `STORES_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 202702.9 ymin: 2662196 xmax: 232773.2 ymax: 2693557\nProjected CRS: TWD97 / TM2 zone 121\n\ntown &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"TOWN_TC\")\n\nReading layer `TOWN_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\nvillage &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"VILLAGE_TC\")\n\nReading layer `VILLAGE_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 622 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\n\nThe output print above shown that the three data sets are in TWD97 / TM2 zone 121 (i.e epsg: 3826) projected coordinates systems.\n\n\n\n\n\n\nImportant\n\n\n\nFor any geospatial analysis, it is important to check and confirm that all the geospatial data are in the similar projected coodinates system.\n\n\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(town) +\n  tm_polygons() +\ntm_shape(village) +\n  tm_polygons() + \ntm_shape(stores) +\n  tm_dots()\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\nNext, read_csv() of readr package will be used to import the two aspatial data sets into R environment.\n\npop2019 &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/pop2019.csv\")\n\nRows: 625 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): COUNTY, TOWN, V_ID, VILLAGE, INFO_TIME\ndbl (6): COUNTY_ID, TOWN_ID, H_CNT, P_CNT, M_CNT, F_CNT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstore_sales &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/Store_sales_data.csv\")\n\nRows: 5499 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Store Code, POD code\ndbl (3): Bills, Amount, Ave Bill\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsummary(store_sales)\n\n  Store Code          POD code             Bills            Amount       \n Length:5499        Length:5499        Min.   :   1.0   Min.   :      0  \n Class :character   Class :character   1st Qu.:  37.0   1st Qu.:  26702  \n Mode  :character   Mode  :character   Median : 186.0   Median : 139643  \n                                       Mean   : 331.9   Mean   : 236787  \n                                       3rd Qu.: 457.0   3rd Qu.: 331952  \n                                       Max.   :7764.0   Max.   :6076060  \n    Ave Bill      \n Min.   :    0.0  \n 1st Qu.:  661.3  \n Median :  730.5  \n Mean   :  825.1  \n 3rd Qu.:  862.2  \n Max.   :24356.0  \n\n\n\n\n\n\n\nCode chunk below is used to aggregate the store sales data at Store Code.\n\nstore_sales_aggre &lt;- store_sales %&gt;%\n  group_by(`Store Code`) %&gt;%\n  summarise(\n    \"Total_Bill\" = sum(Bills, na.rm = TRUE),\n    \"Total_Sales\" = sum(Amount, na.rm = TRUE))\n\n\n\n\nIn the code chunk below, left_join() of dplyr package is used to append data from store_sales data.frame onto stores sf data.frame. At the same time a new field called Store_size has been created.\n\nstores &lt;- stores %&gt;%\n  left_join(store_sales_aggre,\n            by = join_by(\n              Store_CD == `Store Code`)) %&gt;%\n  mutate(Store_size = 100)\n\nNext, summary() is used to check if the are any missing values in the data.frame.\n\nsummary(stores)\n\n  Store_Name         Store_Addr          Store_CD           Total_Bill   \n Length:28          Length:28          Length:28          Min.   : 1767  \n Class :character   Class :character   Class :character   1st Qu.: 5698  \n Mode  :character   Mode  :character   Mode  :character   Median : 8634  \n                                                          Mean   : 8021  \n                                                          3rd Qu.:10349  \n                                                          Max.   :14468  \n  Total_Sales               geometry    Store_size \n Min.   :1475963   POINT        :28   Min.   :100  \n 1st Qu.:4044821   epsg:3826    : 0   1st Qu.:100  \n Median :6116336   +proj=tmer...: 0   Median :100  \n Mean   :5542852                      Mean   :100  \n 3rd Qu.:6959911                      3rd Qu.:100  \n Max.   :8234234                      Max.   :100  \n\n\n\n\n\n\nglimpse(pop2019)\n\nRows: 625\nColumns: 11\n$ COUNTY_ID &lt;dbl&gt; 66000, 66000, 66000, 66000, 66000, 66000, 66000, 66000, 6600…\n$ COUNTY    &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市…\n$ TOWN_ID   &lt;dbl&gt; 66000110, 66000110, 66000110, 66000110, 66000110, 66000110, …\n$ TOWN      &lt;chr&gt; \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區…\n$ V_ID      &lt;chr&gt; \"66000110-002\", \"66000110-009\", \"66000110-021\", \"66000110-00…\n$ VILLAGE   &lt;chr&gt; \"大甲里\", \"中山里\", \"太白里\", \"孔門里\", \"文曲里\", \"文武里\", \"日南里\", \"平安里\", \"江南里…\n$ H_CNT     &lt;dbl&gt; 454, 1781, 406, 383, 355, 2071, 2039, 785, 461, 559, 769, 80…\n$ P_CNT     &lt;dbl&gt; 1255, 5252, 1471, 1001, 1264, 6687, 6739, 2642, 1618, 1977, …\n$ M_CNT     &lt;dbl&gt; 626, 2533, 758, 538, 640, 3244, 3406, 1310, 838, 983, 1273, …\n$ F_CNT     &lt;dbl&gt; 629, 2719, 713, 463, 624, 3443, 3333, 1332, 780, 994, 1239, …\n$ INFO_TIME &lt;chr&gt; \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y…\n\n\n\nglimpse(village)\n\nRows: 622\nColumns: 11\n$ VILLCODE   &lt;chr&gt; \"66000110029\", \"66000120001\", \"66000120002\", \"66000120003\",…\n$ COUNTYNAME &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中…\n$ TOWNNAME   &lt;chr&gt; \"大甲區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水…\n$ VILLNAME   &lt;chr&gt; \"建興里\", \"鰲峰里\", \"靈泉里\", \"清水里\", \"文昌里\", \"南寧里\", \"西寧里\", \"北寧里\", \"中興…\n$ VILLENG    &lt;chr&gt; \"Jianxing Vil.\", \"Aofeng Vil.\", \"Lingquan Vil.\", \"Qingshui …\n$ COUNTYID   &lt;chr&gt; \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\",…\n$ COUNTYCODE &lt;chr&gt; \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"6600…\n$ TOWNID     &lt;chr&gt; \"B11\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B1…\n$ TOWNCODE   &lt;chr&gt; \"66000110\", \"66000120\", \"66000120\", \"66000120\", \"66000120\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((211953.9 27..., MULTIPOLYGON (…\n\n\nThe plot above shown that the village ID (i.e. V_ID) in pop2019 data.frame is not in the same structure as the village code in village sf data.frame.\nCode chunk below will be used to remove the “-” sign from the values.\n\npop2019 &lt;- pop2019 %&gt;%\n  mutate(V_ID = str_remove_all(\n    V_ID, \"-\"))\n\nNow, we are ready to append data in pop2019 data.frame onto village sf data.frame by using left_join() of dplyr package.\n\nvillage &lt;- village %&gt;%\n  left_join(pop2019,\n            by = join_by(\n              VILLCODE == V_ID))\n\n\n\n\n\nsummary(village)\n\n   VILLCODE          COUNTYNAME          TOWNNAME           VILLNAME        \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   VILLENG            COUNTYID          COUNTYCODE           TOWNID         \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   TOWNCODE             NOTE             COUNTY_ID        COUNTY         \n Length:622         Length:622         Min.   :66000   Length:622        \n Class :character   Class :character   1st Qu.:66000   Class :character  \n Mode  :character   Mode  :character   Median :66000   Mode  :character  \n                                       Mean   :66000                     \n                                       3rd Qu.:66000                     \n                                       Max.   :66000                     \n                                       NA's   :5                         \n    TOWN_ID            TOWN             VILLAGE              H_CNT     \n Min.   :6.6e+07   Length:622         Length:622         Min.   : 180  \n 1st Qu.:6.6e+07   Class :character   Class :character   1st Qu.: 724  \n Median :6.6e+07   Mode  :character   Mode  :character   Median :1335  \n Mean   :6.6e+07                                         Mean   :1592  \n 3rd Qu.:6.6e+07                                         3rd Qu.:2195  \n Max.   :6.6e+07                                         Max.   :7599  \n NA's   :5                                               NA's   :5     \n     P_CNT           M_CNT          F_CNT       INFO_TIME        \n Min.   :  487   Min.   : 257   Min.   : 230   Length:622        \n 1st Qu.: 2139   1st Qu.:1095   1st Qu.:1053   Class :character  \n Median : 3862   Median :1911   Median :1934   Mode  :character  \n Mean   : 4545   Mean   :2234   Mean   :2311                     \n 3rd Qu.: 6151   3rd Qu.:3022   3rd Qu.:3171                     \n Max.   :18652   Max.   :8741   Max.   :9911                     \n NA's   :5       NA's   :5      NA's   :5                        \n          geometry  \n MULTIPOLYGON :622  \n epsg:3826    :  0  \n +proj=tmer...:  0  \n                    \n                    \n                    \n                    \n\n\nThe output print above revealed that the are five missing values in H_CNT, P_CNT, M_CNT and F_CNT fields.\nHence, code chunk below is used to remove the five records with missing values from village sf data.frame.\n\nvillage &lt;- village %&gt;%\n  filter(!is.na(P_CNT)) \n\n\nany(is.na(village$H_CNT))\n\n[1] FALSE\n\nany(is.na(village$P_CNT))\n\n[1] FALSE\n\nany(is.na(village$M_CNT))\n\n[1] FALSE\n\nany(is.na(village$F_CNT))\n\n[1] FALSE\n\n\n\n\n\n\nWe will calculate the centroid of each village using the code chunk below. Then, compute the distance between the centroids of the villages and the stores.\n\nvillage_center &lt;- st_point_on_surface(village)\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\ndistmat &lt;- st_distance(village_center, stores) \n\nNext, the code chunk below is used to convert the distance matrix into long data.frame format\n\nrownames(distmat) &lt;- village$`VILLCODE`\ncolnames(distmat) &lt;- stores$`Store_CD`\n\ndistmat_long &lt;- distmat %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"VILLCODE\") %&gt;%\n  pivot_longer(\n    cols = -VILLCODE,\n    names_to = \"Store_CD\",\n    values_to = \"distance\"\n  ) %&gt;%\n  mutate(distance = as.numeric(distance)) \n\n\n\n\nCode chunk below will be used to build the interaction matrix required by MCI package.\n\ninteraction_matrix &lt;- distmat_long %&gt;%\n  left_join(\n    stores %&gt;% \n      st_drop_geometry() %&gt;% \n      select(Store_CD, Store_size),\n    by = \"Store_CD\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\n\nsummary(interaction_matrix)\n\n   VILLCODE           Store_CD           Store_size     distance      \n Length:17276       Length:17276       Min.   :100   Min.   :  108.5  \n Class :character   Class :character   1st Qu.:100   1st Qu.: 6678.7  \n Mode  :character   Mode  :character   Median :100   Median :11965.3  \n                                       Mean   :100   Mean   :12762.7  \n                                       3rd Qu.:100   3rd Qu.:18122.3  \n                                       Max.   :100   Max.   :41395.4  \n\n\n\n\n\n\nhuff_share &lt;- huff.shares(\n  interaction_matrix, \"VILLCODE\", \"Store_CD\", \n  \"Store_size\", \"distance\")\n\n\n\nFirst, let us select market share data of a store by usign Store_CD field (i.e. TG).\n\nmarket_share &lt;- huff_share %&gt;%\n  filter(Store_CD == \"BT\")\n\nNext, trade areas of the selected will be derived by using the code chun below.\n\nTrade_area &lt;- village %&gt;%\n  left_join(market_share)\n\nJoining with `by = join_by(VILLCODE)`\n\n\nBefore plotting the trade area map, code chunk below is used to extract the selected store from stores sf data.frame.\n\nselected_store &lt;- stores %&gt;%\n  filter(Store_CD == \"BT\")\n\nNow, use the code chunk below to plot the trade areas of the selected store.\n\ntm_shape(Trade_area) +\n  tm_polygons(fill = \"p_ij\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                n = 10,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Patronage Probability\")) +\n  tm_shape(selected_store) +\n    tm_dots(size = 0.3,\n            fill = \"red\",\n            col = \"black\") +\n  tm_title(\"Distribution of Patronage Probability of Store BT by village\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha =0.2) \n\n\n\n\n\n\n\n\n\n\n\nOne of the usage of Huff model is to estimate the store sales by using village population.\nFirstly, let us append P_CNT field from village sf.data.frame to huff_share data.frame by using the code chunk below.\n\nhuff_share_pp &lt;- huff_share %&gt;%\n  left_join(village %&gt;% \n      st_drop_geometry() %&gt;% \n      select(VILLCODE, P_CNT),\n    by = \"VILLCODE\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\nNext, shares.total() of MCI package will be used to calculate the total sales by using the code chunk below.\n\nhuff_total &lt;- shares.total(\n  huff_share_pp, \"VILLCODE\", \"Store_CD\",\n  \"p_ij\", \"P_CNT\")\n\nCode chunk below is used to append the estimate store sales (Ej) onto stores sf data.frame.\n\nstore_sales_est &lt;- stores %&gt;%\n  left_join(huff_total,\n            by = join_by(\n              \"Store_CD\" == \"suppliers_single\"))\n\nNow, we can visualise how well the estimate sales as compare to the actual sales by using the code chunk below.\n\nstore_sales_nongeom &lt;- store_sales_est %&gt;% st_drop_geometry()\n\nggscatterstats(\n  store_sales_nongeom,\n  x = sum_E_j,\n  y = Total_Sales)\n\n`stat_xsidebin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_ysidebin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nhuff_share_wider &lt;- huff_share %&gt;%\n  select(VILLCODE, Store_CD, p_ij) %&gt;%\n    pivot_wider(names_from = Store_CD, \n                values_from = p_ij)"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#overview",
    "href": "In-Class_Ex10/in-class_ex10.html#overview",
    "title": "In-class Ex10",
    "section": "",
    "text": "Market area analysis defines and studies where customers originate, using demographics, spending, competition, and observation to guide location, marketing, and expansion decisions. The Huff model estimates store choice probabilities and market share from attractiveness and distance, helping businesses delineate trade areas and forecast revenue among competing outlets and potential profitability."
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#getting-started",
    "href": "In-Class_Ex10/in-class_ex10.html#getting-started",
    "title": "In-class Ex10",
    "section": "",
    "text": "Before we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.\n\npacman::p_load(sf, tidyverse, MCI, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#importing-data",
    "href": "In-Class_Ex10/in-class_ex10.html#importing-data",
    "title": "In-class Ex10",
    "section": "",
    "text": "First, st_read() of sf package will be used to import the three geospatial data sets into R environment by using the code chunk below.\n\nstores &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"STORES_TC\")\n\nReading layer `STORES_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 202702.9 ymin: 2662196 xmax: 232773.2 ymax: 2693557\nProjected CRS: TWD97 / TM2 zone 121\n\ntown &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"TOWN_TC\")\n\nReading layer `TOWN_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\nvillage &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"VILLAGE_TC\")\n\nReading layer `VILLAGE_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 622 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\n\nThe output print above shown that the three data sets are in TWD97 / TM2 zone 121 (i.e epsg: 3826) projected coordinates systems.\n\n\n\n\n\n\nImportant\n\n\n\nFor any geospatial analysis, it is important to check and confirm that all the geospatial data are in the similar projected coodinates system.\n\n\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(town) +\n  tm_polygons() +\ntm_shape(village) +\n  tm_polygons() + \ntm_shape(stores) +\n  tm_dots()\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\n\n\n\nNext, read_csv() of readr package will be used to import the two aspatial data sets into R environment.\n\npop2019 &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/pop2019.csv\")\n\nRows: 625 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): COUNTY, TOWN, V_ID, VILLAGE, INFO_TIME\ndbl (6): COUNTY_ID, TOWN_ID, H_CNT, P_CNT, M_CNT, F_CNT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstore_sales &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/Store_sales_data.csv\")\n\nRows: 5499 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Store Code, POD code\ndbl (3): Bills, Amount, Ave Bill\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsummary(store_sales)\n\n  Store Code          POD code             Bills            Amount       \n Length:5499        Length:5499        Min.   :   1.0   Min.   :      0  \n Class :character   Class :character   1st Qu.:  37.0   1st Qu.:  26702  \n Mode  :character   Mode  :character   Median : 186.0   Median : 139643  \n                                       Mean   : 331.9   Mean   : 236787  \n                                       3rd Qu.: 457.0   3rd Qu.: 331952  \n                                       Max.   :7764.0   Max.   :6076060  \n    Ave Bill      \n Min.   :    0.0  \n 1st Qu.:  661.3  \n Median :  730.5  \n Mean   :  825.1  \n 3rd Qu.:  862.2  \n Max.   :24356.0"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#data-preparation",
    "href": "In-Class_Ex10/in-class_ex10.html#data-preparation",
    "title": "In-class Ex10",
    "section": "",
    "text": "Code chunk below is used to aggregate the store sales data at Store Code.\n\nstore_sales_aggre &lt;- store_sales %&gt;%\n  group_by(`Store Code`) %&gt;%\n  summarise(\n    \"Total_Bill\" = sum(Bills, na.rm = TRUE),\n    \"Total_Sales\" = sum(Amount, na.rm = TRUE))\n\n\n\n\nIn the code chunk below, left_join() of dplyr package is used to append data from store_sales data.frame onto stores sf data.frame. At the same time a new field called Store_size has been created.\n\nstores &lt;- stores %&gt;%\n  left_join(store_sales_aggre,\n            by = join_by(\n              Store_CD == `Store Code`)) %&gt;%\n  mutate(Store_size = 100)\n\nNext, summary() is used to check if the are any missing values in the data.frame.\n\nsummary(stores)\n\n  Store_Name         Store_Addr          Store_CD           Total_Bill   \n Length:28          Length:28          Length:28          Min.   : 1767  \n Class :character   Class :character   Class :character   1st Qu.: 5698  \n Mode  :character   Mode  :character   Mode  :character   Median : 8634  \n                                                          Mean   : 8021  \n                                                          3rd Qu.:10349  \n                                                          Max.   :14468  \n  Total_Sales               geometry    Store_size \n Min.   :1475963   POINT        :28   Min.   :100  \n 1st Qu.:4044821   epsg:3826    : 0   1st Qu.:100  \n Median :6116336   +proj=tmer...: 0   Median :100  \n Mean   :5542852                      Mean   :100  \n 3rd Qu.:6959911                      3rd Qu.:100  \n Max.   :8234234                      Max.   :100  \n\n\n\n\n\n\nglimpse(pop2019)\n\nRows: 625\nColumns: 11\n$ COUNTY_ID &lt;dbl&gt; 66000, 66000, 66000, 66000, 66000, 66000, 66000, 66000, 6600…\n$ COUNTY    &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市…\n$ TOWN_ID   &lt;dbl&gt; 66000110, 66000110, 66000110, 66000110, 66000110, 66000110, …\n$ TOWN      &lt;chr&gt; \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區…\n$ V_ID      &lt;chr&gt; \"66000110-002\", \"66000110-009\", \"66000110-021\", \"66000110-00…\n$ VILLAGE   &lt;chr&gt; \"大甲里\", \"中山里\", \"太白里\", \"孔門里\", \"文曲里\", \"文武里\", \"日南里\", \"平安里\", \"江南里…\n$ H_CNT     &lt;dbl&gt; 454, 1781, 406, 383, 355, 2071, 2039, 785, 461, 559, 769, 80…\n$ P_CNT     &lt;dbl&gt; 1255, 5252, 1471, 1001, 1264, 6687, 6739, 2642, 1618, 1977, …\n$ M_CNT     &lt;dbl&gt; 626, 2533, 758, 538, 640, 3244, 3406, 1310, 838, 983, 1273, …\n$ F_CNT     &lt;dbl&gt; 629, 2719, 713, 463, 624, 3443, 3333, 1332, 780, 994, 1239, …\n$ INFO_TIME &lt;chr&gt; \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y…\n\n\n\nglimpse(village)\n\nRows: 622\nColumns: 11\n$ VILLCODE   &lt;chr&gt; \"66000110029\", \"66000120001\", \"66000120002\", \"66000120003\",…\n$ COUNTYNAME &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中…\n$ TOWNNAME   &lt;chr&gt; \"大甲區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水…\n$ VILLNAME   &lt;chr&gt; \"建興里\", \"鰲峰里\", \"靈泉里\", \"清水里\", \"文昌里\", \"南寧里\", \"西寧里\", \"北寧里\", \"中興…\n$ VILLENG    &lt;chr&gt; \"Jianxing Vil.\", \"Aofeng Vil.\", \"Lingquan Vil.\", \"Qingshui …\n$ COUNTYID   &lt;chr&gt; \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\",…\n$ COUNTYCODE &lt;chr&gt; \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"6600…\n$ TOWNID     &lt;chr&gt; \"B11\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B1…\n$ TOWNCODE   &lt;chr&gt; \"66000110\", \"66000120\", \"66000120\", \"66000120\", \"66000120\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((211953.9 27..., MULTIPOLYGON (…\n\n\nThe plot above shown that the village ID (i.e. V_ID) in pop2019 data.frame is not in the same structure as the village code in village sf data.frame.\nCode chunk below will be used to remove the “-” sign from the values.\n\npop2019 &lt;- pop2019 %&gt;%\n  mutate(V_ID = str_remove_all(\n    V_ID, \"-\"))\n\nNow, we are ready to append data in pop2019 data.frame onto village sf data.frame by using left_join() of dplyr package.\n\nvillage &lt;- village %&gt;%\n  left_join(pop2019,\n            by = join_by(\n              VILLCODE == V_ID))\n\n\n\n\n\nsummary(village)\n\n   VILLCODE          COUNTYNAME          TOWNNAME           VILLNAME        \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   VILLENG            COUNTYID          COUNTYCODE           TOWNID         \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   TOWNCODE             NOTE             COUNTY_ID        COUNTY         \n Length:622         Length:622         Min.   :66000   Length:622        \n Class :character   Class :character   1st Qu.:66000   Class :character  \n Mode  :character   Mode  :character   Median :66000   Mode  :character  \n                                       Mean   :66000                     \n                                       3rd Qu.:66000                     \n                                       Max.   :66000                     \n                                       NA's   :5                         \n    TOWN_ID            TOWN             VILLAGE              H_CNT     \n Min.   :6.6e+07   Length:622         Length:622         Min.   : 180  \n 1st Qu.:6.6e+07   Class :character   Class :character   1st Qu.: 724  \n Median :6.6e+07   Mode  :character   Mode  :character   Median :1335  \n Mean   :6.6e+07                                         Mean   :1592  \n 3rd Qu.:6.6e+07                                         3rd Qu.:2195  \n Max.   :6.6e+07                                         Max.   :7599  \n NA's   :5                                               NA's   :5     \n     P_CNT           M_CNT          F_CNT       INFO_TIME        \n Min.   :  487   Min.   : 257   Min.   : 230   Length:622        \n 1st Qu.: 2139   1st Qu.:1095   1st Qu.:1053   Class :character  \n Median : 3862   Median :1911   Median :1934   Mode  :character  \n Mean   : 4545   Mean   :2234   Mean   :2311                     \n 3rd Qu.: 6151   3rd Qu.:3022   3rd Qu.:3171                     \n Max.   :18652   Max.   :8741   Max.   :9911                     \n NA's   :5       NA's   :5      NA's   :5                        \n          geometry  \n MULTIPOLYGON :622  \n epsg:3826    :  0  \n +proj=tmer...:  0  \n                    \n                    \n                    \n                    \n\n\nThe output print above revealed that the are five missing values in H_CNT, P_CNT, M_CNT and F_CNT fields.\nHence, code chunk below is used to remove the five records with missing values from village sf data.frame.\n\nvillage &lt;- village %&gt;%\n  filter(!is.na(P_CNT)) \n\n\nany(is.na(village$H_CNT))\n\n[1] FALSE\n\nany(is.na(village$P_CNT))\n\n[1] FALSE\n\nany(is.na(village$M_CNT))\n\n[1] FALSE\n\nany(is.na(village$F_CNT))\n\n[1] FALSE"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#computing-distance-matrix",
    "href": "In-Class_Ex10/in-class_ex10.html#computing-distance-matrix",
    "title": "In-class Ex10",
    "section": "",
    "text": "We will calculate the centroid of each village using the code chunk below. Then, compute the distance between the centroids of the villages and the stores.\n\nvillage_center &lt;- st_point_on_surface(village)\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\ndistmat &lt;- st_distance(village_center, stores) \n\nNext, the code chunk below is used to convert the distance matrix into long data.frame format\n\nrownames(distmat) &lt;- village$`VILLCODE`\ncolnames(distmat) &lt;- stores$`Store_CD`\n\ndistmat_long &lt;- distmat %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"VILLCODE\") %&gt;%\n  pivot_longer(\n    cols = -VILLCODE,\n    names_to = \"Store_CD\",\n    values_to = \"distance\"\n  ) %&gt;%\n  mutate(distance = as.numeric(distance))"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#preparing-interaction-matrix",
    "href": "In-Class_Ex10/in-class_ex10.html#preparing-interaction-matrix",
    "title": "In-class Ex10",
    "section": "",
    "text": "Code chunk below will be used to build the interaction matrix required by MCI package.\n\ninteraction_matrix &lt;- distmat_long %&gt;%\n  left_join(\n    stores %&gt;% \n      st_drop_geometry() %&gt;% \n      select(Store_CD, Store_size),\n    by = \"Store_CD\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\n\nsummary(interaction_matrix)\n\n   VILLCODE           Store_CD           Store_size     distance      \n Length:17276       Length:17276       Min.   :100   Min.   :  108.5  \n Class :character   Class :character   1st Qu.:100   1st Qu.: 6678.7  \n Mode  :character   Mode  :character   Median :100   Median :11965.3  \n                                       Mean   :100   Mean   :12762.7  \n                                       3rd Qu.:100   3rd Qu.:18122.3  \n                                       Max.   :100   Max.   :41395.4"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#computing-huff-model",
    "href": "In-Class_Ex10/in-class_ex10.html#computing-huff-model",
    "title": "In-class Ex10",
    "section": "",
    "text": "huff_share &lt;- huff.shares(\n  interaction_matrix, \"VILLCODE\", \"Store_CD\", \n  \"Store_size\", \"distance\")\n\n\n\nFirst, let us select market share data of a store by usign Store_CD field (i.e. TG).\n\nmarket_share &lt;- huff_share %&gt;%\n  filter(Store_CD == \"BT\")\n\nNext, trade areas of the selected will be derived by using the code chun below.\n\nTrade_area &lt;- village %&gt;%\n  left_join(market_share)\n\nJoining with `by = join_by(VILLCODE)`\n\n\nBefore plotting the trade area map, code chunk below is used to extract the selected store from stores sf data.frame.\n\nselected_store &lt;- stores %&gt;%\n  filter(Store_CD == \"BT\")\n\nNow, use the code chunk below to plot the trade areas of the selected store.\n\ntm_shape(Trade_area) +\n  tm_polygons(fill = \"p_ij\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                n = 10,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Patronage Probability\")) +\n  tm_shape(selected_store) +\n    tm_dots(size = 0.3,\n            fill = \"red\",\n            col = \"black\") +\n  tm_title(\"Distribution of Patronage Probability of Store BT by village\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha =0.2) \n\n\n\n\n\n\n\n\n\n\n\nOne of the usage of Huff model is to estimate the store sales by using village population.\nFirstly, let us append P_CNT field from village sf.data.frame to huff_share data.frame by using the code chunk below.\n\nhuff_share_pp &lt;- huff_share %&gt;%\n  left_join(village %&gt;% \n      st_drop_geometry() %&gt;% \n      select(VILLCODE, P_CNT),\n    by = \"VILLCODE\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\nNext, shares.total() of MCI package will be used to calculate the total sales by using the code chunk below.\n\nhuff_total &lt;- shares.total(\n  huff_share_pp, \"VILLCODE\", \"Store_CD\",\n  \"p_ij\", \"P_CNT\")\n\nCode chunk below is used to append the estimate store sales (Ej) onto stores sf data.frame.\n\nstore_sales_est &lt;- stores %&gt;%\n  left_join(huff_total,\n            by = join_by(\n              \"Store_CD\" == \"suppliers_single\"))\n\nNow, we can visualise how well the estimate sales as compare to the actual sales by using the code chunk below.\n\nstore_sales_nongeom &lt;- store_sales_est %&gt;% st_drop_geometry()\n\nggscatterstats(\n  store_sales_nongeom,\n  x = sum_E_j,\n  y = Total_Sales)\n\n`stat_xsidebin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_ysidebin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nhuff_share_wider &lt;- huff_share %&gt;%\n  select(VILLCODE, Store_CD, p_ij) %&gt;%\n    pivot_wider(names_from = Store_CD, \n                values_from = p_ij)"
  },
  {
    "objectID": "In-Class_Ex10/data/geospatial/STORES_TC1.html",
    "href": "In-Class_Ex10/data/geospatial/STORES_TC1.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                PROJCRS[“TWD97 / TM2 zone 121”,BASEGEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],ID[“EPSG”,3824]],CONVERSION[“Taiwan 2-degree TM zone 121”,METHOD[“Transverse Mercator”,ID[“EPSG”,9807]],PARAMETER[“Latitude of natural origin”,0,ANGLEUNIT[“degree”,0.0174532925199433],ID[“EPSG”,8801]],PARAMETER[“Longitude of natural origin”,121,ANGLEUNIT[“degree”,0.0174532925199433],ID[“EPSG”,8802]],PARAMETER[“Scale factor at natural origin”,0.9999,SCALEUNIT[“unity”,1],ID[“EPSG”,8805]],PARAMETER[“False easting”,250000,LENGTHUNIT[“metre”,1],ID[“EPSG”,8806]],PARAMETER[“False northing”,0,LENGTHUNIT[“metre”,1],ID[“EPSG”,8807]]],CS[Cartesian,2],AXIS[“easting (X)”,east,ORDER[1],LENGTHUNIT[“metre”,1]],AXIS[“northing (Y)”,north,ORDER[2],LENGTHUNIT[“metre”,1]],USAGE[SCOPE[“Engineering survey, topographic mapping.”],AREA[“Taiwan, Republic of China - between 120°E and 122°E, onshore and offshore - Taiwan Island.”],BBOX[20.41,119.99,26.72,122.06]],ID[“EPSG”,3826]] +proj=tmerc +lat_0=0 +lon_0=121 +k=0.9999 +x_0=250000 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 27234 3826 EPSG:3826 TWD97 / TM2 zone 121 tmerc EPSG:7019 false"
  },
  {
    "objectID": "In-Class_Ex10/data/geospatial/VILLAGE_TC.html",
    "href": "In-Class_Ex10/data/geospatial/VILLAGE_TC.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       PROJCRS[“TWD97 / TM2 zone 121”,BASEGEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],ID[“EPSG”,3824]],CONVERSION[“Taiwan 2-degree TM zone 121”,METHOD[“Transverse Mercator”,ID[“EPSG”,9807]],PARAMETER[“Latitude of natural origin”,0,ANGLEUNIT[“degree”,0.0174532925199433],ID[“EPSG”,8801]],PARAMETER[“Longitude of natural origin”,121,ANGLEUNIT[“degree”,0.0174532925199433],ID[“EPSG”,8802]],PARAMETER[“Scale factor at natural origin”,0.9999,SCALEUNIT[“unity”,1],ID[“EPSG”,8805]],PARAMETER[“False easting”,250000,LENGTHUNIT[“metre”,1],ID[“EPSG”,8806]],PARAMETER[“False northing”,0,LENGTHUNIT[“metre”,1],ID[“EPSG”,8807]]],CS[Cartesian,2],AXIS[“easting (X)”,east,ORDER[1],LENGTHUNIT[“metre”,1]],AXIS[“northing (Y)”,north,ORDER[2],LENGTHUNIT[“metre”,1]],USAGE[SCOPE[“Engineering survey, topographic mapping.”],AREA[“Taiwan, Republic of China - between 120°E and 122°E, onshore and offshore - Taiwan Island.”],BBOX[20.41,119.99,26.72,122.06]],ID[“EPSG”,3826]] +proj=tmerc +lat_0=0 +lon_0=121 +k=0.9999 +x_0=250000 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 27234 3826 EPSG:3826 TWD97 / TM2 zone 121 tmerc EPSG:7019 false"
  },
  {
    "objectID": "In-Class_Ex10/data/geospatial/TOWN_TC.html",
    "href": "In-Class_Ex10/data/geospatial/TOWN_TC.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.”],BBOX[17.36,114.32,26.96,123.61]],ID[“EPSG”,3824]] +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs 27230 3824 EPSG:3824 TWD97 longlat EPSG:7019 true"
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#importing-geospatial-data",
    "href": "In-Class_Ex10/in-class_ex10.html#importing-geospatial-data",
    "title": "In-class Ex10",
    "section": "",
    "text": "stores &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"STORES_TC\")\n\nReading layer `STORES_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 202702.9 ymin: 2662196 xmax: 232773.2 ymax: 2693557\nProjected CRS: TWD97 / TM2 zone 121\n\ntown &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"TOWN_TC\")\n\nReading layer `TOWN_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\nvillage &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"VILLAGE_TC\")\n\nReading layer `VILLAGE_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 622 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\n\nThe output print above shown that the three data sets are in TWD97 / TM2 zone 121 (i.e epsg: 3826) projected coordinates systems.\n\n\n\n\n\n\nImportant\n\n\n\nFor any geospatial analysis, it is important to check and confirm that all the geospatial data are in the similar projected coodinates system.\n\n\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(town) +\n  tm_polygons() +\ntm_shape(village) +\n  tm_polygons() + \ntm_shape(stores) +\n  tm_dots()\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\"."
  },
  {
    "objectID": "In-Class_Ex10/in-class_ex10.html#importing-aspatial-data",
    "href": "In-Class_Ex10/in-class_ex10.html#importing-aspatial-data",
    "title": "In-class Ex10",
    "section": "",
    "text": "Next, read_csv() of readr package will be used to import the two aspatial data sets into R environment.\n\npop2019 &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/pop2019.csv\")\n\nRows: 625 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): COUNTY, TOWN, V_ID, VILLAGE, INFO_TIME\ndbl (6): COUNTY_ID, TOWN_ID, H_CNT, P_CNT, M_CNT, F_CNT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstore_sales &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/Store_sales_data.csv\")\n\nRows: 5499 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Store Code, POD code\ndbl (3): Bills, Amount, Ave Bill\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsummary(store_sales)\n\n  Store Code          POD code             Bills            Amount       \n Length:5499        Length:5499        Min.   :   1.0   Min.   :      0  \n Class :character   Class :character   1st Qu.:  37.0   1st Qu.:  26702  \n Mode  :character   Mode  :character   Median : 186.0   Median : 139643  \n                                       Mean   : 331.9   Mean   : 236787  \n                                       3rd Qu.: 457.0   3rd Qu.: 331952  \n                                       Max.   :7764.0   Max.   :6076060  \n    Ave Bill      \n Min.   :    0.0  \n 1st Qu.:  661.3  \n Median :  730.5  \n Mean   :  825.1  \n 3rd Qu.:  862.2  \n Max.   :24356.0"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html",
    "href": "Hands-on_Ex10/hand-on_ex10.html",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "",
    "text": "Market area analysis (also known as Trade Area Analysis) is the process of defining and studying the geographical area from which a business draws its customers. It involves analyzing data on consumer demographics, spending habits, and competitors to understand market viability, identify opportunities, and inform decisions about location, marketing, and expansion. It often combines empirical observation, like customer spotting, with mathematical models to identify market segments and understand customer behavior.\nHuff model is a geospatial analysis method commonly used in market area analysis to predict the probability that a consumer from a specific area will patronize a particular store, based on the store’s attractiveness and distance relative to competitors. It calculates a store’s market share by considering factors like size or sales for attractiveness and distance or travel time, with the probability decreasing as distance or competition increases. This tool helps businesses assess potential revenue, define trade areas, and make informed decisions about site selection\n\n\nIn this hands-on exercise, you will gain hands-on experience on how to perform market area analysis by using Huff model of MCI packages.\nBy the end of this hands-on exercise, you will be able:\n\nto import GIS data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto import aspatial data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto clean and tidy the data sets so that they are ready for the analysis,\nto compute distance matrix between the origins (i.e. villages) and destinations (i.e stores),\nto computer interaction probability using Huff model, and\nto visualise the market area by using tmap and ggplot2 packages.\n\n\n\n\nA fast-food franchise is a business model in which an individual (the franchisee) acquires the rights to operate a restaurant under an established brand owned by a franchisor. This arrangement typically involves franchise fees and adherence to standardized operational procedures. Key considerations prior to investment include the scale of initial capital outlay, the strength of the brand, and the strategic importance of store location. Capital investment requirements vary significantly, generally ranging from approximately $200,000 to more than $2 million. Major global fast-food franchise brands include McDonald’s, Burger King, Pizza Hut, KFC, and Taco Bell.\nIn this exercise, you will assume the role of a data analyst within the business development team of a fast-food franchise. Your objective is to delineate the trade areas of existing stores in Taichung City and to produce corresponding spatial maps to support market analysis and strategic decision-making.\n\n\n\nFour data sets will be used in this hands-on exercise, they are:\n\nGeospatial data\n\nSTORES_TN, store locations data in ESRI shapefile format.\nVILLAGE_TN, village boundary data in ESRI shapefile format.\n\nAspatial data\n\npop2019.csv, population data of Taichung City in 2019. H_CNT, P_CNT, M_CNT, and F_CNT indicate Number of household, Total population, Male population and Female population respectively.\nStore_sales_data.csv provides store sales data, and"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#overview",
    "href": "Hands-on_Ex10/hand-on_ex10.html#overview",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "",
    "text": "Market area analysis (also known as Trade Area Analysis) is the process of defining and studying the geographical area from which a business draws its customers. It involves analyzing data on consumer demographics, spending habits, and competitors to understand market viability, identify opportunities, and inform decisions about location, marketing, and expansion. It often combines empirical observation, like customer spotting, with mathematical models to identify market segments and understand customer behavior.\nHuff model is a geospatial analysis method commonly used in market area analysis to predict the probability that a consumer from a specific area will patronize a particular store, based on the store’s attractiveness and distance relative to competitors. It calculates a store’s market share by considering factors like size or sales for attractiveness and distance or travel time, with the probability decreasing as distance or competition increases. This tool helps businesses assess potential revenue, define trade areas, and make informed decisions about site selection\n\n\nIn this hands-on exercise, you will gain hands-on experience on how to perform market area analysis by using Huff model of MCI packages.\nBy the end of this hands-on exercise, you will be able:\n\nto import GIS data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto import aspatial data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto clean and tidy the data sets so that they are ready for the analysis,\nto compute distance matrix between the origins (i.e. villages) and destinations (i.e stores),\nto computer interaction probability using Huff model, and\nto visualise the market area by using tmap and ggplot2 packages.\n\n\n\n\nA fast-food franchise is a business model in which an individual (the franchisee) acquires the rights to operate a restaurant under an established brand owned by a franchisor. This arrangement typically involves franchise fees and adherence to standardized operational procedures. Key considerations prior to investment include the scale of initial capital outlay, the strength of the brand, and the strategic importance of store location. Capital investment requirements vary significantly, generally ranging from approximately $200,000 to more than $2 million. Major global fast-food franchise brands include McDonald’s, Burger King, Pizza Hut, KFC, and Taco Bell.\nIn this exercise, you will assume the role of a data analyst within the business development team of a fast-food franchise. Your objective is to delineate the trade areas of existing stores in Taichung City and to produce corresponding spatial maps to support market analysis and strategic decision-making.\n\n\n\nFour data sets will be used in this hands-on exercise, they are:\n\nGeospatial data\n\nSTORES_TN, store locations data in ESRI shapefile format.\nVILLAGE_TN, village boundary data in ESRI shapefile format.\n\nAspatial data\n\npop2019.csv, population data of Taichung City in 2019. H_CNT, P_CNT, M_CNT, and F_CNT indicate Number of household, Total population, Male population and Female population respectively.\nStore_sales_data.csv provides store sales data, and"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#getting-started",
    "href": "Hands-on_Ex10/hand-on_ex10.html#getting-started",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.\nThe R packages need for this exercise are as follows:\n\nsf for handling geospatial data and performing geoprocessing function such as computing distance matrix,\nMCI for performing Huff and MCI modelling,\ntidyverse for performing data science tasks including visualisation,\nggstatsplot, an R package that extends the ggplot2 visualization package to combine data visualization with statistical analysis, creating plots that include statistical test results directly, and\ntmap for plotting cartographic quality thematic maps.\n\nThe code chunk below installs and launches these R packages into RStudio environment.\n\npacman::p_load(sf, tidyverse, MCI, tmap, ggstatsplot)"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#importing-data",
    "href": "Hands-on_Ex10/hand-on_ex10.html#importing-data",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Importing Data",
    "text": "Importing Data\n\nImporting geospatial data\nFirst, st_read() of sf package will be used to import the three geospatial data sets into R environment by using the code chunk below.\n\nstores &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"STORES_TC\")\n\nReading layer `STORES_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 202702.9 ymin: 2662196 xmax: 232773.2 ymax: 2693557\nProjected CRS: TWD97 / TM2 zone 121\n\ntown &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"TOWN_TC\")\n\nReading layer `TOWN_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\nvillage &lt;- st_read(\n  dsn = \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial\",\n  layer = \"VILLAGE_TC\")\n\nReading layer `VILLAGE_TC' from data source \n  `/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 622 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 194797.1 ymin: 2654887 xmax: 240450.2 ymax: 2703838\nProjected CRS: TWD97 / TM2 zone 121\n\n\nThe output print above shown that the three data sets are in TWD97 / TM2 zone 121 (i.e epsg: 3826) projected coordinates systems.\n\n\n\n\n\n\nImportant\n\n\n\nFor any geospatial analysis, it is important to check and confirm that all the geospatial data are in the similar projected coodinates system.\n\n\n\ntmap_mode(\"view\")\ntm_shape(town) +\n  tm_polygons() +\ntm_shape(village) +\n  tm_polygons() + \ntm_shape(stores) +\n  tm_dots()\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\nImporting aspatial data\nNext, read_csv() of readr package will be used to import the two aspatial data sets into R environment.\n\npop2019 &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/pop2019.csv\")\n\nstore_sales &lt;- read_csv(\n  \"/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/Store_sales_data.csv\")\n\n\nsummary(store_sales)\n\n  Store Code          POD code             Bills            Amount       \n Length:5499        Length:5499        Min.   :   1.0   Min.   :      0  \n Class :character   Class :character   1st Qu.:  37.0   1st Qu.:  26702  \n Mode  :character   Mode  :character   Median : 186.0   Median : 139643  \n                                       Mean   : 331.9   Mean   : 236787  \n                                       3rd Qu.: 457.0   3rd Qu.: 331952  \n                                       Max.   :7764.0   Max.   :6076060  \n    Ave Bill      \n Min.   :    0.0  \n 1st Qu.:  661.3  \n Median :  730.5  \n Mean   :  825.1  \n 3rd Qu.:  862.2  \n Max.   :24356.0"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#data-preparation",
    "href": "Hands-on_Ex10/hand-on_ex10.html#data-preparation",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nAggregating store sales\nCode chunk below is used to aggregate the store sales data at Store Code.\n\nstore_sales_aggre &lt;- store_sales %&gt;%\n  group_by(`Store Code`) %&gt;%\n  summarise(\n    \"Total_Bill\" = sum(Bills, na.rm = TRUE),\n    \"Total_Sales\" = sum(Amount, na.rm = TRUE))\n\n\n\nRelational join\nIn the code chunk below, left_join() of dplyr package is used to append data from store_sales data.frame onto stores sf data.frame. At the same time a new field called Store_size has been created.\n\nstores &lt;- stores %&gt;%\n  left_join(store_sales_aggre,\n            by = join_by(\n              Store_CD == `Store Code`)) %&gt;%\n  mutate(Store_size = 100)\n\nNext, summary() is used to check if the are any missing values in the data.frame.\n\nsummary(stores)\n\n  Store_Name         Store_Addr          Store_CD           Total_Bill   \n Length:28          Length:28          Length:28          Min.   : 1767  \n Class :character   Class :character   Class :character   1st Qu.: 5698  \n Mode  :character   Mode  :character   Mode  :character   Median : 8634  \n                                                          Mean   : 8021  \n                                                          3rd Qu.:10349  \n                                                          Max.   :14468  \n  Total_Sales               geometry    Store_size \n Min.   :1475963   POINT        :28   Min.   :100  \n 1st Qu.:4044821   epsg:3826    : 0   1st Qu.:100  \n Median :6116336   +proj=tmer...: 0   Median :100  \n Mean   :5542852                      Mean   :100  \n 3rd Qu.:6959911                      3rd Qu.:100  \n Max.   :8234234                      Max.   :100  \n\n\n\n\nTidying pop2019 data.frame\nWe are planning to append data in pop2019 data.frame onto village sf data.frame. Before we can do so, it is wise to check in to values in the join field are the same.\n\nglimpse(pop2019)\n\nRows: 625\nColumns: 11\n$ COUNTY_ID &lt;dbl&gt; 66000, 66000, 66000, 66000, 66000, 66000, 66000, 66000, 6600…\n$ COUNTY    &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市…\n$ TOWN_ID   &lt;dbl&gt; 66000110, 66000110, 66000110, 66000110, 66000110, 66000110, …\n$ TOWN      &lt;chr&gt; \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區\", \"大甲區…\n$ V_ID      &lt;chr&gt; \"66000110-002\", \"66000110-009\", \"66000110-021\", \"66000110-00…\n$ VILLAGE   &lt;chr&gt; \"大甲里\", \"中山里\", \"太白里\", \"孔門里\", \"文曲里\", \"文武里\", \"日南里\", \"平安里\", \"江南里…\n$ H_CNT     &lt;dbl&gt; 454, 1781, 406, 383, 355, 2071, 2039, 785, 461, 559, 769, 80…\n$ P_CNT     &lt;dbl&gt; 1255, 5252, 1471, 1001, 1264, 6687, 6739, 2642, 1618, 1977, …\n$ M_CNT     &lt;dbl&gt; 626, 2533, 758, 538, 640, 3244, 3406, 1310, 838, 983, 1273, …\n$ F_CNT     &lt;dbl&gt; 629, 2719, 713, 463, 624, 3443, 3333, 1332, 780, 994, 1239, …\n$ INFO_TIME &lt;chr&gt; \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y12M\", \"108Y…\n\n\n\nglimpse(village)\n\nRows: 622\nColumns: 11\n$ VILLCODE   &lt;chr&gt; \"66000110029\", \"66000120001\", \"66000120002\", \"66000120003\",…\n$ COUNTYNAME &lt;chr&gt; \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中市\", \"臺中…\n$ TOWNNAME   &lt;chr&gt; \"大甲區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水區\", \"清水…\n$ VILLNAME   &lt;chr&gt; \"建興里\", \"鰲峰里\", \"靈泉里\", \"清水里\", \"文昌里\", \"南寧里\", \"西寧里\", \"北寧里\", \"中興…\n$ VILLENG    &lt;chr&gt; \"Jianxing Vil.\", \"Aofeng Vil.\", \"Lingquan Vil.\", \"Qingshui …\n$ COUNTYID   &lt;chr&gt; \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\",…\n$ COUNTYCODE &lt;chr&gt; \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"66000\", \"6600…\n$ TOWNID     &lt;chr&gt; \"B11\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B1…\n$ TOWNCODE   &lt;chr&gt; \"66000110\", \"66000120\", \"66000120\", \"66000120\", \"66000120\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((211953.9 27..., MULTIPOLYGON (…\n\n\nThe plot above shown that the village ID (i.e. V_ID) in pop2019 data.frame is not in the same structure as the village code in village sf data.frame.\nCode chunk below will be used to remove the “-” sign from the values.\n\npop2019 &lt;- pop2019 %&gt;%\n  mutate(V_ID = str_remove_all(\n    V_ID, \"-\"))\n\nNow, we are ready to append data in pop2019 data.frame onto village sf data.frame by using left_join() of dplyr package.\n\nvillage &lt;- village %&gt;%\n  left_join(pop2019,\n            by = join_by(\n              VILLCODE == V_ID))\n\n\n\nCheck for missing or zero values\nCode chunk below is used to check in any records with missing values.\n\nsummary(village)\n\n   VILLCODE          COUNTYNAME          TOWNNAME           VILLNAME        \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   VILLENG            COUNTYID          COUNTYCODE           TOWNID         \n Length:622         Length:622         Length:622         Length:622        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   TOWNCODE             NOTE             COUNTY_ID        COUNTY         \n Length:622         Length:622         Min.   :66000   Length:622        \n Class :character   Class :character   1st Qu.:66000   Class :character  \n Mode  :character   Mode  :character   Median :66000   Mode  :character  \n                                       Mean   :66000                     \n                                       3rd Qu.:66000                     \n                                       Max.   :66000                     \n                                       NA's   :5                         \n    TOWN_ID            TOWN             VILLAGE              H_CNT     \n Min.   :6.6e+07   Length:622         Length:622         Min.   : 180  \n 1st Qu.:6.6e+07   Class :character   Class :character   1st Qu.: 724  \n Median :6.6e+07   Mode  :character   Mode  :character   Median :1335  \n Mean   :6.6e+07                                         Mean   :1592  \n 3rd Qu.:6.6e+07                                         3rd Qu.:2195  \n Max.   :6.6e+07                                         Max.   :7599  \n NA's   :5                                               NA's   :5     \n     P_CNT           M_CNT          F_CNT       INFO_TIME        \n Min.   :  487   Min.   : 257   Min.   : 230   Length:622        \n 1st Qu.: 2139   1st Qu.:1095   1st Qu.:1053   Class :character  \n Median : 3862   Median :1911   Median :1934   Mode  :character  \n Mean   : 4545   Mean   :2234   Mean   :2311                     \n 3rd Qu.: 6151   3rd Qu.:3022   3rd Qu.:3171                     \n Max.   :18652   Max.   :8741   Max.   :9911                     \n NA's   :5       NA's   :5      NA's   :5                        \n          geometry  \n MULTIPOLYGON :622  \n epsg:3826    :  0  \n +proj=tmer...:  0  \n                    \n                    \n                    \n                    \n\n\nThe output print above revealed that the are five missing values in H_CNT, P_CNT, M_CNT and F_CNT fields.\nHence, code chunk below is used to remove the five records with missing values from village sf data.frame.\n\nvillage &lt;- village %&gt;%\n  filter(!is.na(P_CNT)) \n\n\nany(is.na(village$H_CNT))\n\n[1] FALSE\n\nany(is.na(village$P_CNT))\n\n[1] FALSE\n\nany(is.na(village$M_CNT))\n\n[1] FALSE\n\nany(is.na(village$F_CNT))\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#computing-distance-matrix",
    "href": "Hands-on_Ex10/hand-on_ex10.html#computing-distance-matrix",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Computing Distance Matrix",
    "text": "Computing Distance Matrix\nInstead of calculate the distance between the villages and stores, we will calculate the centroid of each village using the code chunk below. Then, compute the distance between the centroids of the villages and the stores.\n\nvillage_center &lt;- st_point_on_surface(village)\ndistmat &lt;- st_distance(village_center, stores) \n\nNext, the code chunk below is used to convert the distance matrix into long data.frame format\n\nrownames(distmat) &lt;- village$`VILLCODE`\ncolnames(distmat) &lt;- stores$`Store_CD`\n\ndistmat_long &lt;- distmat %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"VILLCODE\") %&gt;%\n  pivot_longer(\n    cols = -VILLCODE,\n    names_to = \"Store_CD\",\n    values_to = \"distance\"\n  ) %&gt;%\n  mutate(distance = as.numeric(distance))"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#preparing-interaction-matrix",
    "href": "Hands-on_Ex10/hand-on_ex10.html#preparing-interaction-matrix",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Preparing Interaction Matrix",
    "text": "Preparing Interaction Matrix\nCode chunk below will be used to build the interaction matrix required by MCI package.\n\ninteraction_matrix &lt;- distmat_long %&gt;%\n  left_join(\n    stores %&gt;% \n      st_drop_geometry() %&gt;% \n      select(Store_CD, Store_size),\n    by = \"Store_CD\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\n\nsummary(interaction_matrix)\n\n   VILLCODE           Store_CD           Store_size     distance      \n Length:17276       Length:17276       Min.   :100   Min.   :  108.5  \n Class :character   Class :character   1st Qu.:100   1st Qu.: 6678.7  \n Mode  :character   Mode  :character   Median :100   Median :11965.3  \n                                       Mean   :100   Mean   :12762.7  \n                                       3rd Qu.:100   3rd Qu.:18122.3  \n                                       Max.   :100   Max.   :41395.4"
  },
  {
    "objectID": "Hands-on_Ex10/hand-on_ex10.html#computing-huff-model",
    "href": "Hands-on_Ex10/hand-on_ex10.html#computing-huff-model",
    "title": "Market Area Analysis for Retail and Service Locations using Huff model",
    "section": "Computing Huff model",
    "text": "Computing Huff model\n\nhuff_share &lt;- huff.shares(\n  interaction_matrix, \"VILLCODE\", \"Store_CD\", \n  \"Store_size\", \"distance\")\n\n\nTrade area analysis for a single store\nFirst, let us select market share data of a store by usign Store_CD field (i.e. TG).\n\nmarket_share &lt;- huff_share %&gt;%\n  filter(Store_CD == \"TG\")\n\nNext, trade areas of the selected will be derived by using the code chun below.\n\nTrade_area &lt;- village %&gt;%\n  left_join(market_share)\n\nBefore plotting the trade area map, code chunk below is used to extract the selected store from stores sf data.frame.\n\nselected_store &lt;- stores %&gt;%\n  filter(Store_CD == \"TG\")\n\nNow, use the code chunk below to plot the trade areas of the selected store.\n\ntm_shape(Trade_area) +\n  tm_polygons(fill = \"p_ij\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                n = 10,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Patronage Probability\")) +\n  tm_shape(selected_store) +\n    tm_dots(size = 0.3,\n            fill = \"red\",\n            col = \"black\") +\n  tm_title(\"Distribution of Patronage Probability of Store BT by village\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha =0.2) \n\n\n\n\n\n\n\n\n\n\nStore sales analysis\nOne of the usage of Huff model is to estimate the store sales by using village population.\nFirstly, let us append P_CNT field from village sf.data.frame to huff_share data.frame by using the code chunk below.\n\nhuff_share_pp &lt;- huff_share %&gt;%\n  left_join(village %&gt;% \n      st_drop_geometry() %&gt;% \n      select(VILLCODE, P_CNT),\n    by = \"VILLCODE\") %&gt;%\n  relocate(\n    distance, .after = last_col())\n\nNext, shares.total() of MCI package will be used to calculate the total sales by using the code chunk below.\n\nhuff_total &lt;- shares.total(\n  huff_share_pp, \"VILLCODE\", \"Store_CD\",\n  \"p_ij\", \"P_CNT\")\n\nCode chunk below is used to append the estimate store sales (Ej) onto stores sf data.frame.\n\nstore_sales_est &lt;- stores %&gt;%\n  left_join(huff_total,\n            by = join_by(\n              \"Store_CD\" == \"suppliers_single\"))\n\nNow, we can visualise how well the estimate sales as compare to the actual sales by using the code chunk below.\n\nstore_sales_nongeom &lt;- store_sales_est %&gt;% st_drop_geometry()\n\nggscatterstats(\n  store_sales_nongeom,\n  x = sum_E_j,\n  y = Total_Sales)\n\n\n\n\n\n\n\n\n\nhuff_share_wider &lt;- huff_share %&gt;%\n  select(VILLCODE, Store_CD, p_ij) %&gt;%\n    pivot_wider(names_from = Store_CD, \n                values_from = p_ij)"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Take-home Ex03",
    "section": "10 Building Hedonic Pricing Models using GWmodel",
    "text": "10 Building Hedonic Pricing Models using GWmodel"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-geographical-weighted-model-gwmodel",
    "href": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-geographical-weighted-model-gwmodel",
    "title": "Take-home Ex03",
    "section": "10 Building Hedonic Pricing Models using Geographical Weighted Model (GWmodel)",
    "text": "10 Building Hedonic Pricing Models using Geographical Weighted Model (GWmodel)\n\n10.1 Building Fixed Bandwidth GWR Model\n\n10.1.1 Computing fixed bandwith\n\nbw.fixed &lt;- bw.gwr(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf, \n  approach=\"CV\", \n  kernel=\"gaussian\", \n  adaptive=FALSE, \n  longlat=FALSE)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nFixed bandwidth: 22980.96 CV score: 52450298182739 \nFixed bandwidth: 14205.85 CV score: 50405910455790 \nFixed bandwidth: 8782.54 CV score: 45194023569697 \nFixed bandwidth: 5430.749 CV score: 36056853683589 \nFixed bandwidth: 3359.228 CV score: 26759060010965 \nFixed bandwidth: 2078.957 CV score: 19585044380222 \nFixed bandwidth: 1287.706 CV score: 15825995510315 \nFixed bandwidth: 798.6866 CV score: 13361296871357 \nFixed bandwidth: 496.4557 CV score: 12103582525147 \nFixed bandwidth: 309.6667 CV score: 36921199956863 \nFixed bandwidth: 611.8976 CV score: 12510039567013 \nFixed bandwidth: 425.1087 CV score: 12671200502175 \nFixed bandwidth: 540.5506 CV score: 12201656586863 \nFixed bandwidth: 469.2035 CV score: 12139654424056 \nFixed bandwidth: 513.2984 CV score: 12125108557638 \nFixed bandwidth: 486.0463 CV score: 12104518521382 \nFixed bandwidth: 502.889 CV score: 12108862510624 \nFixed bandwidth: 492.4797 CV score: 12102437726921 \nFixed bandwidth: 490.0223 CV score: 12102626736951 \nFixed bandwidth: 493.9984 CV score: 12102670275509 \nFixed bandwidth: 491.541 CV score: 12102425706259 \nFixed bandwidth: 490.9609 CV score: 12102470054875 \nFixed bandwidth: 491.8996 CV score: 12102418327803 \nFixed bandwidth: 492.1211 CV score: 12102420967206 \nFixed bandwidth: 491.7626 CV score: 12102419312406 \nFixed bandwidth: 491.9842 CV score: 12102418602932 \nFixed bandwidth: 491.8473 CV score: 12102418417032 \nFixed bandwidth: 491.9319 CV score: 12102418341054 \nFixed bandwidth: 491.8796 CV score: 12102418232421 \nFixed bandwidth: 491.8672 CV score: 12102418227742 \nFixed bandwidth: 491.8596 CV score: 12102418304540 \nFixed bandwidth: 491.8719 CV score: 12102418287635 \nFixed bandwidth: 491.8643 CV score: 12102418263747 \nFixed bandwidth: 491.869 CV score: 12102418409363 \nFixed bandwidth: 491.8661 CV score: 12102418262534 \nFixed bandwidth: 491.8679 CV score: 12102418181241 \nFixed bandwidth: 491.8683 CV score: 12102418247709 \nFixed bandwidth: 491.8677 CV score: 12102418335646 \nFixed bandwidth: 491.8681 CV score: 12102418312924 \nFixed bandwidth: 491.8678 CV score: 12102418245347 \nFixed bandwidth: 491.868 CV score: 12102418189466 \nFixed bandwidth: 491.8679 CV score: 12102418375951 \nFixed bandwidth: 491.8679 CV score: 12102418221307 \n\n\n\n\n10.1.2 GWModel method - fixed bandwith\n\ngwr.fixed &lt;- gwr.basic(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf, \n  bw=bw.fixed, \n  kernel = 'gaussian', \n  longlat = FALSE)\n\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-11-16 04:40:19.385607 \n   Call:\n   gwr.basic(formula = resale_price ~ FLOOR_LEVEL + AGE_2025 + DIST_CBD_M + \n    DIST_MRT_M + DIST_ELDERCARE_M + DIST_HAWKER_M + DIST_SUPERMKT_M + \n    DIST_MALL_M + DIST_ELITEPRI_M + CNT_KINDER_350 + CNT_CHILD_350 + \n    CNT_BUS_350 + CNT_PRIMARY_1000, data = analysis_sf, bw = bw.fixed, \n    kernel = \"gaussian\", longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  FLOOR_LEVEL AGE_2025 DIST_CBD_M DIST_MRT_M DIST_ELDERCARE_M DIST_HAWKER_M DIST_SUPERMKT_M DIST_MALL_M DIST_ELITEPRI_M CNT_KINDER_350 CNT_CHILD_350 CNT_BUS_350 CNT_PRIMARY_1000\n   Number of data points: 8679\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-316047  -51215    -484   48553  496537 \n\n   Coefficients:\n                        Estimate   Std. Error t value             Pr(&gt;|t|)    \n   (Intercept)      1080611.4211    5875.0014 183.934 &lt; 0.0000000000000002 ***\n   FLOOR_LEVEL         5855.0336     146.3920  39.996 &lt; 0.0000000000000002 ***\n   AGE_2025           -5387.4524      65.4113 -82.363 &lt; 0.0000000000000002 ***\n   DIST_CBD_M           -19.8589       0.3272 -60.690 &lt; 0.0000000000000002 ***\n   DIST_MRT_M           -56.5047       2.6338 -21.454 &lt; 0.0000000000000002 ***\n   DIST_ELDERCARE_M     -10.4593       1.5036  -6.956     0.00000000000375 ***\n   DIST_HAWKER_M        -25.2966       1.9975 -12.664 &lt; 0.0000000000000002 ***\n   DIST_SUPERMKT_M       16.5813       4.9504   3.349             0.000813 ***\n   DIST_MALL_M          -27.4157       2.7139 -10.102 &lt; 0.0000000000000002 ***\n   DIST_ELITEPRI_M        1.8453       0.4214   4.379     0.00001208459969 ***\n   CNT_KINDER_350     10209.4729     930.8253  10.968 &lt; 0.0000000000000002 ***\n   CNT_CHILD_350      -1978.6793     465.9388  -4.247     0.00002192558270 ***\n   CNT_BUS_350         1446.4400     320.6957   4.510     0.00000655817847 ***\n   CNT_PRIMARY_1000  -11841.6313     654.1697 -18.102 &lt; 0.0000000000000002 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 78550 on 8665 degrees of freedom\n   Multiple R-squared: 0.7612\n   Adjusted R-squared: 0.7608 \n   F-statistic:  2124 on 13 and 8665 DF,  p-value: &lt; 0.00000000000000022 \n   ***Extra Diagnostic information\n   Residual sum of squares: 53466385356000\n   Sigma(hat): 78497.46\n   AIC:  220296.9\n   AICc:  220296.9\n   BIC:  211859.9\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 491.8679 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                             Min.       1st Qu.        Median       3rd Qu.\n   Intercept        -9720464.3458   487694.9682  1093934.4765  1802118.1148\n   FLOOR_LEVEL          1809.8367     3904.9994     4580.5478     5342.8646\n   AGE_2025           -61595.4149    -8070.3415    -5384.4583    -4004.4750\n   DIST_CBD_M          -1442.2074      -93.7489      -19.5136       35.1252\n   DIST_MRT_M          -1178.0496     -118.2303      -54.6497      -17.7726\n   DIST_ELDERCARE_M     -988.0563      -36.1880        7.7433       54.7307\n   DIST_HAWKER_M       -1561.4989      -61.9387      -22.1491       36.9564\n   DIST_SUPERMKT_M     -1081.9524      -42.1534        4.4410       45.8565\n   DIST_MALL_M         -2385.9815      -79.4840      -25.1418       20.6425\n   DIST_ELITEPRI_M      -968.7365      -76.8987       13.2962       84.9666\n   CNT_KINDER_350    -108801.8383    -7681.7316      -85.6126     4454.4133\n   CNT_CHILD_350      -66156.4170    -3111.3533     -188.8035     1904.6263\n   CNT_BUS_350        -39689.4923    -2049.3783      277.0907     2399.8068\n   CNT_PRIMARY_1000  -190274.4962    -4718.3556      -60.5195     4873.2269\n                           Max.\n   Intercept        15499574.52\n   FLOOR_LEVEL         23123.88\n   AGE_2025             5115.10\n   DIST_CBD_M           1399.65\n   DIST_MRT_M           2064.68\n   DIST_ELDERCARE_M      931.88\n   DIST_HAWKER_M         850.08\n   DIST_SUPERMKT_M      1778.69\n   DIST_MALL_M           911.68\n   DIST_ELITEPRI_M      1316.15\n   CNT_KINDER_350     193842.74\n   CNT_CHILD_350       29053.42\n   CNT_BUS_350         23583.98\n   CNT_PRIMARY_1000   118550.63\n   ************************Diagnostic information*************************\n   Number of data points: 8679 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1135.415 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 7543.585 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 206369.1 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 205232 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 203955.5 \n   Residual sum of squares: 8508164178768 \n   R-square value:  0.961995 \n   Adjusted R-square value:  0.956274 \n\n   ***********************************************************************\n   Program stops at: 2025-11-16 04:40:49.219911 \n\n\n\n\n\n10.2 Building Adaptive Bandwidth GWR Model\n\n10.2.1 Computing the adaptive bandwidth\n\nbw.adaptive &lt;- bw.gwr(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf,\n  approach=\"CV\", \n  kernel=\"gaussian\", \n  adaptive=TRUE, \n  longlat=FALSE)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 5371 CV score: 49481923930975 \nAdaptive bandwidth: 3327 CV score: 45839020041489 \nAdaptive bandwidth: 2063 CV score: 40633400665646 \nAdaptive bandwidth: 1282 CV score: 33618379057483 \nAdaptive bandwidth: 799 CV score: 27099356135801 \nAdaptive bandwidth: 501 CV score: 22600501644302 \nAdaptive bandwidth: 316 CV score: 19340718244687 \nAdaptive bandwidth: 202 CV score: 16719951482453 \nAdaptive bandwidth: 131 CV score: 14859639552516 \nAdaptive bandwidth: 88 CV score: 13517047664361 \nAdaptive bandwidth: 60 CV score: 12514316979553 \nAdaptive bandwidth: 44 CV score: 11784528221289 \nAdaptive bandwidth: 33 CV score: Inf \nAdaptive bandwidth: 50 CV score: 11994295816271 \nAdaptive bandwidth: 39 CV score: Inf \nAdaptive bandwidth: 45 CV score: 11818942838708 \nAdaptive bandwidth: 41 CV score: Inf \nAdaptive bandwidth: 43 CV score: 11750529486156 \nAdaptive bandwidth: 45 CV score: 11818942838708 \nAdaptive bandwidth: 44 CV score: 11784528221289 \nAdaptive bandwidth: 45 CV score: 11818942838708 \nAdaptive bandwidth: 44 CV score: 11784528221289 \nAdaptive bandwidth: 44 CV score: 11784528221289 \nAdaptive bandwidth: 43 CV score: 11750529486156 \n\n\n\n\n10.2.2 Constructing the adaptive bandwidth gwr model\n\ngwr.adaptive &lt;- gwr.basic(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf,\n  bw=bw.adaptive, \n  kernel = 'gaussian', \n  adaptive=TRUE, \n  longlat = FALSE)\n\n\n\n\n10.3 Visualising GWR Output\n\n\n10.4 Converting SDF into sf data.frame\n\nanalysis.sf.adaptive &lt;-\n  st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\nglimpse(analysis.sf.adaptive)\n\nRows: 8,679\nColumns: 49\n$ Intercept           &lt;dbl&gt; 1372611, 1378403, 1372611, 1220761, 1378403, 12207…\n$ FLOOR_LEVEL         &lt;dbl&gt; 5273.715, 5958.302, 5273.715, 7309.421, 5958.302, …\n$ AGE_2025            &lt;dbl&gt; -9043.527, -8096.687, -9043.527, -8171.505, -8096.…\n$ DIST_CBD_M          &lt;dbl&gt; -32.39158, -54.41816, -32.39158, -46.73114, -54.41…\n$ DIST_MRT_M          &lt;dbl&gt; -50.418513, 63.015593, -50.418513, 56.030906, 63.0…\n$ DIST_ELDERCARE_M    &lt;dbl&gt; 78.52689, 155.33493, 78.52689, 80.98130, 155.33493…\n$ DIST_HAWKER_M       &lt;dbl&gt; 58.60382, 59.75263, 58.60382, 108.04486, 59.75263,…\n$ DIST_SUPERMKT_M     &lt;dbl&gt; -13.027783, 113.331892, -13.027783, 45.809992, 113…\n$ DIST_MALL_M         &lt;dbl&gt; -18.78535, -87.24034, -18.78535, -15.50219, -87.24…\n$ DIST_ELITEPRI_M     &lt;dbl&gt; -49.532805, -50.985132, -49.532805, -3.041654, -50…\n$ CNT_KINDER_350      &lt;dbl&gt; 3637.694, 7968.247, 3637.694, 3589.385, 7968.247, …\n$ CNT_CHILD_350       &lt;dbl&gt; 3741.448, 2294.173, 3741.448, 3476.573, 2294.173, …\n$ CNT_BUS_350         &lt;dbl&gt; -74.14259, 5907.85122, -74.14259, 2819.22534, 5907…\n$ CNT_PRIMARY_1000    &lt;dbl&gt; -17636.8608, -2841.3086, -17636.8608, -2005.5695, …\n$ y                   &lt;dbl&gt; 582000, 865000, 530000, 560000, 1038000, 542000, 6…\n$ yhat                &lt;dbl&gt; 629711.9, 918046.1, 613890.7, 564810.8, 1025295.6,…\n$ residual            &lt;dbl&gt; -47711.8648, -53046.1246, -83890.7206, -4810.7857,…\n$ CV_Score            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Stud_residual       &lt;dbl&gt; -1.427150448, -1.590992056, -2.492682112, -0.14825…\n$ Intercept_SE        &lt;dbl&gt; 74748.95, 129409.76, 74748.95, 73622.46, 129409.76…\n$ FLOOR_LEVEL_SE      &lt;dbl&gt; 420.3160, 603.4554, 420.3160, 525.2847, 603.4554, …\n$ AGE_2025_SE         &lt;dbl&gt; 219.2896, 311.5762, 219.2896, 243.3810, 311.5762, …\n$ DIST_CBD_M_SE       &lt;dbl&gt; 6.382454, 13.189100, 6.382454, 8.895262, 13.189100…\n$ DIST_MRT_M_SE       &lt;dbl&gt; 14.85785, 18.65955, 14.85785, 14.34840, 18.65955, …\n$ DIST_ELDERCARE_M_SE &lt;dbl&gt; 12.26269, 17.88005, 12.26269, 13.78218, 17.88005, …\n$ DIST_HAWKER_M_SE    &lt;dbl&gt; 24.54045, 28.62644, 24.54045, 22.05642, 28.62644, …\n$ DIST_SUPERMKT_M_SE  &lt;dbl&gt; 23.92338, 37.99900, 23.92338, 24.69726, 37.99900, …\n$ DIST_MALL_M_SE      &lt;dbl&gt; 19.08808, 19.43619, 19.08808, 13.89996, 19.43619, …\n$ DIST_ELITEPRI_M_SE  &lt;dbl&gt; 8.684979, 10.006839, 8.684979, 7.908889, 10.006839…\n$ CNT_KINDER_350_SE   &lt;dbl&gt; 3515.048, 4582.405, 3515.048, 3927.370, 4582.405, …\n$ CNT_CHILD_350_SE    &lt;dbl&gt; 2624.351, 3464.307, 2624.351, 2303.601, 3464.307, …\n$ CNT_BUS_350_SE      &lt;dbl&gt; 1465.556, 1901.806, 1465.556, 1569.143, 1901.806, …\n$ CNT_PRIMARY_1000_SE &lt;dbl&gt; 4240.353, 5251.470, 4240.353, 4231.328, 5251.470, …\n$ Intercept_TV        &lt;dbl&gt; 18.36295, 10.65146, 18.36295, 16.58137, 10.65146, …\n$ FLOOR_LEVEL_TV      &lt;dbl&gt; 12.547023, 9.873642, 12.547023, 13.915162, 9.87364…\n$ AGE_2025_TV         &lt;dbl&gt; -41.24012, -25.98622, -41.24012, -33.57494, -25.98…\n$ DIST_CBD_M_TV       &lt;dbl&gt; -5.075098, -4.125995, -5.075098, -5.253486, -4.125…\n$ DIST_MRT_M_TV       &lt;dbl&gt; -3.3933918, 3.3771225, -3.3933918, 3.9050294, 3.37…\n$ DIST_ELDERCARE_M_TV &lt;dbl&gt; 6.403722, 8.687611, 6.403722, 5.875798, 8.687611, …\n$ DIST_HAWKER_M_TV    &lt;dbl&gt; 2.388050, 2.087323, 2.388050, 4.898568, 2.087323, …\n$ DIST_SUPERMKT_M_TV  &lt;dbl&gt; -0.5445629, 2.9824964, -0.5445629, 1.8548612, 2.98…\n$ DIST_MALL_M_TV      &lt;dbl&gt; -0.9841405, -4.4885515, -0.9841405, -1.1152694, -4…\n$ DIST_ELITEPRI_M_TV  &lt;dbl&gt; -5.7032726, -5.0950289, -5.7032726, -0.3845868, -5…\n$ CNT_KINDER_350_TV   &lt;dbl&gt; 1.0348916, 1.7388787, 1.0348916, 0.9139411, 1.7388…\n$ CNT_CHILD_350_TV    &lt;dbl&gt; 1.4256658, 0.6622315, 1.4256658, 1.5091903, 0.6622…\n$ CNT_BUS_350_TV      &lt;dbl&gt; -0.05059009, 3.10644343, -0.05059009, 1.79666600, …\n$ CNT_PRIMARY_1000_TV &lt;dbl&gt; -4.15929023, -0.54105016, -4.15929023, -0.47398114…\n$ Local_R2            &lt;dbl&gt; 0.9508844, 0.9490105, 0.9508844, 0.9561350, 0.9490…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3…\n\n\n\nsummary(gwr.adaptive$SDF$yhat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 396409  564653  628369  672149  723484 1485595 \n\n\n\n\n10.5 Visualising local R2\n\ntmap_mode(\"plot\")\ntm_shape(mpsz)+\n  tm_polygons(fill_alpha = 0.1) +\ntm_shape(analysis.sf.adaptive) +\n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n10.6 Visualising coefficient estimates\n\n# tmap_mode(\"plot\")\n# AREA_SQM_SE &lt;- tm_shape(mpsz)+\n#   tm_polygons(alpha = 0.1) +\n# tm_shape(analysis.sf.adaptive) +  \n#   tm_dots(col = \"AREA_SQM_SE\",\n#           border.col = \"gray60\",\n#           border.lwd = 1) +\n#   tm_view(set.zoom.limits = c(11,14))\n# \n# AREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n#   tm_polygons(alpha = 0.1) +\n# tm_shape(analysis.sf.adaptive) +  \n#   tm_dots(col = \"AREA_SQM_TV\",\n#           border.col = \"gray60\",\n#           border.lwd = 1) +\n#   tm_view(set.zoom.limits = c(11,14))\n# \n# tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n#              asp=1, ncol=2,\n#              sync = TRUE)\n# \n# tmap_mode(\"plot\")"
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-geographically-weighted-model-gwmodel",
    "href": "Take-home_Ex03/take-home_ex03.html#building-hedonic-pricing-models-using-geographically-weighted-model-gwmodel",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "10 Building Hedonic Pricing Models using Geographically-Weighted Model (GWmodel)",
    "text": "10 Building Hedonic Pricing Models using Geographically-Weighted Model (GWmodel)\nHedonic effects rarely act uniformly across a city. Prices respond to amenities, disamenities, and building attributes in ways that vary with location. This section introduces a spatially varying coefficient approach that relaxes the constant effect assumption of global OLS. We calibrate geographically weighted regressions that estimate location specific parameters while keeping the same dependent and predictor sets established earlier. Two bandwidth strategies are contrasted to control the spatial kernel: a single optimal distance for all locations and an adaptive neighbor count that expands or contracts with point density. We then convert model outputs to spatial objects, map local fit and coefficients, and prepare the ground for interpreting spatial heterogeneity with care.\n\n10.1 Building fixed bandwidth Geographically-Weighted Regression (GWR) model\nBegin with a practical baseline. A fixed bandwidth holds the kernel distance constant across the study area, so each local regression uses neighbours within the same radius. The method is straightforward to tune with cross validation and yields parameters that can be compared directly across places. We will compute the optimal fixed bandwidth, run the model with the specified predictors, and review the global back check that GWmodel reports for reference before moving on to spatial diagnostic\n\n10.1.1 Computing fixed bandwith\nWhat radius best balances bias and variance for our data support? The cross validation search evaluates candidate distances and returns the one minimizing prediction error. This step sets the spatial scale for local regressions and anchors all subsequent estimates.\n\nbw.fixed &lt;- bw.gwr(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf, \n  approach=\"CV\", \n  kernel=\"gaussian\", \n  adaptive=FALSE, \n  longlat=FALSE)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nFixed bandwidth: 22980.96 CV score: 52392049893594 \nFixed bandwidth: 14205.85 CV score: 50356574610703 \nFixed bandwidth: 8782.54 CV score: 45167073241578 \nFixed bandwidth: 5430.749 CV score: 36051674118746 \nFixed bandwidth: 3359.228 CV score: 26770547784048 \nFixed bandwidth: 2078.957 CV score: 19608338479414 \nFixed bandwidth: 1287.706 CV score: 15831700528257 \nFixed bandwidth: 798.6866 CV score: 13350423800665 \nFixed bandwidth: 496.4557 CV score: 12094482301403 \nFixed bandwidth: 309.6667 CV score: 36920475508267 \nFixed bandwidth: 611.8976 CV score: 12497490757783 \nFixed bandwidth: 425.1087 CV score: 12666604861680 \nFixed bandwidth: 540.5506 CV score: 12190731274360 \nFixed bandwidth: 469.2035 CV score: 12132063412439 \nFixed bandwidth: 513.2984 CV score: 12115230799514 \nFixed bandwidth: 486.0463 CV score: 12095955988873 \nFixed bandwidth: 502.889 CV score: 12099452420345 \nFixed bandwidth: 492.4797 CV score: 12093537446156 \nFixed bandwidth: 490.0223 CV score: 12093853356218 \nFixed bandwidth: 493.9984 CV score: 12093692843150 \nFixed bandwidth: 491.541 CV score: 12093573595050 \nFixed bandwidth: 493.0597 CV score: 12093565977647 \nFixed bandwidth: 492.1211 CV score: 12093539125534 \nFixed bandwidth: 492.7012 CV score: 12093543850438 \nFixed bandwidth: 492.3427 CV score: 12093536323213 \nFixed bandwidth: 492.2581 CV score: 12093536593114 \nFixed bandwidth: 492.395 CV score: 12093536590777 \nFixed bandwidth: 492.3104 CV score: 12093536358694 \nFixed bandwidth: 492.3627 CV score: 12093536370392 \nFixed bandwidth: 492.3304 CV score: 12093536291654 \nFixed bandwidth: 492.3227 CV score: 12093536318766 \nFixed bandwidth: 492.3351 CV score: 12093536265622 \nFixed bandwidth: 492.338 CV score: 12093536248669 \nFixed bandwidth: 492.3398 CV score: 12093536289563 \nFixed bandwidth: 492.3369 CV score: 12093536336119 \nFixed bandwidth: 492.3387 CV score: 12093536350291 \nFixed bandwidth: 492.3376 CV score: 12093536360103 \nFixed bandwidth: 492.3383 CV score: 12093536315755 \nFixed bandwidth: 492.3378 CV score: 12093536304390 \nFixed bandwidth: 492.3381 CV score: 12093536258440 \nFixed bandwidth: 492.3379 CV score: 12093536245718 \nFixed bandwidth: 492.3379 CV score: 12093536260521 \nFixed bandwidth: 492.338 CV score: 12093536359900 \n\n\n\n\n10.1.2 GWModel method - fixed bandwith\nWith the fixed bandwidth in hand, the model is fitted using the full predictor set. GWmodel reports both local fits and an embedded global OLS summary for orientation. The output will later be transformed for mapping and coefficient interpretation.\n\ngwr.fixed &lt;- gwr.basic(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf, \n  bw=bw.fixed, \n  kernel = 'gaussian', \n  longlat = FALSE)\n\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-11-16 23:07:34.950438 \n   Call:\n   gwr.basic(formula = resale_price ~ FLOOR_LEVEL + AGE_2025 + DIST_CBD_M + \n    DIST_MRT_M + DIST_ELDERCARE_M + DIST_HAWKER_M + DIST_SUPERMKT_M + \n    DIST_MALL_M + DIST_ELITEPRI_M + CNT_KINDER_350 + CNT_CHILD_350 + \n    CNT_BUS_350 + CNT_PRIMARY_1000, data = analysis_sf, bw = bw.fixed, \n    kernel = \"gaussian\", longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  FLOOR_LEVEL AGE_2025 DIST_CBD_M DIST_MRT_M DIST_ELDERCARE_M DIST_HAWKER_M DIST_SUPERMKT_M DIST_MALL_M DIST_ELITEPRI_M CNT_KINDER_350 CNT_CHILD_350 CNT_BUS_350 CNT_PRIMARY_1000\n   Number of data points: 8679\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-315906  -51235    -549   48570  496052 \n\n   Coefficients:\n                        Estimate   Std. Error t value             Pr(&gt;|t|)    \n   (Intercept)      1081180.5627    5873.4817 184.078 &lt; 0.0000000000000002 ***\n   FLOOR_LEVEL         5853.8558     146.3006  40.013 &lt; 0.0000000000000002 ***\n   AGE_2025           -5390.4551      65.3591 -82.474 &lt; 0.0000000000000002 ***\n   DIST_CBD_M           -19.8313       0.3270 -60.650 &lt; 0.0000000000000002 ***\n   DIST_MRT_M           -56.7805       2.6334 -21.561 &lt; 0.0000000000000002 ***\n   DIST_ELDERCARE_M     -10.4704       1.5022  -6.970      0.0000000000034 ***\n   DIST_HAWKER_M        -25.3443       1.9964 -12.695 &lt; 0.0000000000000002 ***\n   DIST_SUPERMKT_M       16.2692       4.9483   3.288              0.00101 ** \n   DIST_MALL_M          -27.5099       2.7120 -10.144 &lt; 0.0000000000000002 ***\n   DIST_ELITEPRI_M        1.8638       0.4209   4.428      0.0000096076479 ***\n   CNT_KINDER_350     10196.1602     930.1569  10.962 &lt; 0.0000000000000002 ***\n   CNT_CHILD_350      -1953.0874     465.6300  -4.195      0.0000276192396 ***\n   CNT_BUS_350         1461.6051     320.5408   4.560      0.0000051902793 ***\n   CNT_PRIMARY_1000  -12074.6258     656.3931 -18.395 &lt; 0.0000000000000002 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 78510 on 8665 degrees of freedom\n   Multiple R-squared: 0.7615\n   Adjusted R-squared: 0.7611 \n   F-statistic:  2128 on 13 and 8665 DF,  p-value: &lt; 0.00000000000000022 \n   ***Extra Diagnostic information\n   Residual sum of squares: 53403519683267\n   Sigma(hat): 78451.3\n   AIC:  220286.6\n   AICc:  220286.7\n   BIC:  211849.7\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 492.3379 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                             Min.       1st Qu.        Median       3rd Qu.\n   Intercept        -9691812.9492   487393.6737  1095595.0363  1799456.7940\n   FLOOR_LEVEL          1810.4892     3922.8815     4580.7283     5342.3275\n   AGE_2025           -60602.0360    -8072.2124    -5376.5415    -4008.2850\n   DIST_CBD_M          -1423.0500      -93.7795      -19.7608       35.3103\n   DIST_MRT_M          -1177.5377     -120.3100      -56.2344      -18.8214\n   DIST_ELDERCARE_M     -984.8730      -35.7176        7.6035       54.5526\n   DIST_HAWKER_M       -1548.8695      -61.5760      -21.3902       37.2136\n   DIST_SUPERMKT_M     -1078.1663      -41.2874        4.0802       46.1575\n   DIST_MALL_M         -2379.6795      -78.0348      -24.5326       20.3826\n   DIST_ELITEPRI_M      -961.8105      -77.4070       12.9733       83.5971\n   CNT_KINDER_350    -108699.5515    -7561.9978      -87.9998     4305.0773\n   CNT_CHILD_350      -66148.4086    -3055.4548     -129.6014     1911.1327\n   CNT_BUS_350        -39605.4561    -2012.2563      325.5323     2361.9436\n   CNT_PRIMARY_1000  -189594.7032    -5640.4046     -381.7289     4756.7456\n                           Max.\n   Intercept        15271185.21\n   FLOOR_LEVEL         23113.36\n   AGE_2025             5101.08\n   DIST_CBD_M           1379.90\n   DIST_MRT_M           2059.99\n   DIST_ELDERCARE_M      929.88\n   DIST_HAWKER_M         848.74\n   DIST_SUPERMKT_M      1768.27\n   DIST_MALL_M           913.75\n   DIST_ELITEPRI_M      1306.41\n   CNT_KINDER_350     193712.54\n   CNT_CHILD_350       29021.86\n   CNT_BUS_350         23605.43\n   CNT_PRIMARY_1000   118356.30\n   ************************Diagnostic information*************************\n   Number of data points: 8679 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1134.333 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 7544.667 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 206365.8 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 205230.1 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 203946 \n   Residual sum of squares: 8507189175489 \n   R-square value:  0.9619994 \n   Adjusted R-square value:  0.9562853 \n\n   ***********************************************************************\n   Program stops at: 2025-11-16 23:08:05.434838 \n\n\nThe embedded global regression in the GWmodel run is essentially the same multiple OLS model we reported earlier. All 13 predictors are highly significant with p values below 0.001. The key global effects are: each extra floor adds about 5,853 dollars to resale price, each additional year of AGE_2025 reduces price by about 5 389 dollars, every extra meter from the central business district reduces price by about 19.8 dollars and every extra meter from the nearest MRT station reduces price by about 56.8 dollars, holding other factors constant. Distances to eldercare, hawker centres and malls also have global negative coefficients, while distance to supermarkets and elite primary schools, counts of kindergartens and bus stops have positive global effects, and counts of childcare centres and nearby primary schools have negative global effects. Overall fit is strong with R squared about 0.761, residual sum of squares about 53.4 trillion and AICc about 220,289.\nThe GWR section shows what happens when these same coefficients are allowed to vary across space. The medians of the local coefficients are very close to the global estimates. For example the median local effect of FLOOR_LEVEL is about 4 580 dollars, AGE_2025 about minus 5,384 dollars, DIST_CBD_M about minus 19.1 dollars and DIST_MRT_M about minus 56.7 dollars. This confirms that the global OLS equation is a good summary of the central tendency across all neighbourhoods.\nHowever the ranges are very wide, which indicates pronounced spatial non stationarity. The floor premium ranges from about 1,756 to about 23,127 dollars, so higher floors are only weakly valued in some locations but command very large premiums in others. AGE_2025 has local effects from about minus 61 861 dollars, where newness is extremely rewarded, to about plus 5 130 dollars, where older developments actually carry a premium once other attributes are controlled. Several accessibility variables switch sign. Distance to the central business district is strongly negative in many areas but becomes positive in some locations, indicating suburban submarkets where greater separation from the centre is associated with higher prices. Distances to eldercare facilities, hawker centres, supermarkets, malls and elite primary schools all have negative values in some neighbourhoods and positive values in others, meaning that the desirability of being close to each amenity is highly context dependent rather than uniform across Singapore. The count variables show especially large spreads. For instance the coefficient for CNT_KINDER_350 runs from roughly minus 112,190 to about plus 193,876 dollars, so additional kindergartens nearby can either depress or raise prices depending on local conditions. Similar patterns appear for childcare centres, bus stops and primary schools.\nModel diagnostics confirm that allowing coefficients to vary by location delivers a major improvement in fit even after penalising the increased complexity. The effective number of parameters is about 1 136, and the residual sum of squares falls from about 53.4 trillion under the global model to about 8,498,024,692,874 under GWR. R squared increases from about 0.761 to about 0.962 and adjusted R squared to about 0.956. AICc improves from roughly 220 289 to roughly 206,359, and BIC also decreases substantially. These large reductions in information criteria show that the gain in explanatory power more than justifies the extra flexibility of the local model.\nThe main implication is that condominium price determinants in Singapore are strongly location specific. The global hedonic model provides a useful baseline and average effects, but it cannot capture local regimes where, for example, proximity to MRT stations or malls is valued very differently, or where older estates remain attractive despite shorter leases. For policy, planning and valuation, this means that city wide coefficients should be treated with caution. Geographically weighted regression offers a more realistic representation by revealing where each attribute is most influential, and the coefficient surfaces and maps derived from this output can guide neighbourhood tailored interventions, pricing strategies and investment decisions.\n\n\n\n10.2 Building Adaptive Bandwidth GWR Model\nShift perspective to account for uneven point density. An adaptive bandwidth keeps a constant number of nearest neighbours rather than a constant distance. Dense urban cores use smaller search radii while sparse edges expand to secure comparable data support. This typically stabilizes estimates and improves comparability of local regressions when observations cluster. We will compute the optimal neighbour count through cross validation, fit the adaptive model, and stage its results for spatial analysis alongside the fixed bandwidth specification.\n\n10.2.1 Computing the adaptive bandwidth\nThe search process now optimizes the number of neighbors. For each trial size, cross validation measures out of sample error. The winning size defines a location specific distance for each point that encloses the chosen neighbor count.\n\nbw.adaptive &lt;- bw.gwr(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf,\n  approach=\"CV\", \n  kernel=\"gaussian\", \n  adaptive=TRUE, \n  longlat=FALSE)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 5371 CV score: 49438681234211 \nAdaptive bandwidth: 3327 CV score: 45804301985023 \nAdaptive bandwidth: 2063 CV score: 40613968456352 \nAdaptive bandwidth: 1282 CV score: 33631587116541 \nAdaptive bandwidth: 799 CV score: 27110516923525 \nAdaptive bandwidth: 501 CV score: 22614461875755 \nAdaptive bandwidth: 316 CV score: 19354462639156 \nAdaptive bandwidth: 202 CV score: 16714806915953 \nAdaptive bandwidth: 131 CV score: 14839340890505 \nAdaptive bandwidth: 88 CV score: 13502993143099 \nAdaptive bandwidth: 60 CV score: 12506657744850 \nAdaptive bandwidth: 44 CV score: 11783391330857 \nAdaptive bandwidth: 33 CV score: Inf \nAdaptive bandwidth: 50 CV score: 11989958686380 \nAdaptive bandwidth: 39 CV score: Inf \nAdaptive bandwidth: 45 CV score: 11817563590297 \nAdaptive bandwidth: 41 CV score: Inf \nAdaptive bandwidth: 43 CV score: 11757456734254 \nAdaptive bandwidth: 45 CV score: 11817563590297 \nAdaptive bandwidth: 44 CV score: 11783391330857 \nAdaptive bandwidth: 45 CV score: 11817563590297 \nAdaptive bandwidth: 44 CV score: 11783391330857 \nAdaptive bandwidth: 44 CV score: 11783391330857 \nAdaptive bandwidth: 43 CV score: 11757456734254 \n\n\nThe following code chunk displays the adaptive bandwidth.\n\n# display adaptive bw\nbw.adaptive\n\n[1] 43\n\n\n\n\n10.2.2 Constructing the adaptive bandwidth gwr model\nUsing the selected neighbour count, the adaptive GWR is calibrated with the same predictors. Outputs include local coefficients, standard errors, fitted values, and diagnostics, which we will convert to spatial features for visualization.\n\ngwr.adaptive &lt;- gwr.basic(\n  formula = resale_price ~\n    FLOOR_LEVEL + AGE_2025 +\n    DIST_CBD_M + DIST_MRT_M +\n    DIST_ELDERCARE_M + DIST_HAWKER_M +\n    DIST_SUPERMKT_M + DIST_MALL_M + DIST_ELITEPRI_M +\n    CNT_KINDER_350 + CNT_CHILD_350 + CNT_BUS_350 + CNT_PRIMARY_1000,\n  data = analysis_sf,\n  bw=bw.adaptive, \n  kernel = 'gaussian', \n  adaptive=TRUE, \n  longlat = FALSE)\n\nThe code below can be used to display the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2025-11-16 23:14:39.796087 \n   Call:\n   gwr.basic(formula = resale_price ~ FLOOR_LEVEL + AGE_2025 + DIST_CBD_M + \n    DIST_MRT_M + DIST_ELDERCARE_M + DIST_HAWKER_M + DIST_SUPERMKT_M + \n    DIST_MALL_M + DIST_ELITEPRI_M + CNT_KINDER_350 + CNT_CHILD_350 + \n    CNT_BUS_350 + CNT_PRIMARY_1000, data = analysis_sf, bw = bw.adaptive, \n    kernel = \"gaussian\", adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  FLOOR_LEVEL AGE_2025 DIST_CBD_M DIST_MRT_M DIST_ELDERCARE_M DIST_HAWKER_M DIST_SUPERMKT_M DIST_MALL_M DIST_ELITEPRI_M CNT_KINDER_350 CNT_CHILD_350 CNT_BUS_350 CNT_PRIMARY_1000\n   Number of data points: 8679\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-315906  -51235    -549   48570  496052 \n\n   Coefficients:\n                        Estimate   Std. Error t value             Pr(&gt;|t|)    \n   (Intercept)      1081180.5627    5873.4817 184.078 &lt; 0.0000000000000002 ***\n   FLOOR_LEVEL         5853.8558     146.3006  40.013 &lt; 0.0000000000000002 ***\n   AGE_2025           -5390.4551      65.3591 -82.474 &lt; 0.0000000000000002 ***\n   DIST_CBD_M           -19.8313       0.3270 -60.650 &lt; 0.0000000000000002 ***\n   DIST_MRT_M           -56.7805       2.6334 -21.561 &lt; 0.0000000000000002 ***\n   DIST_ELDERCARE_M     -10.4704       1.5022  -6.970      0.0000000000034 ***\n   DIST_HAWKER_M        -25.3443       1.9964 -12.695 &lt; 0.0000000000000002 ***\n   DIST_SUPERMKT_M       16.2692       4.9483   3.288              0.00101 ** \n   DIST_MALL_M          -27.5099       2.7120 -10.144 &lt; 0.0000000000000002 ***\n   DIST_ELITEPRI_M        1.8638       0.4209   4.428      0.0000096076479 ***\n   CNT_KINDER_350     10196.1602     930.1569  10.962 &lt; 0.0000000000000002 ***\n   CNT_CHILD_350      -1953.0874     465.6300  -4.195      0.0000276192396 ***\n   CNT_BUS_350         1461.6051     320.5408   4.560      0.0000051902793 ***\n   CNT_PRIMARY_1000  -12074.6258     656.3931 -18.395 &lt; 0.0000000000000002 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 78510 on 8665 degrees of freedom\n   Multiple R-squared: 0.7615\n   Adjusted R-squared: 0.7611 \n   F-statistic:  2128 on 13 and 8665 DF,  p-value: &lt; 0.00000000000000022 \n   ***Extra Diagnostic information\n   Residual sum of squares: 53403519683267\n   Sigma(hat): 78451.3\n   AIC:  220286.6\n   AICc:  220286.7\n   BIC:  211849.7\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 43 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                              Min.        1st Qu.         Median        3rd Qu.\n   Intercept        -75757417.2874    409773.4005   1181220.6985   2676602.7520\n   FLOOR_LEVEL           1666.4109      3928.1718      4633.9967      5411.3176\n   AGE_2025           -100201.2717     -8448.0373     -5013.9184     -3773.3492\n   DIST_CBD_M          -12633.3460      -193.0836       -23.4580        41.8454\n   DIST_MRT_M          -17514.2292      -118.8915       -56.0866        -3.0742\n   DIST_ELDERCARE_M    -19500.4163       -27.1476        21.4099        79.6569\n   DIST_HAWKER_M       -11750.0244       -73.7205       -17.8461        47.4362\n   DIST_SUPERMKT_M      -9236.3100       -54.8218         3.9836        57.4492\n   DIST_MALL_M         -20167.5661       -84.0771       -28.8356        22.5139\n   DIST_ELITEPRI_M     -17313.8718       -75.7153         9.2729       150.3986\n   CNT_KINDER_350      -91932.8000     -8325.5453      -196.1971      4978.7145\n   CNT_CHILD_350       -83814.3208     -2072.0983       218.7891      2854.2842\n   CNT_BUS_350         -21251.9013     -2101.8634       303.9757      2729.6537\n   CNT_PRIMARY_1000  -2089387.0446     -6948.7400      -485.5829      5158.9479\n                            Max.\n   Intercept        117885924.45\n   FLOOR_LEVEL          10871.15\n   AGE_2025             20310.39\n   DIST_CBD_M           12954.58\n   DIST_MRT_M           13785.35\n   DIST_ELDERCARE_M      5473.92\n   DIST_HAWKER_M         8485.23\n   DIST_SUPERMKT_M        924.59\n   DIST_MALL_M          14056.69\n   DIST_ELITEPRI_M      12628.54\n   CNT_KINDER_350     2401971.91\n   CNT_CHILD_350       701382.56\n   CNT_BUS_350         359569.63\n   CNT_PRIMARY_1000    249176.42\n   ************************Diagnostic information*************************\n   Number of data points: 8679 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1261.738 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 7417.262 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 206973.3 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 205695 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 205162.4 \n   Residual sum of squares: 8879474156237 \n   R-square value:  0.9603364 \n   Adjusted R-square value:  0.9535884 \n\n   ***********************************************************************\n   Program stops at: 2025-11-16 23:15:14.626614 \n\n\nThe adaptive geographically weighted regression refines the global hedonic model by letting each location borrow information from its 43 nearest neighbours, so dense estates use tight windows while sparse areas use wider ones.\nGlobal OLS part: The embedded global regression is almost identical to the earlier OLS model: all predictors remain strongly significant, R squared is about 0.761 and the residual standard error is about 78 500. This confirms that the specification is stable and provides a good city wide average.\nImprovement in overall fit: Under the adaptive geographically weighted model, residual sum of squares falls from about 5.34 × 10¹³ to about 8.88 × 10¹². R squared jumps from about 0.761 to about 0.960 with adjusted R squared about 0.954. Information criteria also drop: AICc falls from roughly 220,289 for OLS to about 206,971, and BIC reduces to about 205,160. These changes mean that allowing coefficients to vary across space produces a far better description of resale prices, even after penalising for the larger effective number of parameters.\nBehaviour of local coefficients: The medians of the local coefficients are close to the global estimates, so the original hedonic equation can be read as the central tendency.\n\nFLOOR_LEVEL: median local premium is about 4,629 per floor, close to the global 5,853, but local effects range from about 1,656 up to about 10,871. Height is valued almost everywhere, yet the size of the premium varies strongly between estates.\nAGE_2025: the median is about −5014, again close to the global −5389, but local values range from about −100,201 to about plus 20,310. In some neighbourhoods newness is rewarded extremely strongly; in others older developments can even carry a positive premium after other attributes are controlled.\nCentrality and transport: DIST_CBD_M has median about −23 and DIST_MRT_M median about −57, consistent with the idea that greater distance from the central business district and MRT generally lowers price. Yet both variables swing from strongly negative to positive values, indicating that in some suburban markets greater separation from these features is associated with higher prices, possibly reflecting preference for quieter or more exclusive environments.\nLocal services: distances to eldercare, hawker centres, supermarkets, malls and elite primary schools show similar sign reversals. For example, the supermarket distance coefficient ranges from strongly negative through a small positive median to clearly positive values. This means proximity to these amenities raises prices in some areas but lowers them in others, depending on local congestion, noise or competition effects.\nCount variables: CNT_KINDER_350, CNT_CHILD_350, CNT_BUS_350 and CNT_PRIMARY_1000 exhibit very wide ranges, from large negative to very large positive values. The same number of kindergartens or primary schools can therefore have opposite price effects in different neighbourhoods, again pointing to strong context dependence.\n\nImplications: The adaptive geographically weighted regression confirms that condominium price determinants in Singapore are highly spatially non stationary. The global hedonic model remains useful as a summary of average relationships, but it hides substantial local variation in both the strength and even the direction of key effects. For valuation, planners and investors, this means that:\n\nPremiums for height, age, centrality and amenities are location specific and cannot be applied uniformly across the island.\nPolicies or planning decisions that rely only on global elasticities may misjudge impacts in particular neighbourhoods.\nMapping the local coefficients from this adaptive model will help identify zones where, for example, access to MRT, malls or schools is most capitalised into prices, and where older estates or quieter locations still command strong premiums.\n\nOverall, the adaptive geographically weighted model substantially improves explanatory power and reveals rich spatial heterogeneity, providing a more realistic foundation for localised housing market analysis than the single global regression.\n\n\n\n10.3 Visualising GWR Output\nMaps translate tables of local coefficients and fit statistics into patterns that can be interpreted geographically. Here we take the structured output from GWmodel, convert it to simple features in the project CRS, and explore basic summaries to ensure data integrity. Visual checks include range, central tendency, and the presence of missing values. These steps are essential for trustworthy cartography and for avoiding misread patterns due to unit or projection mistakes. The prepared layers serve as inputs to thematic maps of local \\(R^2\\) and coefficients, enabling side by side comparison across bandwidth strategies.\n\n10.3.1 Converting SDF into sf data.frame\nGWmodel returns a Spatial Data Frame structure. For consistent handling with the rest of the workflow, we convert the model output into an sf object and enforce EPSG 3414. This conversion preserves attributes such as coefficients, t values, and fitted values while attaching valid geometry. A brief glimpse confirms column names and types. The resulting layer integrates seamlessly with tmap and dplyr, allowing clean joins, selections, and map composition without coercion errors.\n\nanalysis.sf.adaptive &lt;-\n  st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\nglimpse(analysis.sf.adaptive)\n\nRows: 8,679\nColumns: 49\n$ Intercept           &lt;dbl&gt; 1374903, 1386785, 1374903, 1217950, 1386785, 12179…\n$ FLOOR_LEVEL         &lt;dbl&gt; 5277.432, 6002.142, 5277.432, 7308.207, 6002.142, …\n$ AGE_2025            &lt;dbl&gt; -9009.658, -8029.718, -9009.658, -8184.863, -8029.…\n$ DIST_CBD_M          &lt;dbl&gt; -32.66705, -55.03257, -32.66705, -45.96045, -55.03…\n$ DIST_MRT_M          &lt;dbl&gt; -49.11932, 66.28118, -49.11932, 56.05148, 66.28118…\n$ DIST_ELDERCARE_M    &lt;dbl&gt; 78.22071, 154.00055, 78.22071, 80.72166, 154.00055…\n$ DIST_HAWKER_M       &lt;dbl&gt; 56.64781, 46.18170, 56.64781, 107.03918, 46.18170,…\n$ DIST_SUPERMKT_M     &lt;dbl&gt; -9.665631, 121.346718, -9.665631, 45.835332, 121.3…\n$ DIST_MALL_M         &lt;dbl&gt; -23.02675, -93.58701, -23.02675, -17.54431, -93.58…\n$ DIST_ELITEPRI_M     &lt;dbl&gt; -48.667048, -47.066341, -48.667048, -3.074145, -47…\n$ CNT_KINDER_350      &lt;dbl&gt; 3658.406, 8311.043, 3658.406, 3669.709, 8311.043, …\n$ CNT_CHILD_350       &lt;dbl&gt; 4247.138, 3204.654, 4247.138, 3640.797, 3204.654, …\n$ CNT_BUS_350         &lt;dbl&gt; 147.1248, 6139.5015, 147.1248, 2920.5888, 6139.501…\n$ CNT_PRIMARY_1000    &lt;dbl&gt; -19204.168, -7754.240, -19204.168, -3407.202, -775…\n$ y                   &lt;dbl&gt; 582000, 865000, 530000, 560000, 1038000, 542000, 6…\n$ yhat                &lt;dbl&gt; 629211.5, 919120.6, 613379.2, 565113.5, 1027159.1,…\n$ residual            &lt;dbl&gt; -47211.5040, -54120.5658, -83379.2089, -5113.4785,…\n$ CV_Score            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Stud_residual       &lt;dbl&gt; -1.412162419, -1.622737639, -2.477421258, -0.15759…\n$ Intercept_SE        &lt;dbl&gt; 73791.83, 128930.54, 73791.83, 73351.44, 128930.54…\n$ FLOOR_LEVEL_SE      &lt;dbl&gt; 420.1065, 603.1228, 420.1065, 525.1575, 603.1228, …\n$ AGE_2025_SE         &lt;dbl&gt; 220.3508, 313.3994, 220.3508, 242.5908, 313.3994, …\n$ DIST_CBD_M_SE       &lt;dbl&gt; 6.327724, 13.184596, 6.327724, 8.863603, 13.184596…\n$ DIST_MRT_M_SE       &lt;dbl&gt; 14.84159, 18.40148, 14.84159, 14.30122, 18.40148, …\n$ DIST_ELDERCARE_M_SE &lt;dbl&gt; 12.26876, 17.83651, 12.26876, 13.76716, 17.83651, …\n$ DIST_HAWKER_M_SE    &lt;dbl&gt; 24.31817, 28.60073, 24.31817, 21.99188, 28.60073, …\n$ DIST_SUPERMKT_M_SE  &lt;dbl&gt; 24.08814, 38.22529, 24.08814, 24.69431, 38.22529, …\n$ DIST_MALL_M_SE      &lt;dbl&gt; 19.22362, 19.67735, 19.22362, 13.94482, 19.67735, …\n$ DIST_ELITEPRI_M_SE  &lt;dbl&gt; 8.679052, 9.986913, 8.679052, 7.901863, 9.986913, …\n$ CNT_KINDER_350_SE   &lt;dbl&gt; 3515.168, 4620.614, 3515.168, 3927.209, 4620.614, …\n$ CNT_CHILD_350_SE    &lt;dbl&gt; 2647.410, 3544.774, 2647.410, 2315.222, 3544.774, …\n$ CNT_BUS_350_SE      &lt;dbl&gt; 1449.635, 1887.351, 1449.635, 1564.252, 1887.351, …\n$ CNT_PRIMARY_1000_SE &lt;dbl&gt; 4257.002, 5321.439, 4257.002, 4234.401, 5321.439, …\n$ Intercept_TV        &lt;dbl&gt; 18.63219, 10.75606, 18.63219, 16.60432, 10.75606, …\n$ FLOOR_LEVEL_TV      &lt;dbl&gt; 12.562129, 9.951775, 12.562129, 13.916219, 9.95177…\n$ AGE_2025_TV         &lt;dbl&gt; -40.88780, -25.62135, -40.88780, -33.73938, -25.62…\n$ DIST_CBD_M_TV       &lt;dbl&gt; -5.162527, -4.174005, -5.162527, -5.185301, -4.174…\n$ DIST_MRT_M_TV       &lt;dbl&gt; -3.3095732, 3.6019482, -3.3095732, 3.9193489, 3.60…\n$ DIST_ELDERCARE_M_TV &lt;dbl&gt; 6.375602, 8.634004, 6.375602, 5.863350, 8.634004, …\n$ DIST_HAWKER_M_TV    &lt;dbl&gt; 2.329444, 1.614704, 2.329444, 4.867214, 1.614704, …\n$ DIST_SUPERMKT_M_TV  &lt;dbl&gt; -0.4012610, 3.1745137, -0.4012610, 1.8561092, 3.17…\n$ DIST_MALL_M_TV      &lt;dbl&gt; -1.1978364, -4.7560776, -1.1978364, -1.2581241, -4…\n$ DIST_ELITEPRI_M_TV  &lt;dbl&gt; -5.6074155, -4.7128018, -5.6074155, -0.3890406, -4…\n$ CNT_KINDER_350_TV   &lt;dbl&gt; 1.0407485, 1.7986882, 1.0407485, 0.9344317, 1.7986…\n$ CNT_CHILD_350_TV    &lt;dbl&gt; 1.6042613, 0.9040504, 1.6042613, 1.5725473, 0.9040…\n$ CNT_BUS_350_TV      &lt;dbl&gt; 0.1014909, 3.2529734, 0.1014909, 1.8670832, 3.2529…\n$ CNT_PRIMARY_1000_TV &lt;dbl&gt; -4.5111953, -1.4571697, -4.5111953, -0.8046480, -1…\n$ Local_R2            &lt;dbl&gt; 0.9513180, 0.9492120, 0.9513180, 0.9562596, 0.9492…\n$ geometry            &lt;POINT [m]&gt; POINT (30036.29 38360.76), POINT (29283.45 3…\n\n\nThe adaptive GWR output now attaches local coefficients, standard errors, t values, fitted prices, and residuals to every condominium. This enables mapping spatial variation in effects, identifying neighbourhoods where specific predictors matter most, and diagnosing where the global hedonic model under or over estimates resale prices.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 396418  564536  628366  672137  723932 1485595 \n\n\nAdaptive GWR fitted prices range from about 396622 to 1485595. Median predicted price about 628332 and mean about 672147 indicate a slightly right skewed distribution with relatively few high value condominiums pulling the average above the middle of the market.\n\n\n10.3.2 Visualising local \\(R^2\\)\nHow well does the model fit in different neighborhoods? Local \\(R^2\\) indicates the share of variance explained by each local regression, revealing zones of strong and weak explanatory power. We render Local \\(R^2\\) on top of the planning subzones, using a clean legend inside the frame and appropriate zoom bounds. Interpreting this map helps diagnose scale misspecification, missing predictors, or unmodeled spatial processes. High values suggest that the chosen predictors capture local dynamics, while low values motivate refinement or alternative spatial structures.\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(fill_alpha = 0.1) +\ntm_shape(analysis.sf.adaptive) +\n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe Local \\(R^2\\) map shows that the adaptive GWR explains condo prices very well in most neighbourhoods. Darker blue clusters (Local \\(R^2\\) between 0.8 and 1.0) dominate the island, meaning the local regressions capture more than 80% of price variation in these areas. Here, the chosen structural and accessibility variables provide an excellent description of market behaviour and the global conclusions are strongly supported.\nLighter patches (Local \\(R^2\\) between 0.5 and 0.7) appear only in a few planning subzones. In these locations the model leaves more unexplained variation, suggesting that additional local factors such as project specific amenities, tenure mix, redevelopment expectations, or micro environmental qualities may be important. These areas are prime candidates for further refinement, either by enriching the covariate set or exploring alternative spatial scales. Overall, the map confirms that the adaptive GWR greatly improves explanatory power across Singapore while also revealing a small number of neighbourhoods where current predictors are less adequate and targeted model extensions are warranted.\n\n\n10.3.3 Visualising coefficient estimates\nCoefficient maps reveal how the marginal effect of each predictor varies across space. By plotting estimates together with their uncertainty metrics when needed, we distinguish robust spatial trends from noise. The goal is not decoration but decision support. Positive and negative zones guide policy interpretation, for example where proximity to transit associates with larger premiums or where park access effects attenuate. Side by side arrangements facilitate comparison across variables and between fixed and adaptive bandwidths, helping to identify consistent patterns and locations where model assumptions warrant further scrutiny.\n\ntmap_mode(\"plot\")\n\nDIST_MRT_M_SE &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\n  tm_shape(analysis.sf.adaptive) +\n  tm_dots(col = \"DIST_MRT_M_SE\",\n          border.col = \"gray60\", border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nDIST_MRT_M_TV &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\n  tm_shape(analysis.sf.adaptive) +\n  tm_dots(col = \"DIST_MRT_M_TV\",\n          border.col = \"gray60\", border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n# … build DIST_MRT_M_SE and DIST_MRT_M_TV exactly as you have …\ntmap_arrange(DIST_MRT_M_SE, DIST_MRT_M_TV, asp = 1, ncol = 2, sync = TRUE)\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe paired maps show both the precision and the strength of the local MRT effect. On the left, most standard errors for the MRT distance coefficient are small and spatially even, so local estimates are quite precise across the island. Larger standard errors (SE) only appear in a few peripheral clusters where data are sparse. On the right, local \\(t\\) values are strongly negative over wide areas, confirming that greater distance from MRT stations is associated with significantly lower resale prices in most neighbourhoods. Small or near zero \\(t\\) values in a few zones indicate weaker and possibly unstable MRT effects there. Together, these maps suggest that MRT accessibility is a robust and spatially pervasive price driver, but its influence is not uniform, so transport planning and valuation should account for local variation in how strongly buyers capitalise rail access.\n\n\n\n10.4 Case studies of localised HDB re-sale price dynamics: Tampines\nWhat makes Tampines an instructive microcosm for spatial heterogeneity in HDB resale prices is its mix of mature estates, transit connectivity, town centre amenities, and green assets, all within a clear planning area boundary. This case study leverages the adaptive GWR output prepared earlier to examine how explanatory power varies locally and where model fit strengthens or weakens. We isolate the planning area, retain only observations located within it, visualise Local\\(R^2\\) patterns to reveal intra town structure, and summarise distributional properties for transparent reporting. The objective is twofold. First, to translate city scale diagnostics into neighbourhood scale insights that aid interpretation. Second, to establish a reproducible template for other towns under identical data and methods.\n\n10.4.1 Selecting the Tampines planning area polygon\nThis section extracts the Tampines polygon from the Master Plan layer using its planning area field, preserving CRS and geometry validity established earlier. The result is a single, auditable spatial unit that constrains all joins and overlays. By fixing the study footprint at the outset, we prevent leakage from adjacent towns, maintain comparability with official statistics, and ensure that any localised signal we observe later is not an artefact of inconsistent geography but a genuine characteristic of Tampines.\n\n# --- 1) Select the Tampines planning area polygon ------------------------------\n# (Common MP14 fields are PLN_AREA_N (planning area) and SUBZONE_N (subzone).)\ntampines_pa &lt;- mpsz %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\n\n\n\n10.4.2 Retaining only HDB resale points inside Tampines\nStart with the data that matter most for inference. We intersect the adaptive GWR results with the Tampines polygon to keep only transactions whose coordinates fall within the planning area. This step aligns attributes such as Local R² with the town specific sample, removes off boundary outliers, and guarantees that subsequent summaries reflect the population of interest. The operation also preserves row integrity for reproducibility, since no recalculation of GWR occurs here. The outcome is a clean sf layer of Tampines observations ready for mapping and statistics, with geometry driven selection ensuring methodological coherence.\n\n# --- 2) Keep only condo points that fall inside Tampines -----------------------\n# Our GWR results were attached to `condo_resale.sf.adaptive` and include `Local_R2`.\ntampines_pts &lt;- analysis.sf.adaptive[tampines_pa, , op = st_within]\n\n\n\n10.4.3 Visualising Tampines boundary with points coloured by Local \\(R^2\\)\nSee the pattern before measuring it. This visualization overlays the Tampines boundary and plots each retained transaction coloured by Local \\(R^2\\) using quantile classes and an internal legend. The map exposes pockets where the model explains a larger share of variance, often near strong accessibility or amenity gradients, and areas where fit attenuates, suggesting missing predictors or scale mismatch. Using a consistent palette and zoom limits supports side by side comparison with other towns. The figure is not decorative. It is a diagnostic surface that guides targeted refinement and grounds narrative claims in observable spatial structure.\n\n# --- 3) Visualise: Tampines boundary + points coloured by Local R² -------------\ntmap_mode(\"plot\")\n\ntm_shape(tampines_pa) +\n  tm_polygons(fill_alpha = 0.1) +\ntm_shape(tampines_pts) +\n  tm_dots(\n    fill = \"Local_R2\",           # tmap v4: 'fill' controls symbol fill colour\n    col = \"grey30\",              # thin outline\n    size = 0.6,\n    fill.scale = tm_scale(\n      n = 7,\n      style = \"quantile\"         # consistent with the tutorial’s quantile breaks\n    ),\n    fill.legend = tm_legend(title = \"Local R²\")\n  ) +\n  tm_view(set_zoom_limits = c(11, 14))\n\n\n\n\n\n\n\n\n\n\n10.4.4 Summarising Local \\(R^2\\) for Tampines\nDefinition first. Local \\(R^2\\) is the proportion of variance explained by each local regression at an observation. This subsection reports n, min, quartiles, median, mean, and max for Local \\(R^2\\) within Tampines, providing a compact distributional audit. High upper quartile with wide spread signals heterogeneity in model adequacy across micro locations. Narrow spread around a moderate central tendency indicates more uniform performance. These statistics will be cited in Section 10 discussion to justify whether additional neighbourhood variables, alternative bandwidths, or complementary models are warranted for Tampines and, by extension, for comparable new towns.\n\n# --- 4) Summarise Local R² for Tampines ----------------------------------------\n\ntampines_r2_summary &lt;- tampines_pts %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    n      = dplyr::n(),\n    r2_min = min(Local_R2, na.rm = TRUE),\n    r2_q1  = quantile(Local_R2, 0.25, na.rm = TRUE),\n    r2_med = median(Local_R2, na.rm = TRUE),\n    r2_mean= mean(Local_R2, na.rm = TRUE),\n    r2_q3  = quantile(Local_R2, 0.75, na.rm = TRUE),\n    r2_max = max(Local_R2, na.rm = TRUE)\n  )\n\ntampines_r2_summary\n\n    n    r2_min     r2_q1    r2_med   r2_mean     r2_q3    r2_max\n1 712 0.6851969 0.8231481 0.8758055 0.8708912 0.9537619 0.9671227\n\n\nThe updated Local R² results for Tampines show that the adaptive GWR still fits very well but with more variation than in the smaller earlier sample. Across 712 condominiums, Local R² ranges from about 0.685 to 0.965 with a median around 0.876 and upper quartile near 0.951.\nOn the map, darker blue clusters in the west and south east indicate locations where the model explains more than 0.95 of local price variation. Here the hedonic predictors such as floor level, age and accessibility capture condominium prices extremely well. Lighter areas in the central belt correspond to Local R² around 0.75 to 0.85, meaning that prices there depend more on factors not included in the model, for example project specific facilities or micro environment qualities.\nOverall, the adaptive geographically weighted regression (adaptive GWR) model remains highly reliable for Tampines, but the central neighbourhoods merit further refinement or additional covariates to fully explain local price dynamics."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#discussion-of-research-questions",
    "href": "Take-home_Ex03/take-home_ex03.html#discussion-of-research-questions",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "11 Discussion of Research Questions",
    "text": "11 Discussion of Research Questions\n\n11.1 RQ1: Structural attributes and resale price\nThe global Multiple Linear Regression indicates that structural characteristics are strongly and systematically related to resale price. Floor level has a large positive coefficient of about 5,800 dollars per storey, with a highly significant p value. This confirms a clear premium for higher units, consistent with better views, privacy, and reduced traffic noise.\nAge in 2025 shows a similarly strong but negative effect of about 5,400 dollars per additional year. Newer blocks therefore command higher resale prices even after controlling for location and amenities. Remaining lease was dropped from the final specification because it is almost perfectly correlated with age, which created redundancy and made its coefficient unstable. Once age is retained, remaining lease contributes no additional explanatory power and its p value becomes very high.\nOverall, RQ1 is answered affirmatively. Structural factors, in particular vertical position and building age, are highly significant and economically meaningful determinants of resale price in the study window. The model suggests that buyers pay sizeable premiums for newer and higher units over otherwise similar flats.\n\n\n11.2 RQ2: Locational attributes and resale price\nThe same regression highlights several important locational drivers. Distance to the Central Business District carries one of the largest standardised effects. Each additional metre from the central area reduces predicted price by roughly 20 dollars, all else equal. Distance to the nearest MRT station has a sizeable negative coefficient of about 57 dollars per metre, confirming that rail accessibility is capitalised strongly into prices.\nOther facilities show more nuanced patterns. Greater distance from eldercare centres, hawker centres and shopping malls is associated with lower prices, indicating that proximity to these amenities generally enhances value. In contrast, distance to supermarkets exhibits a small but positive coefficient, suggesting that in some locations buyers are willing to pay slightly more to live further from busy retail clusters once other amenities are controlled. Distance to elite primary schools has a small positive coefficient, implying that most of the schooling effect is captured instead by the count variables.\nThe counts of nearby facilities also matter. Each additional kindergarten within 350 metres increases predicted price by roughly 10,000 dollars, while each additional primary school within 1,000 metres reduces price by more than 11,000 dollars. More bus stops within 350 metres are associated with higher prices, whereas a denser supply of childcare centres slightly lowers prices. Together, these results suggest that buyers value diverse early childhood options and good public transport, but may discount locations with very intense primary school or childcare activity, possibly due to congestion and noise.\nHence RQ2 is also answered positively. Multiple dimensions of accessibility and local service mix are significantly associated with resale price, and their marginal effects are both statistically strong and substantively large.\n\n\n11.3 RQ3: Spatial variation in associations\nGeographically weighted regression reveals that these relationships vary substantially across space. For each predictor, local coefficients span wide ranges and often change sign. For example, the floor level premium has a median local effect of about 4,600 dollars but ranges from about 1,700 to more than 10,800 dollars. In some peripheral estates buyers pay only modest premiums for height, whereas in selected cores the premium per storey is very large, perhaps reflecting views over parks or waterfronts.\nThe age effect shows even stronger non stationarity. Local coefficients are strongly negative in many subzones, reinforcing a preference for newer blocks, but turn positive in a few neighbourhoods where older estates may be more centrally located or perceived as more spacious. Similar spatial variability occurs for distances to MRT stations, malls and schools, with some areas showing strong negative effects and others small or even positive effects.\nThese patterns confirm that the global regression summarises only an average response. In reality, the hedonic trade offs between structure, accessibility and surrounding services differ markedly by neighbourhood. RQ3 is therefore answered: the associations identified in RQ1 and RQ2 clearly vary across space, and the geographically weighted model provides direct evidence of this spatial heterogeneity.\n\n\n11.4 RQ4: Model fit and residual behaviour\nComparison of diagnostics between the global MLR and the adaptive geographically weighted regression shows a substantial improvement in goodness of fit and residual properties. The global model achieves R squared of about 0.761 with adjusted R squared essentially identical, indicating that about 76 percent of price variance is explained by the common set of predictors. However, residual plots show curvature in the residual versus fitted relationship and increasing spread at higher fitted values, signalling some non linearity and mild heteroscedasticity.\nSpatial diagnostics reveal more serious departures. The residual map exhibits clear clusters of over prediction and under prediction, and the Monte Carlo Moran I statistic of about 0.33 with a p value effectively zero confirms strong positive spatial autocorrelation in the residuals. This violates the independence assumption and implies that some location specific effects remain unmodelled.\nThe adaptive geographically weighted regression reduces these problems markedly. Global R squared rises to about 0.96 and adjusted R squared to about 0.954, while residual sum of squares drops by more than 80 percent. Information criteria such as AICc and BIC are substantially lower, even after accounting for the increased effective number of parameters. Local R squared values are mostly between 0.8 and 1.0, with only small pockets around 0.6 to 0.7. Residual maps become visually less structured, and spatial clustering of large errors is greatly reduced.\nConsequently, RQ4 is answered affirmatively. The geographically weighted model materially improves overall fit and offers more realistic residual behaviour relative to the global MLR baseline, supporting its use as the main explanatory tool for spatial price variation.\n\n\n11.5 RQ5: Local effects and practical implications\nLocal R squared and coefficient maps, together with planning area summaries, highlight where effects are notably stronger or weaker and suggest implications for different stakeholders.\nIn Tampines, Local R squared ranges from about 0.69 to 0.97 with a median around 0.88 and an upper quartile above 0.95. The highest values cluster near Tampines Central where MRT, bus interchanges and regional malls are concentrated. Here the model explains almost all local price variation, and coefficients on accessibility variables are strong and stable. For buyers and valuers, this means price expectations in central Tampines can be forecast with high confidence from the structural and locational attributes in the model. For planners, it suggests that transport and town centre investments in this area are strongly capitalised into resale values.\nIn contrast, parts of the central belt of Tampines show lower Local R squared around 0.7 to 0.8. These neighbourhoods likely involve additional influences that are not captured by the current predictors, such as block specific facilities, noise exposure or detailed renewal plans. For these micro markets, the model still performs reasonably well but cannot fully substitute for case specific information.\nThe maps of MRT distance standard errors and \\(t\\) values further emphasise that rail access is a robust and spatially pervasive driver. Over large parts of the island, the local t statistic for distance to MRT is strongly negative, and the associated standard errors are small, indicating precise and significant price penalties for being further from stations. Only a few fringe zones show weaker or unstable MRT effects. This reinforces the view that improvements to rail access will generally enhance resale values, but the magnitude of the gain will differ by planning area."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#conclusion",
    "href": "Take-home_Ex03/take-home_ex03.html#conclusion",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nThis study develops a complete spatial hedonic workflow for public housing resale analysis, moving from data preparation and exploratory mapping through diagnostic checking, global calibration, and location specific modelling using geographically weighted regression. The framework is intentionally transparent and modular, so that analysts can substitute alternative variables, spatial weights, or bandwidth choices while retaining a clear sequence of steps and checks.\nBeyond its immediate application to 4 room resale transactions, the workflow provides a reusable template for agencies and valuers who require defensible, map ready evidence on how neighbourhood context relates to housing outcomes. Each stage produces outputs that can be communicated directly to non technical audiences, for example residual maps to flag areas where prevailing valuation rules are unreliable, or local goodness of fit summaries that highlight where further field investigation is warranted.\nSeveral limitations point to promising directions for future work. The design is cross sectional, so temporal dynamics and policy shocks are not yet incorporated. Important unobserved features such as interior quality, micro noise exposure, and detailed project characteristics remain outside the current variable set. Extending the framework to panel data, richer administrative sources, or combined use with machine learning methods that respect spatial structure would deepen understanding of market behaviour.\nOverall, the project contributes a rigorous, practice orientated methodology that can be adopted and adapted by planners, valuers, and researchers who wish to integrate spatial analytics and geographically weighted regression into routine housing market assessment."
  },
  {
    "objectID": "Take-home_Ex03/take-home_ex03.html#references",
    "href": "Take-home_Ex03/take-home_ex03.html#references",
    "title": "Take-home Ex03: Modelling HDB Resale Prices with Geographically Weighted Methods",
    "section": "13 References:",
    "text": "13 References:\n\nBrunsdon, C, Fotheringham, A S and Charlton, M 1996, Geographically weighted regression: a method for exploring spatial nonstationarity, Geographical Analysis, vol 28, no 4, pp 281 to 298.\nBrunsdon, C, Fotheringham, A S and Charlton, M 1999, Some notes on parametric significance tests for geographically weighted regression, Journal of Regional Science, vol 39, no 3, pp 497 to 524.\nCreative Campus 2024, Best primary schools in Singapore 2024, Creative Campus: Learning with Latitude, viewed 16 November 2025, https://www.creativecampus.com.sg/best-primary-schools-in-singapore-2024.\nEdgeProp Singapore 2024, Analysis: the increasing appeal of four room HDB flats, EdgeProp Singapore, 18 February, viewed 16 November 2025, https://www.edgeprop.sg/property-news/analysis-increasing-appeal-four-room-hdb-flats.\nGollini, I, Lu, B, Charlton, M, Brunsdon, C and Harris, P 2015, GWmodel: an R package for exploring spatial heterogeneity using geographically weighted models, Journal of Statistical Software, vol 63, no 17, pp 1 to 50.\nKam, T S 2022a, Calibrating hedonic pricing model for private highrise property with GWR method, in R for Geospatial Data Science and Analytics, Singapore Management University, viewed 16 November 2025, https://r4gdsa.netlify.app/chap13.html.\nKam, T S 2022b, Calibrating hedonic pricing model for private highrise property with GWR method, ISSS626 in class exercise 7, Geospatial Analytics and Applications, School of Computing and Information Systems, Singapore Management University, viewed 16 November 2025, https://isss626-ay2025-26aug.netlify.app/in-class_ex/in-class_ex07/in-class_ex07-gwr.\nKam, T S 2025a, In class exercise 8: data preparation for take home exercise 3, ISSS626 in class exercise 8, Geospatial Analytics and Applications, School of Computing and Information Systems, Singapore Management University, viewed 16 November 2025, https://isss626-ay2025-26aug.netlify.app/in-class_ex/in-class_ex08/in-class_ex08.\nKam, T S 2025b, Lesson 7: geographically weighted regression, ISSS626 Geospatial Analytics and Applications module notes, School of Computing and Information Systems, Singapore Management University, viewed 16 November 2025, https://isss626-ay2025-26aug.netlify.app/lesson/lesson07/lesson07-gwr.\nLu, B, Harris, P, Charlton, M and Brunsdon, C 2014, The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models, Geo spatial Information Science, vol 17, no 2, pp 85 to 101.\nMatthews, S A and Yang, T C 2012, Mapping the results of local statistics: using geographically weighted regression, Demographic Research, vol 26, pp 151 to 166.\nMennis, J 2006, Mapping the results of geographically weighted regression, The Cartographic Journal, vol 43, no 2, pp 171 to 179."
  }
]