---
title: "In-class Ex10"
author: "TAN Chin Khoon"
date: "15 November 2025"
date-modified: "last-modified"
number-sections: true
---

# Modelling Spatial Interaction: Huff and MCI Models

## Overview

Market area analysis defines and studies where customers originate, using demographics, spending, competition, and observation to guide location, marketing, and expansion decisions. The Huff model estimates store choice probabilities and market share from attractiveness and distance, helping businesses delineate trade areas and forecast revenue among competing outlets and potential profitability.

## Getting Started

Before we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.

```{r}
pacman::p_load(sf, tidyverse, MCI, tmap, ggstatsplot)
```

## Importing geospatial data

```{r}
stores <- st_read(
  dsn = "/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial",
  layer = "STORES_TC")

town <- st_read(
  dsn = "/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial",
  layer = "TOWN_TC")

village <- st_read(
  dsn = "/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/geospatial",
  layer = "VILLAGE_TC")
```

The output print above shown that the three data sets are in **TWD97 / TM2 zone 121 (i.e epsg: 3826)** projected coordinates systems.

::: callout-important
For any geospatial analysis, it is important to check and confirm that all the geospatial data are in the similar projected coodinates system.
:::

```{r}
tmap_mode("view")
tm_shape(town) +
  tm_polygons() +
tm_shape(village) +
  tm_polygons() + 
tm_shape(stores) +
  tm_dots()
tmap_mode("plot")
```

## Importing aspatial data

Next, `read_csv()` of **readr** package will be used to import the two aspatial data sets into R environment.

```{r}
pop2019 <- read_csv(
  "/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/pop2019.csv")

store_sales <- read_csv(
  "/Users/cktan/Desktop/SMU/01_Geospatial Analytics (ISSS626)/Hands-on_Ex/In-Class_Ex10/data/aspatial/Store_sales_data.csv")
```

```{r}
summary(store_sales)
```

## Data Preparation

### Aggregating store sales

Code chunk below is used to aggregate the store sales data at Store Code.

```{r}
store_sales_aggre <- store_sales %>%
  group_by(`Store Code`) %>%
  summarise(
    "Total_Bill" = sum(Bills, na.rm = TRUE),
    "Total_Sales" = sum(Amount, na.rm = TRUE))
```

### Relational join

In the code chunk below, `left_join()` of **dplyr** package is used to append data from store_sales data.frame onto stores sf data.frame. At the same time a new field called *Store_size* has been created.

```{r}
stores <- stores %>%
  left_join(store_sales_aggre,
            by = join_by(
              Store_CD == `Store Code`)) %>%
  mutate(Store_size = 100)
```

Next, `summary()` is used to check if the are any missing values in the data.frame.

```{r}
summary(stores)
```

### Tidying pop2019 data.frame

```{r}
glimpse(pop2019)
```

```{r}
glimpse(village)
```

The plot above shown that the village ID (i.e. V_ID) in pop2019 data.frame is not in the same structure as the village code in village sf data.frame.

Code chunk below will be used to remove the "-" sign from the values.

```{r}
pop2019 <- pop2019 %>%
  mutate(V_ID = str_remove_all(
    V_ID, "-"))
```

Now, we are ready to append data in pop2019 data.frame onto village sf data.frame by using `left_join()` of **dplyr** package.

```{r}
village <- village %>%
  left_join(pop2019,
            by = join_by(
              VILLCODE == V_ID))
```

### Check for missing or zero values

```{r}
summary(village)
```

The output print above revealed that the are five missing values in H_CNT, P_CNT, M_CNT and F_CNT fields.

Hence, code chunk below is used to remove the five records with missing values from village sf data.frame.

```{r}
village <- village %>%
  filter(!is.na(P_CNT)) 
```

```{r}
any(is.na(village$H_CNT))
any(is.na(village$P_CNT))
any(is.na(village$M_CNT))
any(is.na(village$F_CNT))
```

## Computing Distance Matrix

We will calculate the centroid of each village using the code chunk below. Then, compute the distance between the centroids of the villages and the stores.

```{r}
village_center <- st_point_on_surface(village)
distmat <- st_distance(village_center, stores) 
```

Next, the code chunk below is used to convert the distance matrix into long data.frame format

```{r}
rownames(distmat) <- village$`VILLCODE`
colnames(distmat) <- stores$`Store_CD`

distmat_long <- distmat %>%
  as.data.frame() %>%
  rownames_to_column(var = "VILLCODE") %>%
  pivot_longer(
    cols = -VILLCODE,
    names_to = "Store_CD",
    values_to = "distance"
  ) %>%
  mutate(distance = as.numeric(distance)) 
```

## Preparing Interaction Matrix

Code chunk below will be used to build the interaction matrix required by MCI package.

```{r}
interaction_matrix <- distmat_long %>%
  left_join(
    stores %>% 
      st_drop_geometry() %>% 
      select(Store_CD, Store_size),
    by = "Store_CD") %>%
  relocate(
    distance, .after = last_col())
```

```{r}
summary(interaction_matrix)
```

## Computing Huff model

```{r}
huff_share <- huff.shares(
  interaction_matrix, "VILLCODE", "Store_CD", 
  "Store_size", "distance")
```

### Trade area analysis for a single store

First, let us select market share data of a store by usign *Store_CD* field (i.e. TG).

```{r}
market_share <- huff_share %>%
  filter(Store_CD == "BT")
```

Next, trade areas of the selected will be derived by using the code chun below.

```{r}
Trade_area <- village %>%
  left_join(market_share)
```

Before plotting the trade area map, code chunk below is used to extract the selected store from stores sf data.frame.

```{r}
selected_store <- stores %>%
  filter(Store_CD == "BT")
```

Now, use the code chunk below to plot the trade areas of the selected store.

```{r}
#| fig-width: 14
#| fig-height: 12
tm_shape(Trade_area) +
  tm_polygons(fill = "p_ij",
              fill.scale = tm_scale_intervals(
                style = "quantile", 
                n = 10,
                values = "brewer.blues"),
              fill.legend = tm_legend(
                title = "Patronage Probability")) +
  tm_shape(selected_store) +
    tm_dots(size = 0.3,
            fill = "red",
            col = "black") +
  tm_title("Distribution of Patronage Probability of Store BT by village") +
  tm_layout(frame = TRUE) +
  tm_borders(fill_alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scalebar() +
  tm_grid(alpha =0.2) 

```

### Store sales analysis

One of the usage of Huff model is to estimate the store sales by using village population.

Firstly, let us append P_CNT field from *village* sf.data.frame to *huff_share* data.frame by using the code chunk below.

```{r}
huff_share_pp <- huff_share %>%
  left_join(village %>% 
      st_drop_geometry() %>% 
      select(VILLCODE, P_CNT),
    by = "VILLCODE") %>%
  relocate(
    distance, .after = last_col())
```

Next, `shares.total()` of **MCI** package will be used to calculate the total sales by using the code chunk below.

```{r}
huff_total <- shares.total(
  huff_share_pp, "VILLCODE", "Store_CD",
  "p_ij", "P_CNT")
```

Code chunk below is used to append the estimate store sales (Ej) onto stores sf data.frame.

```{r}
store_sales_est <- stores %>%
  left_join(huff_total,
            by = join_by(
              "Store_CD" == "suppliers_single"))
```

Now, we can visualise how well the estimate sales as compare to the actual sales by using the code chunk below.

```{r}
store_sales_nongeom <- store_sales_est %>% st_drop_geometry()

ggscatterstats(
  store_sales_nongeom,
  x = sum_E_j,
  y = Total_Sales)

```

```{r}
huff_share_wider <- huff_share %>%
  select(VILLCODE, Store_CD, p_ij) %>%
    pivot_wider(names_from = Store_CD, 
                values_from = p_ij)
```
